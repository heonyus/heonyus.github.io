---
layout: post
title: "An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains(1913)"
date: 2022-06-01
tags: [paper, markov]
---

# Intro

일반적인 확률론의 가정을 깨고, 현실 세계 데이터에서 발생하는 종속성(연결성)을 수학적으로 모델링하고 증명하고자 함

## Data

Pushkin의 소설 Eugene Onegin(실제 데이터)의 첫장 전체와 두 번째 장 16개 연을 분석

- 총 데이터 양: 20,000 개의 러시아어 알파벳 문자(20,000개의 연결된 시행;Connected Trials)
- 제외 문자: 러시아어의 단단한 부호(ъ), 부드러운 부호(ь)는 독립적으로 발음되지 않음

## Variable

20,000개의 문자를 모음(vowel) 또는 자음(consonant) 중 하나로 분류되는 시행으로 간주

- 연결된 시행(Connected Trials)'의 의미: 종속성 입증
- 마르코프가 이 20,000개의 문자를 '독립적인 시행'이 아닌 '연결된 시행'으로 간주했다는 사실 자체가 이 연구의 핵심 목적
- 고전적인 통계 모델(예: 베르누이 시행)은 각 시행이 서로 독립적이라고 가정하기 때문에...!
- 현실 세계의 많은 현상, 특히 언어 텍스트와 같은 연속적인 데이터에서는 고전적인 확률론이 가정하는 것처럼 각 시행이 독립적이지 않다는 것을 마르코프가 증명하고자 했다는 뜻

# Experiment

Quantifying Probability and Dependence  
확률 및 종속성 정량화

1. 기본 확률 ($p$) 계산

마르코프는 먼저 텍스트에서 모음이 나올 **알려지지 않은 상수 확률 $p$**의 근사값을 찾기 위해 관찰

- 20,000개의 문자를 순차적으로 100개의 글자씩 잘라 200개의 sub-sequence로 구성
- example: 첫 번째 100개 글자 그룹 (모음 42개)

```text
мой дядя самых честных правил когда не в шутку занемог 
он уважат себя заставил и лучше выдумат не мог его 
примѣр другим на
```

2. 분산 분석을 통한 종속성 입증

마르코프는 먼저 200개의 sub-sequence에서 얻은 *모음* 개수(42, 46, 40, ...)의 산포도(dispersion)를 분석

## 1단계: 실제 데이터의 분산 계산하기

먼저 마르코프는 자신이 수집한 200개의 모음 개수 데이터(42, 46, 40, ...)가 실제로 얼마나 흩어져 있는지 측정

### 1. 평균 ($\mu$) 계산
- 평균 계산($p$, $\mu$): 200개 숫자의 평균 (결과: 43.19) 
- $x_i$: 각 sub-sequence의 모음 개수 (42, 46, 40, ...)
- $N$: sub-sequence의 총 개수 (200)

$$\mu = \frac{1}{N} \sum_{i=1}^{N} x_i = \frac{1}{200} \sum_{i=1}^{200} x_i = 43.19$$

### 2. 편차 제곱의 합 (Sum of Squared Deviations)
- 편차 제곱의 합 계산: 200개의 각 숫자에서 평균(43.19)을 뺀 값을 제곱하고, 이 제곱한 값들을 모두 더했습니다. (결과: 1022.8) 

$$\sum_{i=1}^{N} (x_i - \mu)^2 = \sum_{i=1}^{200} (x_i - 43.19)^2 = 1022.8$$

### 3. 분산 ($\sigma^2$) 계산
- 평균 계산 (분산 도출): 위에서 구한 합을 데이터의 개수인 200으로 나누어 실제 분산을 구했습니다. (결과: 5.114) 
- 이 값 5.114가 바로 'Eugene Onegin' 텍스트에서 나타난 모음 수 분포의 실제 흩어짐 정도입니다.

$$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 = \frac{1022.8}{200} = 5.114$$


## 2단계: 이론적 분산과 비교하기
다음으로, "만약 문자들 사이에 아무런 관계가 없고 완전히 독립적이라면 분산이 얼마였을까?"를 계산
1. 이론적 분산 계산: 통계학의 이항분포 이론에 따르면, 독립적인 시행에서 분산은 $n \times p \times (1-p)$
- n: 그룹당 글자 수 (100)
- p: 모음일 확률 (약 0.432)
- 계산 결과: $100 \times 0.432 \times (1 - 0.432) \approx 24.5$

2. 두 분산 비교:
- 실제 분산: 약 5.1
- 이론적 분산: 약 24.5

두 값의 차이가 매우 큼<br>
실제 데이터가 이론적인 예측보다 훨씬 덜 흩어져 있고 평균에 몰려 있다는 뜻

3. '**분산 계수**'로 결론: 마르코프는 이 비교를 더 명확하게 보여주기 위해 **(실제 분산) / (이론적 분산)**의 비율을 계산하여 '분산 계수(coefficient of dispersion)' 라는 값을 만듬, 그 값은 약 0.208 

## 3단계: 연결성(종속성)
독립적이라면 이 계수는 1이 되어야 함<br>
0.208이라는 값은 데이터의 흩어짐이 독립적인 경우의 약 20% 수준밖에 되지 않음을 의미하며, 이는 문자들 사이에 서로를 제어하는 강력한 '**연결성(종속성)**'이 존재한다는 명백한 증거

## 4단계: '단순 연쇄(Simple Chain)' 모델로 종속성 설명하기

마르코프는 분산 계수(0.208)를 통해 종속성의 존재를 증명한 뒤, 그 종속성이 구체적으로 어떤 모습인지 설명하고자 함<br>

그래서 **"바로 앞 글자 하나가 다음 글자에 영향을 준다"**는 가설, 즉 **단순 연쇄 모델**을 제시

### 1. 조건부 확률 계산

이 가설을 증명하기 위해, 텍스트에서 두 가지 핵심적인 조건부 확률을 계산함(*Empirical Probability*)

- **$p_1$ (모음 뒤에 모음이 올 확률)**: 텍스트 전체에서 모음 바로 뒤에 또 모음이 나오는 경우를 계산
    - 결과: **약 0.128**
- **$q_0p_0$ (자음 뒤에 모음이 올 확률)**: 자음 바로 뒤에 모음이 나오는 경우를 계산
    - 결과: **약 0.663**

### 2. 단순 연쇄 모델의 증명

앞 글자가 모음이냐 자음이냐에 따라 다음 글자가 모음일 확률이 **0.128**과 **0.663**으로 극적인 차이를 보임<br>
이는 앞 글자가 다음 글자에 직접적인 영향을 미친다는 명백한 증거<br>

마르코프는 이 확률들을 이용해 '단순 연쇄 모델' 기반의 새로운 이론적 분산 계수를 계산했고 그 결과는 **약 0.3** <br>

이 값은 독립을 가정한 **1.0**보다는 실제 관측값 **0.208**에 훨씬 더 가까워진 결과로, 이 모델이 현실을 더 잘 설명함을 보여줌<br>

① 실제 데이터의 분산 계수: **0.208**<br>
② '완전 독립'을 가정한 이론의 분산 계수: **1.0**<br>

①과 ② 사이의 차이가 너무 큼<br>
마르코프는 "'단순 연쇄' 모델을 적용하면 이 차이를 크게 줄일 수 있다"고 주장하며 새로운 이론값을 계산<br>

#### '단순 연쇄' 모델 기반의 이론값 계산

마르코프는 자신이 이전에 발표했던 다른 논문의 공식을 가져와, '단순 연쇄' 모델이 맞다고 가정했을 때 분산 계수가 어떻게 나와야 하는지를 계산<br>

1.  **종속성의 '정도'를 하나의 숫자로 표현하기**
    * 먼저, 종속성의 핵심인 두 확률($p_1$과 $p_0$)의 차이를 계산하여 **델타($\delta$)**라는 값으로 정의<br>

    > $$\delta = p_1 - p_0 = 0.128 - 0.663 = -0.535$$
    <br>

    * 이 $\delta$값은 앞 글자가 뒷 글자에 미치는 영향력의 크기

2.  **새로운 이론적 분산 계수 계산**
    * 이 $\delta$값을 자신의 공식 $\frac{1+\delta}{1-\delta}$에 대입, '단순 연쇄' 모델이 예측하는 새로운 이론적 분산 계수를 계산험<br>

    > $$\frac{1 + (-0.535)}{1 - (-0.535)} = \frac{0.465}{1.535} \approx 0.303$$
    <br>

    * 논문에 나온 **약 0.3**의 정체

#### 결론: 모델의 우수성 입증

이제 세 개의 값을 나란히 놓고 비교할 수 있게 됨됨

* **실제 데이터 관측값**: **0.208**
* **'단순 연쇄' 모델 예측값**: **0.3** ✅
* **'완전 독립' 모델 예측값**: **1.0**

'단순 연쇄' 모델의 예측값(0.3)이 '완전 독립' 모델의 예측값(1.0)보다 **실제 데이터(0.208)에 훨씬 더 가까움**<br>

이는 **"텍스트의 문자들은 완전 독립이 아니라, 바로 앞 글자의 영향을 받는 '단순 연쇄' 구조를 가진다"**는 마르코프의 주장이 통계적으로 매우 설득력 있다는 것을 의미<br>

## 5단계: '복합 연쇄(Complex Chain)' 모델로 정밀도 높이기

마르코프는 여기서 한 걸음 더 나아가 모델의 정확도를 극한으로 끌어올리고자 생각함<br>

**"앞선 두 개의 글자가 다음 글자에 영향을 준다"**는 더 정교한 **복합 연쇄 모델**을 제시<br>

### 1. 추가 확률 계산 및 최종 검증

이를 위해 '모음-모음' 뒤에 모음이 올 확률($p_{1,1}$), '자음-자음' 뒤에 자음이 올 확률($q_{0,0}$) 등 훨씬 더 복잡한 조건부 확률들을 계산<br>

그리고 이 모든 값들을 자신의 복합 연쇄 모델 공식에 대입하여 최종 이론 분산 계수를 계산<br>

> **그 결과는 놀랍게도 0.195**

### 2. 최종 결론: 모델과 현실의 일치

* **실제 데이터 관측값**: **0.208**
* **복합 연쇄 모델 예측값**: **0.195**

마르코프는 이 두 숫자가 거의 일치하는 것을 보며 이것이 단순한 우연이 아니라 자신의 '연쇄(Chain)' 모델이 언어와 같은 현실 데이터의 복잡한 종속 구조를 수학적으로 완벽에 가깝게 설명해 낸다는 증거라고 결론