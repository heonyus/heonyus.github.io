---
layout: post
title: "Finding Structure in Time(1990)"
date: 2022-07-10
tags: [NLP, Recurrent Neural Network, Representation Learning, Cognitive Science]
---
# [논문리뷰] Finding Structure in Time

> Cognitive Science, 1990. [Paper](https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1)
> Jeffrey L. Elman
> University of California, San Diego
> 1990

## Introduction
시간은 인지에서 중요한 요소다.<br>
사람은 말을 순서대로 듣고 이해하지만, 컴퓨터에게 이런 '시간의 흐름'을 이해시키는 건 어렵다.<br>
인공신경망은 시간을 공간의 형태로 즉, 모든 정보를 덩어리로 한번에 처리했다.<br>
이때 길이 제한을 초과한 sequence 길이가 들어오면 처리 할 수 없게 된다.<br>
그리고 `[0, 1, 1, 0]` 이라는 패턴과 `[0, 0, 1, 1]` 이라는 패턴은 사람이 보기에는 같은 `11` 패턴이 이동한 것 이지만, 컴퓨터는 이 둘을 완전히 다른 별개의 데이터로 인식하기 때문에 패턴의 상대적인 시간 구조를 파악하지 못하는 것이다.<br>
<br>
본본 논문에서는 시간을 명시적으로 표현하지 않고 처리를 통해서 시간의 효과가 암묵적으로 드러나도록 한다.<br>
<br>

### Jordan Network
초기 순환 신경망(RNN) 구조

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/FslBP1L.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- INPUT: 현재 시점(t)에 들어오는 정보
- HIDDEN: 현재의 INPUT과 과거의 기억(STATE)을 종합해서 실질적인 계산을 수행하는 중간 처리층
- OUTPUT: 현재 시점(t)에 네트워크가 내보내는 최종 결과물
- STATE: 가장 중요한 부분으로, 바로 직전 시점(t-1)의 OUTPUT 값을 그대로 복사해서 저장하는 기억 장치 역할

### Elman Network
본 논문에서 제안하는 순환 신경망(RNN) 구조

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/In4kNra.png
" referrerpolicy="no-referrer" loading="lazy" />
</p>

가장 중요한 특징은HIDDEN UNITS에서 CONTEXT UNITS로 향하는 되먹임(feedback) 연결이다. 

- 계산: 현재 시점(t)에 HIDDEN UNITS는 INPUT UNITS의 정보와 CONTEXT UNITS에 저장된 과거의 기억을 함께 받아 계산을 수행
- 출력: 이 계산 결과를 바탕으로 OUTPUT UNITS를 통해 결과를 내보냄냄
- 기억 (복사): 동시에, HIDDEN UNITS의 활성화 값(계산 결과)을 그대로 CONTEXT UNITS로 복사해서 저장

> Jordan Network는 Output Unit만 기억하는 반면 Elman Network는 Hidden Unit을 기억함

- 결과물은 압축, 일반화, 단순화 될 수 있음
- 과정은 맥락, 고려사항을 알 수 있음

## Experiment

### XOR Problem
이 실험은 전통적인 XOR 문제는 두개의 입력이 서로 같을때 1로 출력하는 문제다.

| 입력 1 | 입력 2 | 결과 |
|--------|--------|------|
|   0    |   0    |  0   |
|   1    |   1    |  0   |

- Temporal XOR
    - 본 논문에서는 입력을 순차적으로 하나씩 보여줌
    - `[입력1, 입력2, 결과]` 묶음을 무작위로 계속 이어 붙여서 `101000110...` 과 같은 하나의 긴 데이터 스트림을 만듬


<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/kKPBSIi.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- X축 (Cycle): 시간의 흐름(단계)
- Y축 (Error)

`[입력1, 입력2, 결과]` 순서로 데이터가 들어오고, 네트워크는 매 순간 다음 데이터를 예측해야 한다.<br>
입력1과 입력2를 모두 봤을때 다음 오차가 줄어드는것을 알 수 있다.

### STRUCTURE IN LETTER SEQUENCES
이 실험은 문자의 규칙과 무작위성이 섞인 시계열 데이터에서 숨겨진 구조를 발견하는 문제다.
- 데이터: b, d, g 세 자음을 무작위로 나열한 뒤, 정해진 규칙에 따라 모음을 붙여 인공적인 "음절" 시퀀스를 만듬.

| 자음 | 음절 시퀀스스 | 
|--------|--------|
|   b    |   ba    |
|   d    |   dii    |
|   g    |   guu    |

- 예시: `diibaguuubadiiguuu`
- 규칙: 다음에 올 자음은 무작위, 일단 자음이 나오면 뒤따르는 모음은 규칙적

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/VG0APsU.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 입력 표현: 각 알파벳은 6개의 feature를 나타내는 6 bit-vector로 표현

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/JILS2TP.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

#### 학습

학습 횟수 200번 반복해 학습을 진행한다.<br>
1. t 시점 글자(6 bit-vector) 입력
$$h(t) = \sigma_h (W_{ih} x(t) + W_{ch} h(t-1) + b_h)$$
2. t+1 시점 글자 예측
$$\hat{y}(t) = \sigma_y (W_{ho} h(t) + b_o)$$
3. 실제 정답, 예측값 비교 'error' 계산
$$E(t) = \frac{1}{2} \sum_{j=1}^{6} (x_j(t+1) - \hat{y}_j(t))^2$$
4. Backpropagation
$$W_{\text{new}} = W_{\text{old}} - \eta \frac{\partial E(t)}{\partial W}$$
5. 2~4 반복

#### 결과(CONSONANTAL)
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/UmHqcGV.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 자음 feature 예측: 모음 `(i,i)`가 끝나면 다음엔 어떤 자음이 올지는 몰라도 *자음이 온다*라는 사실 자체는 100% 예측 가능
- 따라서 자음을 나타내는 첫번째 bit에 대한 error는 항상 낮게 유지됨

#### 결과(HIGH)
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/9mWYh0M.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 고모음 feature 예측: 반면 고모음은 다음에 올 자음이 `b`인지 `d`인지에 따라 값이 달라지므로 예측이 어려움
- 따라서 해당 bit의 error가 높음

→ 본 논문에서는 '**순차적 입력**'이라는 문제를 '**순환 구조가 제공하는 기억**'이라는 해결책으로 풀었다.

### Discovering the Notion "Word"