---
layout: post
title: "Random Forests (2001)"
date: 2022-11-20
tags: [Machine Learning, Ensemble Learning, Decision Tree, Statistics, Breiman]
---

# [ë…¼ë¬¸ë¦¬ë·°] Random Forests (2001)

> Machine Learning, 45(1), 5â€“32. [PDF](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)  
> Leo Breiman, University of California, Berkeley  
> Published: January 2001


## Introduction

1990ë…„ëŒ€ í›„ë°˜ì€ **ì•™ìƒë¸” í•™ìŠµ(ensemble learning)** ì˜ ì‹œëŒ€ì˜€ë‹¤.<br>
Baggingê³¼ Boostingì˜ ì„±ê³µì€ ë‹¨ì¼ ê²°ì • íŠ¸ë¦¬ì˜ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí–ˆë‹¤.  

### Background
<br>

#### Decision Tree
- Decision Tree: ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì§ˆë¬¸(ì¡°ê±´ë¬¸) ìœ¼ë¡œ ë‚˜ëˆ„ì–´ê°€ë©° ìµœì¢…ì ìœ¼ë¡œ ì˜ˆì¸¡ê°’(class ë˜ëŠ” ìˆ˜ì¹˜) ì„ ì¶œë ¥í•˜ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ì˜ ëª¨ë¸

```text
[ROOT]
 â”œâ”€â”€ x1 < 0.5 ? 
 â”‚     â”œâ”€â”€ Yes â†’ Class A
 â”‚     â””â”€â”€ No  â†’ Class B
```

- ì…ë ¥ íŠ¹ì„±(feature) ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ë¶„í• (split)í•˜ê³  ê° leaf ë…¸ë“œì—ì„œ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰

ê²°ì • íŠ¸ë¦¬ëŠ” í›ˆë ¨ ë°ì´í„° $(x_i, y_i)$ë¥¼ ì…ë ¥ ê³µê°„ì˜ ì—¬ëŸ¬ ì˜ì—­ $R_m$ìœ¼ë¡œ ë¶„í• í•˜ê³ , ê° ì˜ì—­ë§ˆë‹¤ ê³ ì •ëœ ì˜ˆì¸¡ê°’ $c_m$ì„ í• ë‹¹í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

$$
\hat{f}(x) = \sum_{m=1}^M c_m \cdot I(x \in R_m)
$$
- $R_m$: më²ˆì§¸ ë¶„í•  ì˜ì—­ (ì˜ˆ: â€œ$x_1 < 3.2$ ì´ê³  $x_2 \geq 0.5$â€)
- $c_m$: í•´ë‹¹ ì˜ì—­ì˜ í‰ê·  ì‘ë‹µê°’(íšŒê·€) ë˜ëŠ” ìµœë¹ˆ í´ë˜ìŠ¤(ë¶„ë¥˜)
- $I(\cdot)$: Indicator í•¨ìˆ˜ (ì¡°ê±´ì´ ì°¸ì´ë©´ 1, ê±°ì§“ì´ë©´ 0)

ì¦‰, íŠ¸ë¦¬ êµ¬ì¡°ëŠ” ì—¬ëŸ¬ ifâ€“thenâ€“else ê·œì¹™ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

#### Bagging & Boosting

- **Bagging**: ë°ì´í„° ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„ ì—¬ëŸ¬ íŠ¸ë¦¬ë¥¼ í•™ìŠµ â†’ í‰ê· í™” 

<div align="center">

$$
\text{(regression)}\quad \hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}_b(x)
$$

$$
\text{(classification)}\quad \hat{f}_{\text{bag}}(x) = \mathrm{Mode}\left\{ \hat{f}_1(x), \ldots, \hat{f}_B(x) \right\}
$$

</div>
- **Boosting**: ì´ì „ ì˜¤ì°¨(ë¶„ë¥˜ê°€ í‹€ë¦° ìƒ˜í”Œ)ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë‘ê³ , ì—¬ëŸ¬ ì•½í•œ ë¶„ë¥˜ê¸°(íŠ¸ë¦¬)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°©ë²•

  - ìµœì¢… ì˜ˆì¸¡ í•¨ìˆ˜:  

    $$
    F_M(x) = \sum_{m=1}^M \alpha_m h_m(x)
    $$

    - $h_m(x)$: më²ˆì§¸ ì•½í•œ ë¶„ë¥˜ê¸°(ì˜ˆ: ì‘ì€ ê²°ì •íŠ¸ë¦¬)
    - $\alpha_m$: më²ˆì§¸ ë¶„ë¥˜ê¸°ì˜ ê°€ì¤‘ì¹˜ (ì˜¤ë¶„ë¥˜ìœ¨ì— ë”°ë¼ ì¡°ì •)
    - $M$: ì•½í•œ ë¶„ë¥˜ê¸° ê°œìˆ˜

  - ê°€ì¤‘ì¹˜ ê³„ì‚°(Adaboost ê¸°ì¤€):  
    $$
    \alpha_m = \frac{1}{2} \ln\left(\frac{1-\epsilon_m}{\epsilon_m}\right)
    $$
    - $\epsilon_m$: më²ˆì§¸ ë¶„ë¥˜ê¸°ì˜ ì˜¤ë¶„ë¥˜ìœ¨

  - ìš”ì•½:  
    1. ì´ì „ ë‹¨ê³„ì—ì„œ ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬  
    2. ì•½í•œ ë¶„ë¥˜ê¸°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€  
    3. ê° ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“¦

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/hu06JLX.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

í•˜ì§€ë§Œ ë‘ ë°©ë²• ëª¨ë‘ í•œê³„ê°€ ìˆì—ˆë‹¤.


| ë¬¸ì œì  | ì„¤ëª… |
|--------|------|
| Bagging | íŠ¸ë¦¬ ê°„ **ìƒê´€ê´€ê³„(correlation)** ê°€ ë†’ì•„ ì˜¤ì°¨ ê°ì†Œê°€ ì œí•œì  |
| Boosting | **ë…¸ì´ì¦ˆì™€ ì´ìƒì¹˜(outlier)** ì— ë¯¼ê°, ì˜¤ë²„í”¼íŒ… ê°€ëŠ¥ |

Breimanì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Random Forest (RF)** ë¥¼ ì œì•ˆí–ˆë‹¤.  
í•µì‹¬ì€ **íŠ¸ë¦¬ ìƒì„± ê³¼ì •ì— â€˜ë¬´ì‘ìœ„ì„±(randomness)â€™ì„ ì£¼ì…í•˜ì—¬ ìƒê´€ì„ ì¤„ì´ê³ , í‰ê· í™”ë¡œ ê°•ê±´ì„±ì„ í™•ë³´**í•˜ëŠ” ê²ƒì´ë‹¤.

> â€œRandom forests are a combination of tree predictors,  
> each constructed using a random vector sampled independently.â€ â€” *Breiman (2001)*

## Method
<br>

### 1ï¸âƒ£ ì •ì˜ (Definition)

ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” **ì„œë¡œ ë‹¤ë¥¸ ëœë¤ ë²¡í„° $\Theta_k$** ì— ì˜í•´ ìƒì„±ëœ  
íŠ¸ë¦¬ ì§‘í•© $\{h(x, \Theta_k)\}$ ì˜ ì•™ìƒë¸”ì´ë‹¤.

$$
H(x) = \text{majority\_vote}\{h(x, \Theta_1), h(x, \Theta_2), \dots, h(x, \Theta_K)\}
$$

ê° íŠ¸ë¦¬ëŠ” **í›ˆë ¨ ìƒ˜í”Œ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ + ë¬´ì‘ìœ„ feature ì„ íƒ**ìœ¼ë¡œ í•™ìŠµëœë‹¤.  
íŠ¸ë¦¬ ìˆ˜ê°€ ì¶©ë¶„íˆ ë§ìœ¼ë©´, ì˜ˆì¸¡ í™•ë¥ ì´ ì•ˆì •í™”ë˜ì–´ ì˜¤ë²„í”¼íŒ…ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤.

---

### 2ï¸âƒ£ ì¼ë°˜í™” ì˜¤ì°¨ ìˆ˜ë ´ (Convergence)

ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì¼ë°˜í™” ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
PE^* = P_{X,Y}\big(P_\Theta(h(X,\Theta)=Y) - \max_{j \neq Y} P_\Theta(h(X,\Theta)=j) < 0 \big)
$$

ì´ëŠ” **ê°•í•œ ëŒ€ìˆ˜ì˜ ë²•ì¹™(Strong Law of Large Numbers)** ì— ì˜í•´  
íŠ¸ë¦¬ ìˆ˜ $K \to \infty$ ì¼ ë•Œ ìˆ˜ë ´í•œë‹¤.  
ì¦‰, íŠ¸ë¦¬ë¥¼ ë¬´í•œíˆ ì¶”ê°€í•´ë„ **overfittingì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.**

---

### 3ï¸âƒ£ Strengthâ€“Correlation ì´ë¡ 

Breimanì€ RFì˜ ì •í™•ë„(Generalization Error)ê°€  
íŠ¸ë¦¬ì˜ **ê°•ë„(strength)** ì™€ **ìƒê´€(correlation)** ì˜ í•¨ìˆ˜ì„ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í–ˆë‹¤.

$$
PE^* \le \frac{\rho (1 - s^2)}{s^2}
$$

| í•­ëª© | ì˜ë¯¸ |
|------|------|
| $s$ | ê° íŠ¸ë¦¬ì˜ í‰ê· ì ì¸ ë¶„ë¥˜ ì •í™•ë„ (Strength) |
| $\rho$ | íŠ¸ë¦¬ ê°„ ì˜ˆì¸¡ ìƒê´€ê´€ê³„ (Correlation) |

ì¦‰,  
- íŠ¸ë¦¬ì˜ **ê°•ë„(s)** ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ê³ ,  
- íŠ¸ë¦¬ ê°„ **ìƒê´€(Ï)** ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ë‹¤.

> **Rule of Thumb**  
> ë‚®ì€ Ï / ë†’ì€ s â†’ ë‚®ì€ ì¼ë°˜í™” ì˜¤ì°¨

---

### 4ï¸âƒ£ Out-of-Bag (OOB) ì¶”ì •

OOBëŠ” RFì˜ í•µì‹¬ ë‚´ë¶€ í‰ê°€ ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤.  
íŠ¸ë¦¬ í•™ìŠµ ì‹œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ìƒ˜í”Œ(ì•½ 1/3)ì„ ì´ìš©í•´  
ë³„ë„ì˜ ê²€ì¦ ì—†ì´ ì¼ë°˜í™” ì˜¤ì°¨ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.

- **OOB Error â‰ˆ Test Error**
- **OOB Strength, OOB Correlation** ë„ ë‚´ë¶€ì ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥

ì´ë¡œì¨ RFëŠ” ë³„ë„ì˜ validation set ì—†ì´ë„ í•™ìŠµ ì¤‘ì— ìê¸° í‰ê°€(self-evaluation)ê°€ ê°€ëŠ¥í•˜ë‹¤.

---

### 5ï¸âƒ£ Random Feature Selection

ê° ë…¸ë“œì—ì„œ $M$ê°œì˜ feature ì¤‘ ë¬´ì‘ìœ„ë¡œ $F$ê°œë¥¼ ë½‘ì•„ split í›„ë³´ë¡œ ì‚¬ìš©í•œë‹¤.  
ì´ë¥¼ **Forest-RI (Random Input)** ë¼ í•œë‹¤.

$$
F = 1 \text{ ë˜ëŠ” } F = \lfloor \log_2 M + 1 \rfloor
$$

ë˜ ë‹¤ë¥¸ ë³€í˜•ì¸ **Forest-RC (Random Combination)** ì€  
ëœë¤ ì„ í˜• ê²°í•©ìœ¼ë¡œ featureë¥¼ ìƒì„±í•œë‹¤.

$$
z = \sum_{i=1}^{L} w_i x_i, \quad w_i \sim U[-1,1]
$$

ì´ ë°©ì‹ì€ feature ìˆ˜ê°€ ì ì€ ë¬¸ì œì—ì„œ ìœ íš¨í•˜ë©°,  
AdaBoostë³´ë‹¤ **ë¹ ë¥´ê³ **, **ë…¸ì´ì¦ˆì— ê°•í•˜ë©°**, **ë³‘ë ¬í™”ê°€ ì‰¬ìš´** ì¥ì ì„ ê°€ì§„ë‹¤.

---

## ğŸ“Š Experiment & Result

### 1ï¸âƒ£ ì‹¤í—˜ êµ¬ì„±

- **ë°ì´í„°ì…‹:** 13ê°œ UCI + 3ê°œ ëŒ€ê·œëª¨ ë°ì´í„° + 4ê°œ Synthetic  
- **ë¹„êµ ëŒ€ìƒ:** AdaBoost (50 trees) vs Random Forest (100 trees)  
- **ì§€í‘œ:** Test Error, OOB Error  
- **ëª¨ë¸:** Forest-RI / Forest-RC / Adaboost

---

### 2ï¸âƒ£ ì£¼ìš” ê²°ê³¼ ìš”ì•½

| Dataset | AdaBoost | Forest-RI | Forest-RC |
|:--|:--:|:--:|:--:|
| Breast Cancer | 3.2 | 2.9 | 3.1 |
| Diabetes | 26.6 | 24.2 | 23.0 |
| Sonar | 15.6 | 15.9 | 13.6 |
| Vowel | 4.1 | 3.4 | 3.3 |
| Image | 1.6 | 2.1 | 1.6 |
| Letters | 3.4 | 3.5 | 3.4 |
| Zip-code | 6.2 | 6.3 | 6.2 |

> ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì…‹ì—ì„œ Random Forestê°€ AdaBoostì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë‚®ì€ ì˜¤ì°¨ìœ¨ì„ ê¸°ë¡í–ˆë‹¤.

---

### 3ï¸âƒ£ Strengthâ€“Correlation ê´€ê³„ ë¶„ì„

Breimanì€ ê° ë…¸ë“œ ë¶„í• ì— ì‚¬ìš©í•  feature ìˆ˜ $F$ë¥¼ 1~50ê¹Œì§€ ë³€í™”ì‹œì¼œ  
strength, correlation, test errorì˜ ë³€í™”ë¥¼ ì¸¡ì •í–ˆë‹¤.

#### ğŸ“ˆ Figure 1. Sonar ë°ì´í„°ì…‹

![figure1](https://raw.githubusercontent.com/jeheon-tech/blog-assets/main/randomforest2001_fig1.png)

- $F > 4$ ì´í›„ strengthëŠ” ê±°ì˜ ì¼ì •  
- correlationì€ ì¦ê°€  
- test errorëŠ” U-ì í˜•íƒœë¡œ ì¦ê°€ â†’ ìµœì  $F$ëŠ” 4~8

#### ğŸ“‰ Figure 2. Breast Cancer ë°ì´í„°ì…‹

![figure2](https://raw.githubusercontent.com/jeheon-tech/blog-assets/main/randomforest2001_fig2.png)

- correlationì€ ì™„ë§Œíˆ ì¦ê°€  
- strengthëŠ” ê±°ì˜ ì¼ì •  
- ìµœì†Œ errorëŠ” $F=1$ ê·¼ì²˜ì—ì„œ ë°œìƒ

#### ğŸŒ Figure 3. Satellite ë°ì´í„°ì…‹

![figure3](https://raw.githubusercontent.com/jeheon-tech/blog-assets/main/randomforest2001_fig3.png)

- ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” strengthê°€ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€  
- correlationì€ ë¹ ë¥´ê²Œ í¬í™”  
- ê²°ê³¼ì ìœ¼ë¡œ errorëŠ” ë¯¸ì„¸í•˜ê²Œ ê°ì†Œ

> **ê²°ë¡ :** ì‘ì€ ë°ì´í„°ì…‹ì€ â€œë‚®ì€ F â†’ ì €ìƒê´€â€, í° ë°ì´í„°ì…‹ì€ â€œë†’ì€ F â†’ ë†’ì€ strengthâ€ê°€ ìœ ë¦¬í•˜ë‹¤.

---

### 4ï¸âƒ£ Noise Robustness ì‹¤í—˜

5%ì˜ ë¼ë²¨ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, AdaboostëŠ” ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ëœ ë°˜ë©´  
Random ForestëŠ” ê±°ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šì•˜ë‹¤.

| Dataset | AdaBoost Î”Error | Random Forest Î”Error |
|:--|--:|--:|
| Breast Cancer | +43.2% | +1.8% |
| Ionosphere | +27.7% | +3.8% |
| Votes | +48.9% | +6.3% |

---

### 5ï¸âƒ£ Variable Importance ì‹œê°í™”

#### ğŸ©¸ Figure 4. Diabetes ë°ì´í„°ì…‹ (F=1)
![figure4](https://raw.githubusercontent.com/jeheon-tech/blog-assets/main/randomforest2001_fig4.png)
- ë³€ìˆ˜ 2, 6, 8ë²ˆì´ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ë‚˜íƒ€ë‚¨

#### ğŸ§¬ Figure 6. Votes ë°ì´í„°ì…‹ (ì •ë‹¹ ë¶„ë¥˜)
![figure6](https://raw.githubusercontent.com/jeheon-tech/blog-assets/main/randomforest2001_fig6.png)
- ë³€ìˆ˜ 4ë²ˆ(í•µì‹¬ ì´ìŠˆ)ì˜ ì¤‘ìš”ë„ê°€ ì••ë„ì   
- ë‹¨ì¼ ë³€ìˆ˜ë§Œìœ¼ë¡œë„ ì „ì²´ ëª¨ë¸ ìˆ˜ì¤€ì˜ ë¶„ë¥˜ ì •í™•ë„ ë‹¬ì„±

---

## ğŸ“ˆ Regression Extension

Breimanì€ RFë¥¼ íšŒê·€ë¡œ í™•ì¥í•˜ë©° ë‹¤ìŒ ê²°ê³¼ë¥¼ ì œì‹œí–ˆë‹¤:

$$
PE^*_{forest} \le \rho \, PE^*_{tree}
$$

ì¦‰, **íŠ¸ë¦¬ ê°„ ì”ì°¨(residual) ìƒê´€ì´ ë‚®ì„ìˆ˜ë¡ ì˜¤ì°¨ê°€ ì‘ì•„ì§„ë‹¤.**  
ì‹¤í—˜ ê²°ê³¼, Bagging ëŒ€ë¹„ ì•½ 10~20%ì˜ ì˜¤ì°¨ ê°ì†Œë¥¼ ë³´ì˜€ë‹¤.

| Dataset | Bagging | Random Forest |
|:--|:--:|:--:|
| Boston Housing | 11.4 | **10.2** |
| Ozone | 17.8 | **16.3** |
| Friedman #1 | 6.3 | **5.7** |
| Abalone | 4.9 | **4.6** |

---

## ğŸ§  Discussion & Conclusion

ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë‹¨ìˆœí•œ íŠ¸ë¦¬ ì§‘í•©ì„ ë„˜ì–´  
**Biasâ€“Variance trade-offë¥¼ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” êµ¬ì¡°**ë¥¼ ê°–ëŠ”ë‹¤.

| íŠ¹ì„± | ì„¤ëª… |
|------|------|
| ğŸ¯ ì •í™•ë„ | AdaBoostì™€ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë†’ìŒ |
| ğŸ§± ê²¬ê³ ì„± | ë…¸ì´ì¦ˆ, ì´ìƒì¹˜ì— ê°•í•¨ |
| âš™ï¸ íš¨ìœ¨ì„± | ë³‘ë ¬í™” ë° ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì— ì í•© |
| ğŸ” í•´ì„ì„± | Feature Importance ë° OOBë¡œ ë‚´ë¶€ ê²€ì¦ ê°€ëŠ¥ |

Breimanì€ ë…¼ë¬¸ ë§ë¯¸ì— ë‹¤ìŒê³¼ ê°™ì€ í¥ë¯¸ë¡œìš´ ê°€ì„¤ì„ ë‚¨ê²¼ë‹¤.

> â€œIn later stages, AdaBoost may be emulating a random forest.â€  
> â€” ì¦‰, AdaboostëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ í™•ë¥ ì  í˜•íƒœì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

---

> â€œRandomness reduces correlation.  
> Strength increases bias reduction.  
> Together they make forests powerful.â€  
> â€” *Leo Breiman (2001)*

---

## ğŸ”— References

- Breiman, L. (2001). *Random Forests*. Machine Learning, 45(1), 5â€“32.  
- Breiman, L. (1996). *Bagging Predictors*. Machine Learning, 26(2), 123â€“140.  
- Freund, Y. & Schapire, R. (1996). *Experiments with a new boosting algorithm*.  
- Dietterich, T. (1998). *An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees*.  
- Ho, T.K. (1998). *The Random Subspace Method for Constructing Decision Forests*. IEEE PAMI.

---

#ï¸âƒ£ #RandomForest #EnsembleLearning #MachineLearning #LeoBreiman #DecisionTree
