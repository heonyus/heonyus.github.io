---
layout: post
title: "Latent Dirichlet Allocation (2003)"
date: 2022-12-15
category: Machine Learning
tags: [Topic Modeling, Bayesian Inference, NLP, LDA]
---

# [논문리뷰] Latent Dirichlet Allocation (2003)
> David M. Blei, Andrew Y. Ng, Michael I. Jordan  
> *Journal of Machine Learning Research*, Vol. 3 (2003)  
> [Paper](https://jmlr.org/papers/volume3/blei03a/blei03a.pdf)


## Introduction

텍스트 코퍼스(corpus)를 확률적으로 모델링하려는 시도는 오래전부터 이어져 왔다.<br>
기존의 대표적인 접근법은 다음과 같다:<br>

### Background
1. **TF-IDF** : 문서를 단어의 빈도 벡터로 변환

<div id="tfidf_plot" style="width:100%; height:450px; margin: 20px auto;"></div>

<script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
<script>
  const words = ["data","model","topic","bayes","graph","markov"];
  const weights = [0.35,0.25,0.10,0.05,0.15,0.10];
  
  Plotly.newPlot('tfidf_plot', [{
    type: 'bar',
    x: words,
    y: weights,
    marker: {
      color: '#60a5fa',
      line: { color: '#3b82f6', width: 1.5 }
    },
    hovertemplate: '<b>%{x}</b><br>Weight: %{y:.2f}<extra></extra>'
  }], {
    title: { text: 'TF-IDF Example (Doc1)', font: { size: 18 } },
    xaxis: { title: 'Words', tickangle: -45 },
    yaxis: { title: 'TF-IDF Weight', range: [0, 0.4] },
    margin: { l: 60, r: 30, t: 60, b: 80 },
    plot_bgcolor: 'rgba(0,0,0,0)',
    paper_bgcolor: 'rgba(0,0,0,0)'
  }, {
    responsive: true,
    displayModeBar: true,
    displaylogo: false
  });
</script>


**정의**: 단어 $t$가 문서 $d$에서 얼마나 중요한지 측정하는 가중치. <br> “문서 내부의 상대적 빈도$(TF)$”와 “코퍼스 전체에서의 희소성$(IDF)$”의 곱.

* **수식**
  $$
  \mathrm{TF\text{-}IDF}(t, d, D) = \mathrm{TF}(t, d) \times \mathrm{IDF}(t, D)
  $$
  $$
  \mathrm{TF}(t, d) = \frac{f_{t,d}}{\sum_{t'} f_{t',d}}, \qquad
  \mathrm{IDF}(t, D) = \log \left( \frac{N}{1 + n_t} \right)
  $$

  * $f_{t,d}$: 단어 $t$의 문서 $d$ 내 등장 횟수
  * $N$: 전체 문서 수, $n_t$: 단어 $t$를 포함한 문서 수


* **출력**: 각 문서는 $\mathbb{R}^V$의 고정 길이 벡터(희소).

* **장점**: 계산 간단, 해석 쉬움, 검색/유사도에 강함.

* **한계**: 단어 순서·문맥 무시(BoW), 동의어/다의어 처리 약함, 통계적 생성 의미 없음.


2. **Latent Semantic Indexing (LSI)** : 특이값 분해(SVD)로 차원 축소

**정의**: TF-IDF 행렬 $X\in\mathbb{R}^{V\times M}$를 SVD로 분해해 **낮은 차원의 잠재 의미 공간**으로 투영하여 유사도/검색을 개선.

* **수식 (랭크-(k) 근사)**
  $$
  X \approx X_k = U_k \Sigma_k V_k^\top
  $$

  * $U_k \in \mathbb{R}^{V \times k}$: 단어 잠재축, $V_k \in \mathbb{R}^{M \times k}$: 문서 잠재표현, $\Sigma_k$: 특이값 대각행렬
  * 문서 $j$의 임베딩: $\mathbf{d}_j = \Sigma_k V_k^\top[:,j]$
  * 쿼리 $\mathbf{q}$ 투영: $\tilde{\mathbf{q}} = \Sigma_k^{-1} U_k^\top \mathbf{q}$ 후 코사인 유사도

* **효과**: 노이즈 제거, **동의어/다의어** 일부 해결(선형 결합으로 주제축 학습)

* **장점**: 빠른 근사 검색, 선형대수 기반 구현 용이

* **한계**: **생성 모델 아님**(확률 의미 부족), 새 문서 **fold-in** 필요, k 선택 민감, 음수 허용(해석성 약화)

3. **Probabilistic LSI (pLSI)** : 단어를 *잠재 주제(topic)* 의 혼합으로 표현

**정의**: 문서 $d$와 단어 $w$의 결합을 **잠재 토픽 $z$** 로 매개하는 확률 모델. <br>
 각 문서는 토픽의 혼합비 $p(z\mid d)$를 갖고, 단어는 $p(w\mid z)$로 생성된다.

* **수식**
  $$
  p(d, w) = p(d) \sum_{z=1}^{K} p(w \mid z) \, p(z \mid d)
  $$

  * 문서 $d$의 토픽 분포: $\mathbf{\pi}_d = p(z \mid d)$
  * 토픽 $z$의 단어 분포: $\phi_z = p(w \mid z)$

* **추정(EM 개요)**

  * **E-step**: 
    $$
    p(z \mid d, w) \propto p(w \mid z) \, p(z \mid d)
    $$
  * **M-step**:
    $$
    p(w \mid z) \propto \sum_d n(d, w) \, p(z \mid d, w)
    $$
    $$
    p(z \mid d) \propto \sum_w n(d, w) \, p(z \mid d, w)
    $$

* **장점**: LSI에 **확률적 의미** 부여, 문서에 **다중 토픽 혼합** 허용

* **핵심 한계**:

  * $\mathbf{\pi}_d$가 **훈련 문서 지표에 묶임** → **새 문서의 사전 확률 정의 불가** (진정한 생성 모델 아님)
  * 파라미터 수 $O(KV+KM)$로 **문서 수 $M$** 에 선형 증가 → **과적합** 용이

이러한 방법들은 다음 한계를 가진다.

1. **문서 단위의 생성 확률 모델 부재**  
   → pLSI는 학습된 문서 외의 새로운 문서에 대해 확률을 정의할 수 없다.  
2. **파라미터 수 폭발 및 과적합 문제**  
   → pLSI는 문서 수 $M$에 비례하는 파라미터를 학습해야 한다.  

> 이에 본 논문은 **Latent Dirichlet Allocation (LDA)** 를 제안하였다.<br>
> LDA는 각 문서를 **여러 주제의 확률적 혼합**으로 표현하는 **3계층 계층적 베이즈 모델(Hierarchical Bayesian Model)** 이다.

### Bayesian Model

**베이즈 모델(Bayesian Model)** 은 데이터가 생성되는 확률적 과정을 **사전(prior)**, **우도(likelihood)**, **사후(posterior)** 의 세 관계로 설명하는 통계적 틀이다.

1. **사전분포 (Prior)**  
   - 데이터가 관측되기 전에 파라미터에 대한 믿음(불확실성)을 확률분포로 표현  
   - 예: $\theta \sim \mathrm{Dir}(\alpha)$

2. **우도 (Likelihood)**  
   - 주어진 파라미터로부터 데이터가 생성될 확률  
   - 예: $w_n \sim p(w_n \mid \theta, \beta)$

3. **사후분포 (Posterior)**  
   - 데이터를 관찰한 후 파라미터에 대한 수정된 믿음  
   - 베이즈 정리로 계산:
     $$
     p(\theta \mid w) = \frac{p(w \mid \theta)\, p(\theta)}{p(w)}
     $$

이 구조를 이용하면 **모수의 불확실성까지 모델링**할 수 있으며 새로운 데이터가 등장해도 **사후→사전 갱신(posterior update)** 을 통해 자연스럽게 적응한다.

> 즉, LDA는 “문서가 주제의 혼합으로부터 생성된다”는 과정을  <br>
> 베이즈적 확률 모델로 표현한 것이다.


## Method

### 1. LDA의 개념

LDA는 각 문서가 $K$개의 잠재 주제(topic)로 이루어져 있다고 가정한다.<br>
각 주제는 단어 분포로 표현되며, 문서는 이 주제들의 혼합으로 구성된다.

$$
\text{Document} \sim \sum_{k=1}^{K} \theta_k \cdot \text{Topic}_k, \quad
\text{Topic}_k \sim \text{Multinomial}(\beta_k)
$$



여기서 $\theta$는 문서별 주제 비율이며, $\beta_k$는 주제별 단어 분포이다.


### 2. Generative Process

각 문서 $d$에 대해 다음 과정을 따른다:

1. 문서 길이 $N_d \sim \text{Poisson}(\xi)$  
2. 주제 분포 $\theta_d \sim \text{Dir}(\alpha)$  
3. 각 단어 $n = 1, \dots, N_d$에 대해  
   - 주제 선택: $z_{dn} \sim \text{Multinomial}(\theta_d)$  
   - 단어 선택: $w_{dn} \sim p(w_{dn} | z_{dn}, \beta)$  

즉, 단어는 문서 내 주제 분포 $\theta_d$에 따라 선택된 주제 $z_{dn}$로부터 생성된다.

---

### 🧩 **3. 수식 표현**

하나의 문서에 대한 결합 확률은 다음과 같다.

$$
p(\theta, z, w | \alpha, \beta)
= p(\theta | \alpha) \prod_{n=1}^{N} p(z_n | \theta) p(w_n | z_n, \beta)
$$

이를 적분 및 합산하여 문서의 주변확률을 얻는다.

$$
p(w | \alpha, \beta)
= \int p(\theta | \alpha) 
\prod_{n=1}^{N} \sum_{z_n} p(z_n | \theta)p(w_n | z_n, \beta)
\, d\theta
$$

---

### 🧮 **4. 그래픽 모델**

<p align="center">
<img src="https://miro.medium.com/v2/resize:fit:1200/1*Wn8O6qDMPNsM0YmXUyXyZg.png" width="500"/>
</p>

- **α, β**: corpus-level 파라미터  
- **θ**: 문서 수준 주제 분포  
- **z, w**: 단어 수준의 숨은 주제와 관측 단어  

> 문서가 하나의 주제에 고정되지 않고, 여러 주제의 혼합으로 표현된다는 점이 LDA의 핵심이다.

---

### 🔍 **5. Variational Inference**

후행 분포 $p(\theta, z | w, \alpha, \beta)$ 는 직접 계산이 불가능하다.  
따라서 **Variational Inference** 를 통해 근사한다.

근사 분포를 다음과 같이 정의한다:

$$
q(\theta, z | \gamma, \phi)
= q(\theta | \gamma) \prod_{n=1}^{N} q(z_n | \phi_n)
$$

이때 KL divergence를 최소화하는 $\gamma, \phi$를 찾는다.

$$
(\gamma^\*, \phi^\*) 
= \arg\min_{\gamma, \phi} D_{KL}
\big(q(\theta, z | \gamma, \phi) \,||\, p(\theta, z | w, \alpha, \beta)\big)
$$

업데이트 규칙은 다음과 같다:

$$
\phi_{ni} \propto \beta_{i w_n} \exp\{ \Psi(\gamma_i) - \Psi(\sum_j \gamma_j)\}
$$

$$
\gamma_i = \alpha_i + \sum_{n=1}^{N} \phi_{ni}
$$

---

### 🧠 **6. Parameter Estimation (EM Algorithm)**

1. **E-step:** 각 문서별 $(\gamma_d, \phi_d)$ 업데이트  
2. **M-step:**  
   $$
   \beta_{ij} \propto \sum_d \sum_n \phi_{dni} w_{dn}^j
   $$
   $\alpha$는 Newton-Raphson 방법으로 갱신된다.

이 과정을 반복하여 수렴시킨다.

---

## **Experiment**

### 📚 **Dataset**

| 데이터셋 | 설명 |
|-----------|------|
| **TREC AP** | 16,000 뉴스 기사, 23,000 단어 |
| **C. Elegans** | 5,225 논문 초록 |
| **EachMovie** | 영화 추천 협업 필터링 데이터 |

---

### 🔬 **비교 모델**

| 모델 | 설명 |
|------|------|
| **Unigram** | 단일 다항분포 모델 |
| **Mixture of Unigrams** | 문서별 단일 주제 |
| **pLSI** | 문서-단어 쌍 기반 확률 모델 |
| **LDA** | 문서별 주제 혼합 + 생성 가능 모델 |

---

### 📈 **결과: Perplexity 비교**

<p align="center">
<img src="https://miro.medium.com/v2/resize:fit:1200/1*RpVqvAip9oD9VyMBQYDWvA.png" width="520"/>
</p>

- **LDA**는 pLSI보다 **일관되게 낮은 Perplexity**를 기록  
- Mixture of Unigrams은 $k$ 증가 시 **극심한 과적합**  
- pLSI는 파라미터 수 $O(Mk)$ → **데이터셋 크기에 비례**  
- LDA는 $O(kV)$로 **corpus 크기에 무관한 안정적 성능**

---

### 🎨 **Topic 예시**

| Topic | 상위 단어 (Top Words) |
|-------|------------------------|
| **Arts** | film, music, actor, play, opera |
| **Budgets** | tax, federal, congress, billion |
| **Children** | family, parents, care, child |
| **Education** | school, teacher, students, public |

> 주제별 단어 분포 $\beta_k$가 의미 있는 클러스터로 수렴함을 확인할 수 있다.

---

## **Conclusion**

LDA는 기존 pLSI의 구조적 한계를 극복한  
**완전한 확률 생성 모델(generative probabilistic model)** 이다.  

- **문서 생성 모델링 가능** (unseen 문서에도 확률 할당)  
- **주제 혼합 표현**으로 문서의 다중 의미를 포착  
- **변분추론 기반 EM 학습**으로 대규모 corpus에 적용 가능  

LDA는 이후 수많은 확장 모델의 토대가 되었다.  
예를 들어,  
**CTM (Correlated Topic Model)**, **HDP (Hierarchical Dirichlet Process)**,  
**DTM (Dynamic Topic Model)**, **Neural LDA**, **BERTopic** 등이 모두  
LDA의 베이즈적 틀 위에서 발전했다.

---

## 🧠 **Discussion & Conclusion**

| 항목 | 설명 |
|------|------|
| 🎯 **핵심 아이디어** | 문서 = 주제의 확률적 혼합 |
| 📊 **수학적 구조** | Dirichlet–Multinomial conjugacy |
| 💡 **장점** | 해석력, 확장성, unseen 문서 대응 |
| ⚠️ **한계** | bag-of-words 가정 → 문맥/순서 정보 손실 |
| 🔮 **후속 연구** | CTM, DTM, HDP, Neural LDA, BERTopic 등 |

---

> “LDA는 ‘단어의 집합’ 속에서 *의미의 구조*를 발견하려는  
> 베이즈 확률론적 언어 모델의 출발점이다.” — *Blei et al., 2003*

