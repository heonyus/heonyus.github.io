---
layout: post
title: "Finding Structure in Time(1990)"
date: 2025-07-01
category: NLP
tags: [NLP, Recurrent Neural Network, Representation Learning, Cognitive Science]
---
# [논문리뷰] Finding Structure in Time

> Cognitive Science, 1990. [Paper](https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1)<br>
> Jeffrey L. Elman<br>
> University of California, San Diego<br>
> 1990<br>

## Introduction
시간은 인지에서 중요한 요소다.<br>
사람은 말을 순서대로 듣고 이해하지만, 컴퓨터에게 이런 '시간의 흐름'을 이해시키는 건 어렵다.<br>
인공신경망은 시간을 공간의 형태로 즉, 모든 정보를 덩어리로 한번에 처리했다.<br>
이때 길이 제한을 초과한 sequence 길이가 들어오면 처리 할 수 없게 된다.<br>
그리고 `[0, 1, 1, 0]` 이라는 패턴과 `[0, 0, 1, 1]` 이라는 패턴은 사람이 보기에는 같은 `11` 패턴이 이동한 것 이지만, 컴퓨터는 이 둘을 완전히 다른 별개의 데이터로 인식하기 때문에 패턴의 상대적인 시간 구조를 파악하지 못하는 것이다.<br>
<br>
본 논문에서는 시간을 명시적으로 표현하지 않고 처리를 통해서 시간의 효과가 암묵적으로 드러나도록 한다.<br>
<br>

### Jordan Network
초기 순환 신경망(RNN) 구조

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/FslBP1L.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- INPUT: 현재 시점(t)에 들어오는 정보
- HIDDEN: 현재의 INPUT과 과거의 기억(STATE)을 종합해서 실질적인 계산을 수행하는 중간 처리층
- OUTPUT: 현재 시점(t)에 네트워크가 내보내는 최종 결과물
- STATE: 가장 중요한 부분으로, 바로 직전 시점(t-1)의 OUTPUT 값을 그대로 복사해서 저장하는 기억 장치 역할

### Elman Network
본 논문에서 제안하는 순환 신경망(RNN) 구조

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/In4kNra.png
" referrerpolicy="no-referrer" loading="lazy" />
</p>

가장 중요한 특징은HIDDEN UNITS에서 CONTEXT UNITS로 향하는 되먹임(feedback) 연결이다. 

- 계산: 현재 시점(t)에 HIDDEN UNITS는 INPUT UNITS의 정보와 CONTEXT UNITS에 저장된 과거의 기억을 함께 받아 계산을 수행
- 출력: 이 계산 결과를 바탕으로 OUTPUT UNITS를 통해 결과를 내보냄냄
- 기억 (복사): 동시에, HIDDEN UNITS의 활성화 값(계산 결과)을 그대로 CONTEXT UNITS로 복사해서 저장

> Jordan Network는 Output Unit만 기억하는 반면 Elman Network는 Hidden Unit을 기억함

- 결과물은 압축, 일반화, 단순화 될 수 있음
- 과정은 맥락, 고려사항을 알 수 있음

## Experiment
<br>

### XOR Problem
이 실험은 전통적인 XOR 문제는 두개의 입력이 서로 같을때 1로 출력하는 문제다.

| 입력 1 | 입력 2 | 결과 |
|--------|--------|------|
|   0    |   0    |  0   |
|   1    |   1    |  0   |

- Temporal XOR
    - 본 논문에서는 입력을 순차적으로 하나씩 보여줌
    - `[입력1, 입력2, 결과]` 묶음을 무작위로 계속 이어 붙여서 `101000110...` 과 같은 하나의 긴 데이터 스트림을 만듬


<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/kKPBSIi.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- X축 (Cycle): 시간의 흐름(단계)
- Y축 (Error)

`[입력1, 입력2, 결과]` 순서로 데이터가 들어오고, 네트워크는 매 순간 다음 데이터를 예측해야 한다.<br>
입력1과 입력2를 모두 봤을때 다음 오차가 줄어드는것을 알 수 있다.

### STRUCTURE IN LETTER SEQUENCES
이 실험은 문자의 규칙과 무작위성이 섞인 시계열 데이터에서 숨겨진 구조를 발견하는 문제다.
- 데이터: b, d, g 세 자음을 무작위로 나열한 뒤, 정해진 규칙에 따라 모음을 붙여 인공적인 "음절" 시퀀스를 만듬.

| 자음 | 음절 시퀀스스 | 
|--------|--------|
|   b    |   ba    |
|   d    |   dii    |
|   g    |   guu    |

- 예시: `diibaguuubadiiguuu`
- 규칙: 다음에 올 자음은 무작위, 일단 자음이 나오면 뒤따르는 모음은 규칙적

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/VG0APsU.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 입력 표현: 각 알파벳은 6개의 feature를 나타내는 6 bit-vector로 표현

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/JILS2TP.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

#### 학습

학습 횟수 200번 반복해 학습을 진행한다.<br>
1. t 시점 글자(6 bit-vector) 입력
$$h(t) = \sigma_h (W_{ih} x(t) + W_{ch} h(t-1) + b_h)$$
2. t+1 시점 글자 예측
$$\hat{y}(t) = \sigma_y (W_{ho} h(t) + b_o)$$
3. 실제 정답, 예측값 비교 'error' 계산
$$E(t) = \frac{1}{2} \sum_{j=1}^{6} (x_j(t+1) - \hat{y}_j(t))^2$$
4. Backpropagation
$$W_{\text{new}} = W_{\text{old}} - \eta \frac{\partial E(t)}{\partial W}$$
5. 2~4 반복

#### 결과(CONSONANTAL)
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/UmHqcGV.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 자음 feature 예측: 모음 `(i,i)`가 끝나면 다음엔 어떤 자음이 올지는 몰라도 *자음이 온다*라는 사실 자체는 100% 예측 가능
- 따라서 자음을 나타내는 첫번째 bit에 대한 error는 항상 낮게 유지됨

#### 결과(HIGH)
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/9mWYh0M.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 고모음 feature 예측: 반면 고모음은 다음에 올 자음이 `b`인지 `d`인지에 따라 값이 달라지므로 예측이 어려움
- 따라서 해당 bit의 error가 높음

→ 본 논문에서는 '**순차적 입력**'이라는 문제를 '**순환 구조가 제공하는 기억**'이라는 해결책으로 풀었다.

### Discovering the Notion "Word"
이 실험은 띄어쓰기가 없는 연속된 글자 데이터 속에서 '단어'라는 단위를 통계적으로 발견할 수 있는지 확인하는 문제다.<br>
STRUCTURE IN LETTER SEQUENCES 실험에서는 인공적으로 생성한 데이터이고 이번 실험에서는 실제 단어들을 사용한다.<br>

1. 데이터: 15개 단어를 조합한 200개의 문장
2. 연속 스트림화: `theydidnot...`처럼 띄어쓰기가 없는 문자열
3. 벡터 변화: 5-bit vector(무작위 변환)

#### 결과
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/5j5JlK7.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 단어의 시작: 새로운 단어의 첫 글자는 예측하기 매우 어려움(error 높음)
- 단어의 중간/끝: 단어의 일부(예:`a-p-p`)가 주어지면, 나머지 부분(`l-e`)은 예측하기 쉬워짐(error 점차 감소)

→ 예측 오차(prediction error) 그 자체가 매우 유용한 정보가 될 수 있음을 보인다.

### Discovering Lexical Classes from Word Order
이 실험은 네트워크가 단어의 순서 정보만으로 '명사', '동사', '생물', '무생물'과 같은 문법적/의미적 범주를 스스로 학습할 수 있는지를 증명해보는 문제다.<br>

1. 데이터: NOUN-HUM(인간 명사), VERB-EAT(먹는 동사)와 같이 문법 템플릿을 활용해 10,000개의 문장을 만듬(예: man eat cookie)
2. One-Hot Vector: 실험에 사용되는 29개의 단어는 **서로 아무런 유사성 정보가 없는** 데이터(직교 벡터, orthogonal vector)

#### Methology
학습이 완료된 네트워크가 각 단어를 처리할 때의 내부 상태, 즉 은닉 상태 벡터(hidden state vector)를 분석한다.
- 분석을 위해, 각 단어가 여러 문맥에서 가졌던 은닉 상태 벡터들의 평균값을 계산하여 해당 단어의 대표 벡터로 사용
$$\bar{h}_w = \frac{1}{N_w} \sum_{t \in T_w} h(t)$$
- 계층적 클러스터링(Hierarchical Clustering) 기법을 사용해, 네트워크 내부에서 서로 유사하게 표현되는(가깝게 위치하는) 단어들끼리 그룹으로 묶어 관계를 시각화

#### 결과
<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/10S2iUq.png" referrerpolicy="no-referrer" loading="lazy" />
</p>
네트워크는 입력값에 아무런 힌트가 없었음에도 불구하고, 단어들의 의미적/문법적 범주를 벽하게 학습하여 위위 같은 계층 구조로 정리됐다다.

- 최상위 레벨: 명사(Nouns)와 동사(Verbs)를 가장 먼저 구분
- 명사 그룹: 그 안에서 생물(Animates)과 무생물(Inanimates)을 구분
- 생물 그룹: 사람(Human)과 동물(Animals)
- 무생물 그룹: 음식(Food)과 부서지는 것(Breakables)

→ 이는 '문법'이나 '의미'와 같은 추상적인 지식은 반드시 명시적인 규칙으로 주어질 필요가 없다는 것을 보여준다.<br>
→ 단어들이 사용되는 통계적 패턴 속에 이미 암묵적으로 녹아 있으며, '다음 단어 예측'이라는 단순한 과제를 수행하는 순환 신경망(RNN)은 그 숨겨진 구조를 스스로 발견하여 내재화할 수 있음을 증명했다.<br>

### TYPES, TOKENS, AND STRUCTURED REPRESENTATIONS

Discovering Lexical Classes from Word Order의 후속 연구로서 네트워크가 단어의 일반적인 의미(유형)와 문맥 속에서의 구체적인 의미(토큰)를 어떻게 동시에 표현하는지를 증명하는 실험<br>

- 이전 실험에서 'man', 'boy' 등이 하나의 그룹으로 묶인 것은 네트워크가 **유형(Type)**을 학습

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/Dp7lIT8.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

위 실험에서 더 나아가 'boy'라는 개별 token에서 type 영역 안에 무작위 하게 흩어져 있는 것이 아닌 **문법적 맥락(grammatical context)**에 따라 그룹핑 돼 있었다.

- 문장 맨 앞에 주어로 사용된 'boy'의 모든 토큰들은 서로 가깝게 묶임
- 동사 뒤에 목적어로 사용된 'boy'의 토큰들은 또 다른 그룹으로 묶임

<p align="center">
  <img alt="Figure 1" src="https://i.imgur.com/H6i4NwF.png" referrerpolicy="no-referrer" loading="lazy" />
</p>

- 놀라운 점은 'boy'의 내부 구조와 'girl'의 내부 구조가 거의 평행하게, 동일한 방식으로 조직화되었다는 것

→ 이는 네트워크가 '주어'나 '목적어'와 같은 문법적 역할에 따라 단어의 의미를 체계적으로 변화시키는 일반적인 규칙을 학습했음을 의미한다.

## Conclusion

본 논문의 결론에서는 처음으로 살펴봤던 **시간적 문제의 본질**에 대해 언급한다.<br>
그리고 데이터에 **내부적 구조(subregularities)**가 존재한다면 네트워크는 이를 활용해 인간과 비슷한 부분 예측을 수행함으로써 학습이 더 잘되는 효과가 있다고 한다.<br>
