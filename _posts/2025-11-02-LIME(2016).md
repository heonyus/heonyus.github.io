---
layout: post
title: "LIME(2016)"
date: 2025-11-02
description: "Why Should I Trust You? Explaining the Predictions of Any Classifier"
tags:
  - XAI
---
# Introduction
머신러닝 모델은 높은 정확도를 보여도 왜 **특정 예측**을 했는지 이해하기가 어렵죠.
그래서 논문 이름도 "왜 내가 너를 믿어야해?" 라고 지은 것 같습니다.

본 논문에서는 거의 모든 모델에서 적용가능한(agnostic) 기법인 LIME(Local Interpretable Model-Agnostic Explanations)을 제안합니다.

목표는 다음과 같습니다

- 모델의 로컬 충실도(local fidelity) 확보
- 사람이 이해할 수 있는(simple & interpretable) 설명 제공

> LIME은 복잡한 모델의 예측값을 그 주변(local)에서 단순한 모델로 근사하여 설명하는 접근방식

# 핵심 Idea

<img src="https://miro.medium.com/v2/resize%3Afit%3A1400/1%2A3GbQtBmxz_n1m2EKLkzDwQ.png" alt="local linear approximation" width="800" referrerpolicy="no-referrer">

LIME의 핵심은 로컬 선형 근사(local linear approximation)입니다.
다시 풀어서 얘기해보자면 특정 입력 $x$ 주변에서 원래 모델 $f$의 출력을 잘 따라가는 **단순한 모델 $g$**를 학습하는 것입니다.

$$
\xi(x) = \arg\min_{g \in G} L(f, g, \pi_x) + \Omega(g)
$$

- $f$: 원래 복잡한 모델 (black-box)
- $g$: 단순하고 해석 가능한 모델 (예: linear model, decision tree 등)
- $L(f, g, \pi_x)$: 원래 모델 $f$와 단순한 모델 $g$의 오차 (local loss)
- $\Omega(g)$: 단순한 모델 $g$의 복잡도 (특성 수 제한, regularization)
- $\pi_x(z)$: 샘플 $z$가 입력 $x$와 얼마나 가까운지 나타내는 함수값 (가중치)

# Step by Step

자, 우리는 지금 LIME의 핵심 수식을 알아봤습니다.
그럼 이제 `어떻게`를 알아봅시다.

LIME은 복잡한 입력을 사람이 이해하기 쉬운 형태로 단순화한 뒤 입력 근처에서 여러 교란 샘플(perturbation sample)을 만들어 모델 반응을 관찰합니다.
그 샘플들에 거리 기반 가중치를 주어 단순한 선형 모델로 근사하고 중요한 특성만 선택해 해석력을 높입니다.
이를 통해 “모델이 왜 이런 예측을 했는가”를 직관적으로 설명할 수 있습니다.

## 설명가능한 입력 공간

복잡한 입력을 사람이 이해하기 쉬운 형태로 바꿉니다.

- 텍스트 -> 각 단어의 존재 여부(binary)
- 이미지 -> 슈퍼픽셀(superpixel)의 on/off 상태

이를 $x'$라 하며 원래 입력 $x$의 단순화된 표현입니다.

$$
x' = [1, 0, 1, 1,...]
$$

binary vector의 각 원소는 특징이 켜짐(1) 또는 꺼짐(0)을 의미합니다.

## 샘플 주변 데이터 생성

<img src="https://i.imgur.com/KYVkK3t.png" alt="perturbation sampling" width="400" referrerpolicy="no-referrer">

설명하고 싶은 입력 $x$ 주변에서 다양한 변형 샘플 $z'$을 생성(perturbation sampling)합니다.

- 텍스트: 일부 단어 제거
- 이미지: 일부 슈퍼픽셀 masking

그 후 $z'$를 원래 공간으로 복원하여 모델 $f$의 예측값을 얻습니다.

$$
\text{Dataset: } {(z', f(z))}_{1}^{N}
$$

여기서 각 샘플은 로컬 가중치 $\pi_x(z)$를 가지게 됩니다.

## 로컬 가중치

그럼 로컬가중치는 어떻게 생기는지 알아봅시다.
$x$에서 멀리 떨어진 샘플은 설명에 영향을 적게 줘야합니다.
이를 위해서 가우시안 커널 기반의 가중치 함수를 정의합니다.

$$
\pi_x(z) = \exp\left(-\frac{D(x, z)^2}{\sigma^2}\right)
$$

- $D(x,z)$: 거리 (예: 코사인, 유클리드)
- $\sigma$: 지역 범위 조절 파라미터

즉, $x$와 가까운 샘플일 수록 가중치가 커지게 된다라는 겁니다.

## 설명 모델 학습

이제 perturbation sample과 local weight를 이용해 단순 모델 $g$를 학습합니다.
가장 일반적으로 **희소 선형 모델(sparse linear model)**을 사용합니다.

$$
L(f,g,\pi_x) = \sum_{z} \pi_x(z) \big( f(z) - g(z') \big)^2
$$

위 loss를 최소화하면서 **feature 개수 제한(희소성)**을 둡니다.

$$
\Omega(g) = \lvert w \rvert_0 \leq K
$$

즉, $K$개 이하의 중요한 feature만 선택합니다.
학습과정은 다음과 같습니다.

1. $N$개의 샘플 $(z', f(z))$을 생성
2. 가중 최소제곱 회귀(weighted least squares) 수행
3. LASSO를 통해 상위 $K$개의 feature만 유지

# SP-LIME

LIME은 한 샘플($x$)에 대한 로컬 설명만 제공합니다.
하지만 모델 전체를 이해하려면 **대표 샘픔 집합**이 필요합니다.

SP-LIME(Submodular Pick LIME)은 여러 설명 중 서로 다른 부분을 잘 덮는(submodular coverage) 샘플을 선택합니다.

## Submodular Pick의 필요성

저자들은 “모델 전반”을 이해하기 위해 대표적인 샘플 집합을 선택하려 했습니다.

> "We need to pick a subset of instances such that their explanations represent the model as a whole."

즉, 각 샘플마다 LIME으로 feature-importance를 얻고 그중 중복되지 않게 모델의 다양한 측면을 보여주는 샘플들을 선택하는 문제입니다.
이게 바로 조합최적화(combinatorial optimization) 문제입니다.

## Submodular Function

- 각 샘플의 설명을 행렬 $W$로 표현(feature * example)
  - example: LIME을 각 데이터 샘플마다 한 번씩 적용해서 얻은 “로컬 설명 결과”
- 덮개 함수(coverage function) $c(V)$를 최대화하는 문제를 정의

$$
\max_{V, \lvert V \rvert \leq B} c(V) = \sum_j \mathbb{1}[\exists i \in V, W_{ij} > 0]
$$

- $V$: 선택된 샘플 집합
- $B$: 선택할 샘플 개수
- $W$: 각 샘플의 설명을 행렬로 표현(feature * example)
- $W_{ij}$: 샘플 $i$에서 feature $j$의 중요도
- $c(V)$: 선택된 샘플 집합 $V$가 덮은 부분(coverage)

이는 submodular greedy algorithm을 사용하여 근사할 수 있습니다.

### submodular greedy algorithm

집합 함수 $f(S)$가 submodular하다는 것은
“이미 많이 선택했을수록, 새로운 원소를 추가했을 때 얻는 이익이 줄어드는” 성질을 갖는다는 뜻입니다.

즉, 이 말은 이미 LIME으로 $g$를 통해 $z$들의 설명을 많이 선택했다라면 새로운 샘플 $z_1$을 추가해도 새롭게 덮는 feature가 많지 않을 가능성이 크다라는 것입니다.

$$
f(A \cup {z_1}) - f(A) \ge f(B \cup {z_1}) - f(B) \quad \text{if } A \subseteq B
$$

- **$A$**: 적게 선택된 집합
- **$B$**: 이미 많이 선택된 집합

같은 원소 $z_1$을 추가하더라도 $B$에 추가할 때의 이득이 $A$에 추가할 때보다 작거나 같다라는 겁니다.

→ LIME이 가장 높은 충실도를 보임

# Experiments
## Faithfulness (충실도)
단순 모델이 원래 모델의 중요 feature를 얼마나 잘 복원하는지 평가합니다.

| 방법     | 충실도 리콜(%) |
| ------ | --------- |
| LIME   | **91.2**  |
| Parzen | 83.5      |
| Greedy | 70.4      |

## Model Trust (신뢰 판단)
"이 예측을 믿을 수 있나?"를 테스트하기 위해 일부 feature를 불신하는 feature로 지정합니다.

| 방법     | F1 (신뢰 판단) |
| ------ | ---------- |
| LIME   | **0.92**   |
| Greedy | 0.75       |
| Random | 0.64       |

→ LIME이 잘못된 모델의 신뢰도를 가장 잘 구별

## 모델 비교 (Model Selection)

비슷한 정확도의 두 모델을 제시하고, 사용자가 설명만 보고 어느 쪽을 배포할지 판단합니다.

- 결과: SP-LIME > RP-LIME > Greedy 순으로 더 정확하게 “좋은 모델”을 선택함

# Conclusion

1. **신뢰(trust)를 위한 설명의 중요성**

   * 복잡한 블랙박스 모델들이 널리 쓰이고 있지만, 사용자가 모델이나 개별 예측을 충분히 이해하지 못하면 신뢰할 수 없습니다.
   * 따라서 단순히 높은 정확도를 보이는 것만으로는 충분하지 않으며, *왜* 그런 예측을 했는지를 설명할 수 있어야 합니다.

2. **로컬 설명을 통한 모델 해석(Local Interpretable Model‑agnostic Explanations, LIME)**

   * LIME은 어떠한 분류기나 회귀 모델이라도 **해석 가능한 형태(interpretable representation)**로 변환하고, 특정 입력 주변에서 그 모델을 **국소적으로 근사(local approximation)** 합니다.
   * 이렇게 하면 사용자는 “이 예측이 왜 나왔는가”에 대해 직관적으로 이해할 수 있습니다.

3. **모델 전체를 요약하는 방법(Submodular Pick LIME, SP-LIME)**

   * 단일 입력에 대한 설명만으로는 모델 전체의 행동을 파악하기 어렵습니다.
   * 이에 저자들은 여러 개의 설명을 골라내는 방식으로 대표적이고 중복되지 않는 설명 집합을 만들기 위해 **부분모듈러(submodular) 최적화**를 이용했습니다.
   * 이는 “모델이 대체로 어떤 기준으로 작동하는가”에 대한 더 넓은 통찰을 제공해줍니다.

4. **실험을 통한 유효성 확인**

   * 저자들은 텍스트·이미지 데이터에 대해 다양한 모델을 대상으로 LIME과 SP-LIME을 적용하고, 사용자 실험(human subject experiments)에서도 이 설명 방식이 *모델 선택*, *불신 모델 개선*, *예측 신뢰 판단* 등 여러 과제에서 효과적이었음을 보여주었습니다.

# Appendix
[코드 구현](https://github.com/heonyus/ai-implementation/tree/main/LIME)