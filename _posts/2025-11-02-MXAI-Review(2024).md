---
layout: post
title: "MXAI Survey(2024)"
date: 2025-11-02
description: "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future"
tags:
  - XAI
  - Multimodal
---
# Introduction
본 논문은 Multimodal eXplainable Artificial Intelligence (MXAI) 방법론의 과거, 현재, 미래를 역사적 관점에서 리뷰합니다.
AI 모델이 커지면 커질 수록 그리고 복잡해지면 복잡해질수록 모델의 블랙박스 특성을 이해하려는 XAI의 중요성이 더 커지고 있죠.
최근 들어서는 다양한 modality의 데이터를 처리하는 multimodal의 환경에서 연구가 진행되고 있습니다.

## MXAI 방법론의 역사적 순서
저자들은 MXAI 방법론을 4가지 역사적 시대로 나누어 설명합니다.

  1. **The Era of Traditional Multimodal Learning(2000 ~ 2009)**
      - ImageNet 데이터셋 공개 이전시기
      - 제한적인 구조화된 데이터를 기반으로 수동적인 feature engineering과 rule-based 시스템에 중점을 두었음
      - Decision Tree와 같은 본질적으로 해석 가능한 모델이 주로 사용됐음
      - 평가 또한 정확성과 인간 중심 지표에 집중
      - Scalability와 유연성에는 한계가 있었음

  2. **The Era of Deep Learning(2010 ~ 2016)**
      - ImageNet의 등장과 함께 DNN이 부상한 시기
      - CNN과 RNN 계열의 다양한 딥러닝 아키텍쳐를 겹합하여 multimodal 데이터를 처리하고 해석하는 방법을 개발
      - joint representation learning에 초점을 맞춤
      <img src="https://i.imgur.com/UlFlIjZ.png" alt="joint representation" width="800" referrerpolicy="no-referrer">
        - joint representation learning: 위 figure와 같이 각각의 단일 네트워크에서 학습된 representation을 공통된 semantic subspace에 projection하는 것을 목표로함
        - 이를 통해 shared subspace에 위치하는 single vector를 얻게 됨
    
  3. **The Era of Discrimizative Foundation Models(2017 ~ 2021)**
      - Transformer 모델과 CLIP과 같은 foundation 모델의 확산
      - 이 시대 MXAI 방법론은 attention visualization, feature attribution, gradient-based method와 같은 기술을 활용했음

  4. **The Era of Generative Large Language Models(2022 ~ 2024)**
      - ChatGPT와 같은 generative LLM의 등장으로 촉발된 시기
      - chatGPt나 Claude, Gemini같은 모델들은 직접적인 접든이 제한적이기에 LLM과의 상호작용성을 활용한 방법론이 필요함
      - 적응형 설명을 강화하고 multimodal 데이터 및 출력의 복잡성을 관리하며 편향(bias)같은 윤리적 문제 해결을 진행

## MXAI 방법론의 범주
각 시대별 MXAI 방법론은 다시 세 가지 범주로 세분화 될 수 있습니다

<img src="https://i.imgur.com/snlOh02.png" alt="Figure 1" width="800" referrerpolicy="no-referrer">

  1. **Data Explainability(데이터 설명가능성 )** 
      - 데이터 수집, 분석, 정제 등 모델 학습 이전에 데이터를 요약하고 분석해 인사이트를 제공
      - 예를 들어 Traditional ML 시대에서는 PCA와 같은 차원 축소 기법이 여러 modality에 걸쳐 feature의 직교성과 차원 축소를 위해 사용됨
      - DL 시대에서는 데이터 품질분석(additive noise models 등)과 데이터 상호작용 분석(Seq2Seq model 등)을 통해 데이터의 무결성과 일관성을 보장
      - Discrimizative Foundation Models 시대에는 multimodal 데이터셋을 분석하여 dataset noise 및 bias 문제를 해결
      - Generative LLMs 시대에는 LLM을 활용하여 데이터셋을 설명하고 Multimodal C4와 같은 엄격한 데이터 필터링을 통해 데이터셋 품질과 다양성을 높임
        - Graph Convolutional Network(GCN)를 활용하여 이미지 내 객체 관계를 추론하는 등 구조적 관계 구축
        - Graph Modeling을 통해 복잡한 데이터 관계를 명시적으로 나타내 모델의 해석 가능성을 향상
  
  2. **Model Explainability(모델 설명가능성)**
      - 모델의 내부 구조와 작동 방식을 structural explanation 및 behavior explanation을 통해 밝혀냄
        - structural explanation: 구조에 따라; 모듈의 ‘역할’을 실험(헤드 제거/패칭 등)로 규명, 모델이 규칙/계수로 스스로 설명
        - behavior explanation: 행동에 따라; 단일 샘플에서 시각화/기여도, tabular data에서 열별 중요도(샘플/국소/local 기여도)
      - Traditional ML 시대에는 Logistic Regression 및 Decision Tree와 같이 본질적으로 투명한(inherently interpretable) 모델이 주로 사용됐음
      - DL 시대에는 unit response visualization, deconvolutional networks를 통한 decomposability 그리고 attention-based networks 및 disentangled representation을 통해 모델의 학습과정을 설명하려고 함
        - unit response visualization: 특정 뉴런/채널/필터가 어떤 입력(패턴)에 반응하는지를 직접 시각화해보는 방법
          - 실제 이미지를 통과시키며 레이어 활성값을 살펴봄 -> 픽셀을 역최적화해 그 뉴런이 최대 반응을 하는 입력 생성(activation maximization); [Understanding Neural Networks Through Deep Visualization](https://arxiv.org/abs/1506.06579?utm_source=chatgpt.com)
        - deconvolutional networks: conv-net 내부의 mid-level feature가 원 이미지의 어떤 부분에 대응하는지 역투영(deconv)으로 보여서 구성요소 단위로 해부(decompose)
          - DeconvNet으로 mid-level feature를 이미지 space로 투사해 필터가 감지하는 edge/pattern 등을 해석; [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901?utm_source=chatgpt.com)
        - attention-based networks: 어텐션 가중치를 시각화해 모델이 어디/무엇에 주목했는지를 설명 근거로 제시
          - neural machine translation, image captioning 등에서 정렬(alignment) 자체가 해석 포인트
          - 단어 <-> 단어 alignment를 어텐션으로 학습 및 시각화; [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com)
        - disentangled representation:  잠재변수의 각 차원이 하나의 생성 요인(각도, 조명, 머리카락 유뮤 등)에 대응하도록 학습해 설명 가능한 축을 갖게 하는 표현학습
          - [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657?utm_source=chatgpt.com)

      - Discrimizative Foundation Models 시대에는 Transformer의 probing task, ablation study, token attribution, attention weight analysis 밑 CLIP 모델 시각화와 input perturbation과 같은 behavioral explanation 방법을 사용
        - 
        

          
 

      