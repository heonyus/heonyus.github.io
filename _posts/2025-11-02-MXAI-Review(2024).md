---
layout: post
title: "MXAI Survey(2024)"
date: 2025-11-02
description: "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future"
tags:
  - XAI
  - Multimodal
---
# Introduction
본 논문은 Multimodal eXplainable Artificial Intelligence (MXAI) 방법론의 과거, 현재, 미래를 역사적 관점에서 리뷰합니다.
AI 모델이 커지면 커질 수록 그리고 복잡해지면 복잡해질수록 모델의 블랙박스 특성을 이해하려는 XAI의 중요성이 더 커지고 있죠.
최근 들어서는 다양한 modality의 데이터를 처리하는 multimodal의 환경에서 연구가 진행되고 있습니다.

## MXAI 방법론의 역사적 순서
저자들은 MXAI 방법론을 4가지 역사적 시대로 나누어 설명합니다.

  1. **The Era of Traditional Multimodal Learning(2000 ~ 2009)**
      - ImageNet 데이터셋 공개 이전시기
      - 제한적인 구조화된 데이터를 기반으로 수동적인 feature engineering과 rule-based 시스템에 중점을 두었음
      - Decision Tree와 같은 본질적으로 해석 가능한 모델이 주로 사용됐음
      - 평가 또한 정확성과 인간 중심 지표에 집중
      - Scalability와 유연성에는 한계가 있었음

  2. **The Era of Deep Learning(2010 ~ 2016)**
      - ImageNet의 등장과 함께 DNN이 부상한 시기
      - CNN과 RNN 계열의 다양한 딥러닝 아키텍쳐를 겹합하여 multimodal 데이터를 처리하고 해석하는 방법을 개발
      - joint representation learning에 초점을 맞춤
      <img src="https://i.imgur.com/UlFlIjZ.png" alt="joint representation" width="800" referrerpolicy="no-referrer">
        - joint representation learning: 위 figure와 같이 각각의 단일 네트워크에서 학습된 representation을 공통된 semantic subspace에 projection하는 것을 목표로함
        - 이를 통해 shared subspace에 위치하는 single vector를 얻게 됨
    
  3. **The Era of Discrimizative Foundation Models(2017 ~ 2021)**
      - Transformer 모델과 CLIP과 같은 foundation 모델의 확산
      - 이 시대 MXAI 방법론은 attention visualization, feature attribution, gradient-based method와 같은 기술을 활용했음

  4. **The Era of Generative Large Language Models(2022 ~ 2024)**
      - ChatGPT와 같은 generative LLM의 등장으로 촉발된 시기
      - chatGPt나 Claude, Gemini같은 모델들은 직접적인 접든이 제한적이기에 LLM과의 상호작용성을 활용한 방법론이 필요함
      - 적응형 설명을 강화하고 multimodal 데이터 및 출력의 복잡성을 관리하며 편향(bias)같은 윤리적 문제 해결을 진행

## MXAI 방법론의 범주
각 시대별 MXAI 방법론은 다시 세 가지 범주로 세분화 될 수 있습니다

<img src="https://i.imgur.com/snlOh02.png" alt="Figure 1" width="800" referrerpolicy="no-referrer">

  1. **Data Explainability(데이터 설명가능성 )** 
      - 데이터 수집, 분석, 정제 등 모델 학습 이전에 데이터를 요약하고 분석해 인사이트를 제공
      - 예를 들어 Traditional ML 시대에서는 PCA와 같은 차원 축소 기법이 여러 modality에 걸쳐 feature의 직교성과 차원 축소를 위해 사용됨
      - DL 시대에서는 데이터 품질분석(additive noise models 등)과 데이터 상호작용 분석(Seq2Seq model 등)을 통해 데이터의 무결성과 일관성을 보장
      - Discrimizative Foundation Models 시대에는 multimodal 데이터셋을 분석하여 dataset noise 및 bias 문제를 해결
      - Generative LLMs 시대에는 LLM을 활용하여 데이터셋을 설명하고 Multimodal C4와 같은 엄격한 데이터 필터링을 통해 데이터셋 품질과 다양성을 높임
        - Graph Convolutional Network(GCN)를 활용하여 이미지 내 객체 관계를 추론하는 등 구조적 관계 구축
        - Graph Modeling을 통해 복잡한 데이터 관계를 명시적으로 나타내 모델의 해석 가능성을 향상
  
  2. **Model Explainability(모델 설명가능성)**
      - 모델의 내부 구조와 작동 방식을 structural explanation 및 behavior explanation을 통해 밝혀냄
        - structural explanation: 구조에 따라; 모듈의 ‘역할’을 실험(헤드 제거/패칭 등)로 규명, 모델이 규칙/계수로 스스로 설명
        - behavior explanation: 행동에 따라; 단일 샘플에서 시각화/기여도, tabular data에서 열별 중요도(샘플/국소/local 기여도)
      - Traditional ML 시대에는 Logistic Regression 및 Decision Tree와 같이 본질적으로 투명한(inherently interpretable) 모델이 주로 사용됐음
      - DL 시대에는 unit response visualization, deconvolutional networks를 통한 decomposability 그리고 attention-based networks 및 disentangled representation을 통해 모델의 학습과정을 설명하려고 함
        - unit response visualization: 특정 뉴런/채널/필터가 어떤 입력(패턴)에 반응하는지를 직접 시각화해보는 방법
          - 실제 이미지를 통과시키며 레이어 활성값을 살펴봄 -> 픽셀을 역최적화해 그 뉴런이 최대 반응을 하는 입력 생성(activation maximization); [Understanding Neural Networks Through Deep Visualization](https://arxiv.org/abs/1506.06579?utm_source=chatgpt.com)
        - deconvolutional networks: conv-net 내부의 mid-level feature가 원 이미지의 어떤 부분에 대응하는지 역투영(deconv)으로 보여서 구성요소 단위로 해부(decompose)
          - DeconvNet으로 mid-level feature를 이미지 space로 투사해 필터가 감지하는 edge/pattern 등을 해석; [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901?utm_source=chatgpt.com)
        - attention-based networks: 어텐션 가중치를 시각화해 모델이 어디/무엇에 주목했는지를 설명 근거로 제시
          - neural machine translation, image captioning 등에서 정렬(alignment) 자체가 해석 포인트
          - 단어 <-> 단어 alignment를 어텐션으로 학습 및 시각화; [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com)
        - disentangled representation:  잠재변수의 각 차원이 하나의 생성 요인(각도, 조명, 머리카락 유뮤 등)에 대응하도록 학습해 설명 가능한 축을 갖게 하는 표현학습
          - [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657?utm_source=chatgpt.com)

      - Discrimizative Foundation Models 시대에는 Transformer의 probing task, ablation study, token attribution, attention weight analysis 밑 CLIP 모델 시각화와 input perturbation과 같은 behavioral explanation 방법을 사용
        - Transformer 행동 설명
          - Probing task: 특정 층/헤드가 보유한 언어·지식 속성을 소규모 분류기로 점검
            - 전처리된 이미지-문장 페어에도 확장되어 크로스모달 표현에 담긴 속성을 검증
          - Ablation study: 프리트레이닝 레시피/모듈 제거로 성능 기여도를 계통적으로 분석
            - 예: 크로스모달 입력 제거로 융합의 실효성 점검
          - Token attribution: 입력 토큰별 기여도 산출로 개별 예측 근거를 국소적으로 파악
          - Attention weight analysis: 크로스모달 어텐션 맵을 시각화해 어떤 영역/토큰에 정렬되는지 설명

        - CLIP 행동 설명
          - Visualization / Input perturbation: 텍스트-이미지 정렬이 실제 어느 영역에 의해 좌우되는지 시각화/교란으로 검증
            - 구조 수술(구성요소 교체)·특징 수술(노이즈 억제)로 시각화 오류를 줄이는 CLIP Surgery, 로컬→글로벌 태깅으로 해석력/분류를 높이는 TagCLIP, 객체 공간적 로컬라이제이션 분석 등
          - Structural transparency(구조 투명성): GNN/Knowledge Graph 기반: 장면·객체 관계를 그래프로 구성해 고차 상호작용과 추론 경로를 명시화(VQA, 추천 등)
            - 언어·시각 노드와 릴레이션을 연결하면 링크 예측 자체가 해석 가능한 추론이 됨
          - Causal 프레임: 언어 편향·상관혼동을 **개입(intervention)/반사실(counterfactual)**로 분리해 원인적 기여를 설명
        
  3. Post-hoc Explainability(사후 설명가능성)
  학습 완료 후 예측 근거를 재구성하는 범주로, 멀티모달에서 특히 반사실, 편향 완화, 학습과정 가시화가 중점입니다. 
      - Counterfactual reasoning: 질문/이미지에서 최소 수정으로 답이 바뀌는 사례를 생성·분석해 원인 특성을 특정(VQA/캡셔닝/강화학습 설정까지 확장)
      - Bias mitigation: 데이터 불균형/스퓨리어스 상관을 사후 진단·시각화하고, 교정 데이터·로스·제약을 도입해 공정성을 개선
      - 멀티모달 학습 과정 해석
        - 표현—하이퍼그래프 등으로 모달 간 상호작용을 정량화
        - 추론—언어 편향을 인과효과로 모델링
        - 시각화 툴—어텐션/히든을 대화형으로 탐색
 
# MXAI의 현재

생성형 MLLM은 직접 내부를 들여다보기 어렵기 때문에, 상호작용성을 활용한 과정 설명과 데이터·지식 관리가 핵심 축으로 부상합니다. 

  - *Data Explainability*
    - Explaining datasets: LLM으로 데이터 의미를 요약·시각화하고, 품질·노이즈·편향을 분석
    - Data selection: Multimodal C4처럼 엄격한 필터링(얼굴/비정상 비율/저해상도 제외)로 일반화·해석가능성을 동시 개선
    - Graph modeling: 객체·개념·시간을 노드/엣지로 명시화해 MLLM의 암묵 관계를 명시 관계로 보완

  - *Model Explainability*
    - Process explanation
      - Multimodal ICL: 예시 기반 지시 따르기를 시각·언어에 확장(DEFICS, OpenFlamingo 등), 과제 전이 비용을 줄이며 사람이 읽을 수 있는 설명형 프롬프트를 채택
      - Multimodal CoT: 텍스트/이미지 근거를 단계화해 중간 사유(rationale)를 생성—시각 편향 신호를 단계적으로 주입해 추론 일관성을 높임

    - Inherent interpretability
      - Image–Text: BLIP-2(두 단계 정렬), LLaVA(시각 인스트럭션 튜닝) 등
      - Video–Text: 시간·인과를 담는 데이터셋과 모델(VideoChat 등)로 사건 국소화/사고 추론을 지원
      - Audio–Text: Salmonn, Qwen-audio 등으로 시계열 음향 해석을 통합
      - Multimodal–Text: Flamingo, MM-REACT 류의 프롬프트 설계로 공간 좌표/파일명 같은 비언어 신호를 명시

    - *Explainable Data Augmentation*

      - 강건성 향상: 반사실/희귀 상황을 합성해 스퓨리어스 의존을 줄이고 OOD 민감도를 낮춤(SelTDA 등)
      - 스몰 모델 티칭: 고품질 CoT 합성(T-SciQ 등)으로 소형 멀티모달 학생 모델을 설명 신호로 지도

  - *Post-hoc Explainability*

      - Probing: (분류기 의존/무모수) 내부 표현·모듈 역할을 시험. 비언어 프롬프트가 시각 추론에 역효과를 내는 망각 현상도 보고. 
      - Reasoning: (모듈러, 외부 지식, 피드백) 분리 컴포넌트/상태 유지/피드백 루프로 추론 경로를 외현화. 

      - Example-based: 반사실로 신뢰성 점검, 적대예시로 CoT 취약점과 추론 경로 전이를 분석. 

  - *Evaluation*
    - 멀티모달 설명의 품질 평가는 텍스트·비전·멀티모달 지표로 나뉨  
      - 텍스트 설명: BLEU, METEOR, ROUGE, CIDEr, SPICE + 사람 평가, 최근엔 GPT-guided 메트릭도 병행
      - 시각 설명: IoU(정답 마스크와 중첩)로 시각 근거 정합성 평가
      - 멀티모달: CLIP-Score(문장–이미지 정합), MMEval(비디오 이상 인과 이해)에 특화
      - 대표 과제: VQA-X/ACT-X/SCIENCEQA/BDD-X/VCR 등(텍스트·시각 결합의 설명 라벨 포함)

# Open Challenges
  - 환각(Hallucination) 억제: 반사실 패널티 등 국지 처방을 넘어 자동 샘플 생성/일반화/효율/사용자 피드백 통합이 병목
  - 고차원 비전 처리: 세부·추론 결핍, 크로스모달 융합의 병목을 해석 가능하게 진단·보완해야 함(속도·비용 제약 포함)
  - 인지 정렬: 사람의 멀티모달 인지 과정에 맞춘 설명 구조(주의→근거→추론)를 표준화하고, 실제 과업 맥락에서 행동가능한 설명을 제공해야 함
  - 정답(ground truth) 부재 환경의 평가: 주관성·표현 다양성 때문에 표준 벤치와 설명-일관성 지표의 정교화가 필요

# Conclusion

본 리뷰의 관점은 시대(Traditional → Deep → Discriminative FM → Generative LLMs) × *범주(Data / Model / Post-hoc)*의 2차원 프레임으로 MXAI를 재배열하는 데 있습니다. 실무적으로는 다음이 실질적 체크리스트가 됩니다

  - 데이터 단계: 노이즈·편향 분석과 엄격 필터링/선정, 필요 시 그래프 명시화로 숨은 상관을 드러낼 것
  - 모델 단계: Transformer/CLIP엔 probes/ablation/attribution/attention을 기본 세트로, 복잡 관계는 GNN/KG/인과로 구조화
  - 사후 단계: 반사실·편향완화·대화형 시각화로 예측→근거를 역추적. MLLM 시대엔 ICL/CoT로 과정 자체를 노출하고, 설명형 데이터 증강으로 강건성과 학생 모델 티칭을 병행

결론적으로 MXAI는 “무엇을 설명할 것인가(데이터/모델/사후)”를 각 시대의 도구로 체계화하는 일입니다.
생성형 멀티모달로 넘어오며 상호작용적 과정 설명과 **데이터 거버넌스(선정·그래프화)**의 비중이 급증했습니다.
앞으로는 환각 억제, 고차원 비전 이해, 인지 정렬, 정답 부재 평가의 네 축을 냉정하게 해결하는 팀이 실제 현장에서 신뢰 가능한 멀티모달 시스템을 주도할 것입니다.

      