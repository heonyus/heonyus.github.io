---
layout: post
title: "SHAP(2017)"
date: 2025-11-04
description: "A Unified Approach to Interpreting Model Predictions"
tags:
  - XAI
---

# Introduction

**문제 정의.** 복잡한 *black-box* 모델(예: Ensemble (앙상블), Deep Neural Network (심층신경망))의 **개별 예측 근거(local explanation)**를 **일관되고 공리적**으로 산출하는 방법이 필요합니다. 기존 LIME/DeepLIFT/LRP 등은 서로 다른 가정과 구현을 사용하여 **일관성(consistency)**, **로컬 정확성(local accuracy)**을 항상 보장하지 못합니다. 

**배경/맥락.** 본 논문은 **Additive Feature Attribution (가산적 특성 기여)** 계열을 **통합**하고, 그 안에서 세 가지 간단한 공리(로컬 정확성, 결측무영향, 일관성)를 모두 만족하는 **유일한 해**가 **SHAP (SHapley Additive exPlanations)** 값임을 보입니다. 또한 **Kernel SHAP(모델 비특정)**과 **Deep SHAP(모델 특화, 합성 규칙)**을 제안해 계산효율을 크게 개선합니다. 

**핵심 기여.**

* **통합 관점:** LIME/DeepLIFT/LRP/고전 Shapley 추정 등을 **하나의 가산적 설명 모델**로 재정의. 
* **유일성 증명:** 세 공리를 만족하는 **유일 해가 Shapley 값**임을 정식화(특징별 φ 값). 
* **실용 알고리즘:** **Kernel SHAP**(가중 최소제곱 회귀 + Shapley 커널), **Deep SHAP**(합성 규칙) 제안 및 **사용자 연구/연산비교**로 타 기법 대비 **일관성·표본효율** 우위를 입증. 

**대표 관련 연구(연대/하입 순).**

* **LIME (2016)**: 로컬 선형 모델로 근사, 커널과 정규화는 **휴리스틱**. **일관성 보장 부재.** [Paper/Code]
* **DeepLIFT (2016–2017)**: 기준 입력(reference) 대비 기여 전파. **합성 규칙은 휴리스틱**, max-pooling 취약. [Paper/Code]
* **LRP (2015)**: layer-wise relevance 분해. DeepLIFT와 유사 관점.
* **SHAP (2017)**: **게임이론적 유일성**·**통합**·**효율적 근사** 제시. 

<p align="center"><img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png" referrerpolicy="no-referrer"/></p>
<p align="center"><i>그림 1. SHAP 개념도: 기대 예측값에서 시작해 각 특성이 조건부 추가될 때의 기대치 변화가 φ로 누적됩니다.</i></p>

# Method

## 2.1 Additive Feature Attribution (가산적 특성 기여) – 공통 설명 모델

**정의(식 1).**
$$
g(z') ;=; \phi_0 + \sum_{i=1}^M \phi_i z'_i
$$

* **식 의미:** 단순한 **가산 모델** $g$가 원모델 $f$의 로컬 거동을 설명. 각 특성의 기여도는 $\phi_i$.
* **기호 설명:** $z'\in{0,1}^M$=해석가능 표현(Interpretable representation), $\phi_i$=특성 $i$의 기여, $\phi_0$=베이스라인(base value).
* **직관/해석 과정:** **“켜짐/꺼짐”**(존재/부재) 형태의 단순화 입력 위에서 **선형 합**으로 설명을 구성. LIME/DeepLIFT/LRP/고전 Shapley가 **이 형식**을 공유. 

## 2.2 LIME (Local Interpretable Model-agnostic Explanations)

**목표(식 2).**
$$
\xi ;=; \arg\min_{g\in\mathcal{G}} ; L(f,g,\pi_{x'}) + \Omega(g)
$$

* **식 의미:** 원모델 $f$를 로컬 근방에서 $g$로 근사하며, **커널 가중** $\pi_{x'}$로 로컬 중요도를 반영하고, 복잡도 $\Omega$를 제어.
* **기호 설명:** $L$=로컬 손실(보통 가중 최소제곱), $\pi_{x'}$=로컬 커널, $\Omega$=복잡도(예: L1/L2), $x'$=해석가능 표현.
* **직관/해석 과정:** **근방 샘플링**으로 $g$를 피팅. 단, LIME의 기본 커널/정규화 선택은 **휴리스틱**이며 **Shapley 공리 불만족** 가능. 

> **LIME류 필수 포인트 요약.**
>
> * **해석가능 표현** $x'$: 입력을 **존재/부재**로 단순화(텍스트=단어 존재, 이미지=슈퍼픽셀 존재).
> * **커널** $\pi_x$: 예시 **가중 함수**로서, 예컨대 $\pi_x(z)=\exp!\big(-D(x,z)^2/\sigma^2\big)$ (**확률분포 아님**).
> * **로컬 손실** $L(f,g,\pi_x)$: 커널 가중 최소제곱 등.
> * **복잡도** $\Omega(g)$: **희소성/규모** 제약.
> * **$\pi_x$ vs. $\pi_x(z)$**: **함수 자체** vs. **특정 $z$에서의 함수값**.

## 2.3 Shapley 공리와 유일성

**로컬 정확성(식 5).**
$$
f(x)=g(x')=\phi_0+\sum_{i=1}^M \phi_i x'_i
$$

* **식 의미:** 설명모델 $g$는 **해당 점에서 원모델과 동일 출력**을 보장.
* **기호 설명:** $x!=!h_x(x')$ 매핑으로 $x'$가 $x$를 대표, $\phi_0,\phi_i$는 (식 1)의 계수.
* **직관/해석 과정:** 설명이 **적어도 그 점에선 정확**해야 한다는 최소 요건. 

**결측 무영향(식 6).**
$$
x'_i=0 ;\Rightarrow; \phi_i=0
$$

* **식 의미:** **부재한 특성은 기여도 0**.
* **기호 설명:** $x'_i$=특성 $i$의 존재 플래그.
* **직관/해석 과정:** 결측 특성은 **영향을 주지 않는다**는 자연스러운 공리. 

**일관성(식 7).**
$$
\forall z'!:; f'_x(z')-f'_x(z'\setminus i);\ge;f_x(z')-f_x(z'\setminus i) ;\Rightarrow; \phi_i(f',x)\ge \phi_i(f,x)
$$

* **식 의미:** 어떤 특성 $i$의 **기여가 전반적으로 증가**했다면, 그 **할당값도 감소하지 않아야** 함.
* **기호 설명:** $f_x(z')=f(h_x(z'))$, $z'\setminus i$= $i$를 0으로 세팅한 벡터.
* **직관/해석 과정:** **단조성/일관성**을 규정해, 설명의 **역설적 변화**를 방지. 

**Shapley 유일 해(식 8).**
$$
\phi_i(f,x)=!!\sum_{z'\subseteq x'} \frac{|z'|!,(M-|z'|-1)!}{M!},\Big[,f_x(z')-f_x(z'\setminus i)\Big]
$$

* **식 의미:** 세 공리를 만족하는 **유일한 가산적 특성 기여**가 **Shapley 값**.
* **기호 설명:** $|z'|$=1의 개수, $M$=특성 수, 가중치는 **모든 순열 평균**과 동치.
* **직관/해석 과정:** **모든 특성 순서**에 대해 $i$를 추가할 때의 **한계 기여**를 평균. **유일성**을 확보. 

## 2.4 SHAP 정의와 근사

**조건부 기대 정의(식 9–12).**
$$
\begin{aligned}
&f(h_x(z'))= \mathbb{E}!\left[f(z)\mid z_S\right] \
&= \mathbb{E}*{z*{\bar S}\mid z_S}[f(z)] \
&\approx \mathbb{E}*{z*{\bar S}}[f(z)] \quad\text{(특성 독립 가정)}\
&\approx f!\big([z_S,;\mathbb{E}[z_{\bar S}]]\big)\quad\text{(모델 선형 가정)}
\end{aligned}
$$

* **식 의미:** 임의의 결측 패턴 $S$에서의 **모델 출력 대체값**을 **조건부 기대**로 정의.
* **기호 설명:** $S$=활성 특성 인덱스 집합, $\bar S$=보완 집합.
* **직관/해석 과정:** 임의 결측을 처리 못하는 모델을 위해 **조건부 기대**로 **결측 대체**. 독립/선형은 **근사 가정**. 

**Shapley 커널(정리 2).**
$$
\Omega(g)=0,;;
\pi_{x'}(z')=\frac{(M-1)}{\binom{M}{|z'|},|z'|,(M-|z'|)},;;
L=\sum_{z'\in\mathcal{Z}}!!\big[f(h_x^{-1}(z'))-g(z')\big]^2 \pi_{x'}(z')
$$

* **식 의미:** 위의 **특수 가중 커널**과 **가중 최소제곱**을 쓰면, **선형 회귀 해가 Shapley 값**을 정확히 복원.
* **기호 설명:** $\pi_{x'}$=Shapley 커널, $|z'|=0,M$에서는 **무한 가중**(제약 처리로 대체 가능).
* **직관/해석 과정:** **LIME의 회귀식**을 **Shapley 공리 보존** 방식으로 **정교화** → **Kernel SHAP**. 

**Deep SHAP 합성 규칙(식 13–16).**
$$
m_{x_j}^{f_3}=\frac{\phi_j(f_3,x)}{x_j-\mathbb{E}[x_j]},\quad
m_{y_i}^{f_j}=\frac{\phi_i(f_j,y)}{y_i-\mathbb{E}[y_i]},\quad
m_{y_i}^{f_3}=\sum_j m_{y_i}^{f_j} m_{x_j}^{f_3},\quad
\phi_i(f_3,y)\approx m_{y_i}^{f_3}(y_i-\mathbb{E}[y_i])
$$

* **식 의미:** **구성요소별(선형, max, 활성화 등) Shapley 해**를 **연쇄 규칙**으로 합성해 **전체 네트워크 φ**를 근사.
* **기호 설명:** $m$=승수(multiplier), $f_3$=상위 모듈, $f_j$=하위 모듈.
* **직관/해석 과정:** DeepLIFT의 **역전파 합성 아이디어**를 **Shapley 정합**으로 재정의 → **Deep SHAP**. 

## 2.5 Kernel SHAP 의사코드 (모델 불문)

```text
Algorithm 1: Kernel SHAP (model-agnostic)
Input: black-box model f, instance x, M features, sampling budget T
1: Construct interpretable space z' ∈ {0,1}^M with mapping h_x
2: Initialize dataset D = ∅
3: for t = 1..T do
4:     Sample subset S ⊆ {1..M}; form z'_S with (z'_i=1 if i∈S else 0)
5:     Evaluate y_t ← f(h_x(z'_S))   ▷ use conditional expectation approx.
6:     Compute Shapley kernel weight w_t = (M-1)/[C(M,|S|)*|S|*(M-|S|)]
7:     Append (z'_S, y_t, w_t) to D
8: end for
9: Solve weighted least squares: minimize Σ_t w_t (y_t - (φ0 + Σ_i φ_i z'_{t,i}))^2
10: Enforce constraints at |S|=0 and |S|=M to determine φ0 and Σ_i φ_i
Output: φ = (φ1..φM) and base value φ0
```

# Experiment

**데이터/모델/지표.** 저자들은 (i) **결정트리**에서의 **표본 효율** 비교, (ii) **AMT 사용자 연구**로 **직관 일치성**, (iii) **MNIST CNN**에서의 **마스킹 실험(로그오즈 변화)**를 수행했습니다. 

**표 1. 벤치마크·지표·핵심 결과 요약**

| **셋업**  | **모델/데이터**                 | **지표(단위)**       | **비교군(baselines)**                      | **핵심 결과(굵게)**                                               |
| ------- | -------------------------- | ---------------- | --------------------------------------- | ----------------------------------------------------------- |
| 샘플 효율   | Decision Tree(10f, 3/100f) | 추정오차 vs 함수 평가 횟수 | **Kernel SHAP**, Shapley Sampling, LIME | **Kernel SHAP**가 **적은 평가**로 **더 정확**. LIME은 공리 미준수로 편향 가능.  |
| 사용자 연구  | 합성 규칙/최대값 시나리오             | **인간 설명과의 일치율**  | LIME, DeepLIFT                          | **SHAP 일치율 최상**. 특히 **max 함수**에서 DeepLIFT 약점 보완.            |
| CNN 마스킹 | MNIST ConvNet              | **로그오즈 감소량**     | LIME, DeepLIFT(old/new), **SHAP**       | **SHAP/개선 DeepLIFT**가 **타겟 전환에 더 효과적**으로 픽셀 제거를 유도.         |

**세팅/재현성 포인트.**

* **Kernel SHAP:** Shapley 커널 가중 회귀, $|S|=0,M$ 제약 처리(변수 제거로 구현). 샘플 수 수만 단위에서도 효율적. 
* **Deep SHAP:** 모듈별 해석(선형/Max/활성화) **분석적 Shapley** → 합성. **참조값**은 $\mathbb{E}[x]$ 해석. 
* **하이퍼파라미터:** LIME 대비 **커널·정규화 비휴리스틱화**(정리 2). 사용자 연구는 AMT. CNN은 2 conv + 2 dense + softmax. 

**에블레이션/민감도(핵심).**

1. **커널 선택**: Shapley 커널 사용 시 **일관성 보장** 및 **추정 분산 감소**.
2. **참조값 선택**: Deep SHAP에서 **$\mathbb{E}[x]$ 근처** 참조가 안정적.
3. **독립/선형 가정**: 조건부 기대 근사 품질이 설명 신뢰도에 영향.

**실무적 해석.** **신용/의료/제조** 등 고위험 도메인에서 **공리 보장**은 **규제 준수·신뢰성**의 핵심입니다. **Kernel SHAP**은 **모델 불문**으로 적용 가능하고, **Deep SHAP**은 **딥모델 실시간 분석**에 적합합니다.

# Conclusion

* **핵심 기여 재정리**

  * **가산적 설명 모델**의 **통합**과 **Shapley 유일성** 증명.
  * **Kernel SHAP/Deep SHAP**로 **효율·일관성** 동시 달성.
  * **사용자 연구/실험**으로 **직관 정합성**과 **성능 우위** 확인.
* **한계**

  * **조건부 기대 근사**(독립/선형 가정)의 현실적 제약.
  * **상호작용(Interaction) SHAP** 및 **고차원 희소성**에서의 계산/추정 난도.
* **후속 연구**

  * **정확한 조건부 분포 추정**(copula·생성모델) 기반 SHAP.
  * **상호작용 값(Shapley-Taylor)**과 **온디바이스/스트리밍** 최적화.

# 🧠 Discussion & Conclusion (요약)

본 논문은 **설명 = 또 다른 모델**이라는 관점을 도입하여 **가산적 특성 기여** 계열을 통합하고, **로컬 정확성·결측무영향·일관성**을 만족하는 **유일 해가 Shapley 값(=SHAP)**임을 증명합니다. 핵심 수식은 (i) **가산 모델** $g(z')=\phi_0+\sum \phi_i z'_i$, (ii) **공리 집합**과 **Shapley 폐형식**(식 8), (iii) **조건부 기대 정의**(식 9–12), (iv) **Shapley 커널**을 통한 **가중 회귀 복원**(정리 2)입니다. **Kernel SHAP**은 LIME의 회귀 프레임을 **공리 보존형 커널**로 치환해 **표본 효율**을 개선하고, **Deep SHAP**은 모듈 수준 Shapley 해를 **합성 규칙**으로 결합해 **딥모델**에 실용적입니다. 실험적으로 SHAP은 **인간 직관**과 더 일치하고, **결정트리/CNN** 시나리오에서 **경쟁 기법 대비 안정적 성능**을 보입니다. 실무에서는 **규제/감사 친화성**, **모델 불변성**, **성능-해석성 균형**을 동시에 달성하는 **기본값(baseline)**으로 SHAP을 채택하는 것이 합리적입니다. 

---

## Notes

* **$\pi_x$는 확률분포가 아니라 가중 함수**입니다. 표본 추출 빈도와는 독립적으로 **회귀의 가중치**로만 작동합니다.
* **$x'$(함수)와 $x'(z)$(함수값) 구분**: $\pi_x$는 함수, $\pi_x(z)$는 $z$에서의 스칼라 값. 동일 표기 혼동을 경계하십시오.

---

## References

* Lundberg, S. M., & Lee, S.-I. (2017). **A Unified Approach to Interpreting Model Predictions**. *NeurIPS 2017*. [Paper](https://arxiv.org/abs/1705.07874), [Code](https://github.com/slundberg/shap). 
* Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). **“Why Should I Trust You?” Explaining the Predictions of Any Classifier**. *KDD 2016*. [Paper](https://doi.org/10.1145/2939672.2939778)
* Shrikumar, A., Greenside, P., & Kundaje, A. (2017). **Learning Important Features Through Propagating Activation Differences (DeepLIFT)**. arXiv:1704.02685. [Paper](https://arxiv.org/abs/1704.02685)
* Bach, S. et al. (2015). **On Pixel-Wise Explanations by Layer-Wise Relevance Propagation (LRP)**. *PLOS ONE*, 10(7):e0130140. [Paper](https://doi.org/10.1371/journal.pone.0130140)
* Štrumbelj, E., & Kononenko, I. (2014). **Explaining Prediction Models and Individual Predictions with Feature Contributions**. *KAIS*, 41(3), 647–665. [Paper](https://doi.org/10.1007/s10115-013-0679-x)

---

**데이터셋/지표/베이스라인 명시(요청 변수 반영).**

* **논문:** *A Unified Approach to Interpreting Model Predictions*, 링크: 위 참조, 연도: **2017**
* **분야/태그:** Explainable AI, SHAP, LIME, Deep Learning
* **데이터셋/지표/베이스라인:** MNIST(CNN, 로그오즈 변화), 합성 트리(추정오차 vs 함수평가), AMT 사용자 연구(일치율); **비교군:** LIME, DeepLIFT(old/new), Shapley Sampling
* **배경 연구:** LIME(2016), DeepLIFT(2016–2017), LRP(2015)

---
