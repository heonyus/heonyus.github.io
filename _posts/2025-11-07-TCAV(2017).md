---
layout: post
title: "TCAV(2017)"
date: 2025-11-07
description: "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors"
tags:
  - XAI
  - Vision
---

# Introduction

본 논문의 출발점은 아주 단순해버립니다.

> 이 CNN이 얼룩말이라고 예측해버릴때 진짜로 줄무늬라는 개념 때문에 그렇게 예측한걸까?

기존의 XAI methods는 대부분 pixel이나 feature attribution을 기반으로 설명을 제공합니다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fb772CS%2FbtrdpCz3I58%2FAAAAAAAAAAAAAAAAAAAAABbkNzB0T5tVw2Locc6_15GBWgDiTmXIhL48DO-kkNz3%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DlDhf9rtc3flvPB0B8x4VvVvESEk%253D" alt="Model Interpretability를 위한 기존 접근 방법" width="600" referrerpolicy="no-referrer">

- [Grad-CAM](https://heonyus.github.io/2025/11/Grad-CAM(2017)-copy.html), Integrated Gradients, LRP 같은 방법들은 이 pixel이나 region이 prediction에 얼마나 기여했는지 heatmap으로 보여줌

하지만 사람들은 실제로 이렇게 생각하지 않죠?

> "줄무니가 있으니 얼룩말이네" 이런식으로 생각하기 마련입니다.

이게 무슨 말이냐?

model은 input features, neural activation과 같은 data에 관련된 vector를 즉 pixel이나 feature space에서 계산하고 **사람은 human-interpretable concept**에 관련된 vector를 즉 **human concept space**에서 사고한다라는 겁니다.

저자가 해결하려는 core problem은 다음과 같습니다.
1. model의 inner representation을 사람이 사용하는 high-level concept space로 mapping하는 것
2. 한두 sample 수준(local)이 아니라 이 class 전체에서 이 concept가 얼마나 중요한지를 global하게 평가하는 것
3. 기존 model을 retrain하지 않고(post-hoc) 제 3의 user-defined concept를 유연하게 적용하는 것

두괄식으로 먼저 TCAV의 solution은 다음과 같습니다.

- 특정 concept `C`(ex. stripe, gender 등)에 대한 example image를 모음
- network의 어떤 layer에서 그 concept이 향하는 방향 vector(CAV)를 extract한 다음 그 방향으로 activation을 살짝 밀었을때 class logit이 얼마나 민감한지(directional derivative)를 계산
- 해당 directional derivative이 양수인 sample 비율을 TCAV score(0~1)로 정의해서 concept `C`가 class `y`에 대해 얼마나 중요한지 평가

# Background

## Feature-based vs Concept-based Explaination

### Feature-based Attribution

Grad-CAM, IG, [SHAP](https://heonyus.github.io/2025/11/SHAP(2017).html), LRP 등은 해당 feature나 pixel이 중요하다! 를 말해주는 methods입니다.

해당 methods의 단점으로는 다음 세가지를 얘기해 볼 수 있습니다.

1. 사람이 쓰는 추상적인 개념과 직접적으로 mapping하기 어려움
2. image local attribution이 주라서 "이 class 전체에서 stripe가 중요해??" 와 같은 global한 질문에 답하기 어려움
3. 해석하는 사람이 heatmap을 잘못 읽어버리면 잘못된 인과 해석(illusion)을 할 수도 있음

### Concept-based Attribution

사람은 "줄무늬, 질감, 색, 성별"과 같은 concept 단위로 사고합니다.
그래서 필요한것들을 한번 살펴보자구요.

1. network inner representation space에서 concept를 하나의 방향 벡터로 표현할 방법(CAV; Concept Activation Vectors)
2. 그 방향이 특정 class prediction에 얼마나 민감한지 측정하는 함수(directional derivative)
3. 그 민감도를 class 전체에 대해 통계적으로 요약하는 metric(TCAV score)

**TCAV**는 바로 이 **concept-based global explanation**을 제공하는 method입니다.

## Concept Activation Vector

concept `C`에 대한 example image set을 $P_C$라고 합시다.
"그냥 not C" 또는 무관한 random image set $N$을 준비합니다.

- network의 특정 layer `l`에서 activation을 뽑으면 각 이미지 $x$에 대해서 $f_l(x) \in \mathbb{R}^m$($m$차원 feature vector)
- $\{f_l(x): x \in P_C\}$와 $\{f_l(x): x \in N\}$ 두 set은 activation space 상에서 서로 다른 영역에 모여있을 가능성이 큼
- 이 두 set을 linear classifier(ex. linear SVM, logistic regression)로 분리하면 hyperplare(결정 경계)의 nomal(법선) vector가 concept `C`가 향하는 대표 방향이 됨

이렇게 얻은 nomal vector를 Concept Activation Vector(CAV)라고 부릅니다.

> CAV는 "layer l의 embedding space에서 concept C가 있는 쪽으로 가는 대표 방향"이라고 볼 수 있습니다.

## Concept Sensitivity

어떤 class `k`의 logit을 $h_{l,k}( \cdot )$라고 합시다.
그리고 layer `l`에서의 activation이 $f_l(x)$라고 할 때 CAV 방향으로 activation을 아주 조금 움직이면 class `k`의 logit이 얼마나 증가 혹은 감소해버리냐? 이걸 directional derivative(방향 미분; 도함수수)으로 측정합니다.

$$
S_{C,k,l}(x) = \nabla_{f_l}h_{l,k}(f_l(x)) \cdot v_C^l
$$

- $v_C^l$: layer `l`에서 concept `C`가 향하는 방향 vector(CAV)
- $\nabla_{f_l}h_{l,k}$: activation에 대한 logit의 gradient
- dot product(내적) -> "`C`가 향하는 방향으로의 변화량"

sign($S$)로 해석해보면

- $S_{C,k,l}(x) > 0$ -> concept`C`쪽으로 activation을 밀면 class `k`의 logit이 올라감
  - 해당 sample에서 `C`는 `k`prediction에 positive influence
- $S_{C,k,l}(x) < 0$ -> concept`C`가 해당 class를 방해하는 방향

> 이 per-sample(각 sample image 하나하나에 대해) sensitivity를 class 전체로 aggregate(부호가 양수인 비율 계산)한 것이 바로 **TCAV score**입니다.

# Method

## Step 1: Concept Dataset

- 관심 concept `C` definition
  - ex. "stripe", "gender", "age", "hair color" 등

- positive set $P_C$ 수집
  - concept `C`에 해당하는 images
  - 반드시 training set일 필요는 없음 사후에 따로 모아도 됨

- negative set $N$ 수집
  - random images, not C images
  - `C`에 비해 명확히 다른 distribution을 가지도록 하는 것이 좋음

## Step 2: Layer Selection & Activation Extraction

- 분석할 layer `l` 선택
  - ex. 마지막 conv layer, bottleneck layer, fully-connected 직전 등
  - low-level concept(color, edge) -> lower layers
  - high-level concept(object, texture, semantic) -> higher layers

- 각 image $x$에 대해 layer `l` activation 계산

$$
f_l(x) = \mathbb{R}^m
$$