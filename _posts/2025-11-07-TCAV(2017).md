---
layout: post
title: "TCAV(2017)"
date: 2025-11-07
description: "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors"
tags:
  - XAI
  - Vision
---

# Introduction

본 논문의 출발점은 아주 단순해버립니다.

> 이 CNN이 얼룩말이라고 예측해버릴때 진짜로 줄무늬라는 개념 때문에 그렇게 예측한걸까?

기존의 XAI methods는 대부분 pixel이나 feature attribution을 기반으로 설명을 제공합니다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fb772CS%2FbtrdpCz3I58%2FAAAAAAAAAAAAAAAAAAAAABbkNzB0T5tVw2Locc6_15GBWgDiTmXIhL48DO-kkNz3%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DlDhf9rtc3flvPB0B8x4VvVvESEk%253D" alt="Model Interpretability를 위한 기존 접근 방법" width="600" referrerpolicy="no-referrer">

- Grad-CAM, Integrated Gradients, LRP 같은 방법들은 이 pixel이나 region이 prediction에 얼마나 기여했는지 heatmap으로 보여줌

하지만 사람들은 실제로 이렇게 생각하지 않죠?

"줄무니가 있으니 얼룩말이네" 이런식으로 생각하기 마련입니다.

이게 무슨 말이냐?

model은 input features, neural activation과 같은 data에 관련된 vector를 즉 pixel이나 feature space에서 계산하고 사람은 human-interpretable concept에 관련된 vector를 즉 human concept space에서 사고한다는 겁니다.

저자가 해결하려는 core problem은 다음과 같습니다.
1. model의 inner representation을 사람이 사용하는 high-level concept space로 mapping하는 것
2. 한두 sample 수준(local)이 아니라 이 class 전체에서 이 concept가 얼마나 중요한지를 global하게 평가하는 것
3. 기존 