---
layout: post
title: "GCN(2016)"
date: 2025-11-09
description: "Semi-Supervised Classification with Graph Convolutional Networks"
tags:
  - Graph
---

# Introduction

우리는 graph 위의 node classification task를 하고 싶습니다.
예를들어 논문 인용 relation(edge)에서 각 논문(node)의 주제(label)을 예측하는 것이죠.
하지만 label이 있는 nodel는 소수이고 대부분의 node는 unlabeled입니다.

-> 전형적인 **semi-supervised learning** *but on graph problem*

기존의 approach는 다음과 같습니다.

$$
L = L_0 + \lambda L_{\mathrm{reg}}
$$

$$
L_{\mathrm{reg}} = \sum_{i,j} A_{ij} \left\lVert f(X_i) - f(X_j) \right\rVert^2 = f(X)^\top \Delta f(X)
$$

- $L_0$: labdel이 있는 node에 대한 supervised loss
- $L_{\mathrm{reg}}$: [graph laplacian](https://heonyus.github.io/2025/11/Laplacian-Matrix.html) 기반 regularization

-> 가정: 연결된 node는 유사한 label을 가짐(smoothness assumption; feature가 비슷한 샘플은 같은 class일 확률이 높음)

하지만 이 방식은 edge = similarity라는 assumption에 너무 의존적이고 label information은 loss에서만 쓰이게 됩니다.
그래서 graph architecture는 일종의 regularization term으로만 작용하게 됩니다.(feature level에서 직접 활용할 수 없음)

본 논문에서 저자들은 graph architecture를 loss의 regularization으로만 쓰는 대신에 model 자체를 graph에 **conditional한 neural network** $f(X, A)$로 설계합니다.

즉, **Graph Convolutional Network(GCN)**이라는 layer를 define해서 각 layer에서 neighbor node의 feature를 smoothing(섞어주고) 그 결과 representation을 사용해 label을 predict합니다.

자 그럼 architecture에 대해서 두괄식으로 설명해보겠습니다.

- graph spectral theory에서 출발해 매우 단순한 localized first-order approximation을 취함
- GCN propagation rule

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

> 위 GCN propagation rule을 통해서 성능이 잘나오고 복잡도가 $O(\lvert E \rvert)$로 매우 효율적인 architecture를 얻을 수 있음

# Background

## Graph Data Setting

본 논문에서 다루는 base object는 graph 더 detail하게는 graph-structured data죠.

- graph $G = (V, E)$
  - $V$: node set, $N = \lvert V \rvert$
  - $E$: edge set(여기서는 undirected, unweighted 가정)
- adjacency matrix(인접 행렬) $A \in \mathbb{R}^{N \times N}$ (binary 또는 weighted, undirected 가정; graph spectral theory)
  - $A_{ij} = 1$ 이면 node $i$와 node $j$사이에 edge가 존재함을 의미
  - 없으면 $0$
- node feature matrix $X \in \mathbb{R}^{N \times C}$
  - 각 node $i$에 대해 feature vector $x_i \in \mathbb{R}^C$(한 행)
- label matrix $Y$: 일부 node만 labeled되어 있음
  - labeled node index set: $\mathcal{Y}_L$

본 논문의 목적은 semi-supervised node classification task를 수행하는 것입니다.
-> graph architecture $A$와 node feature $X$를 모두 활용해서 labeling돼 있는 소수의 node의 정보를 나머지 node로 퍼뜨리며 모든 node의 class를 predict하는 것입니다.

핵심적인 가정은 아무래도 앞서 설명했던 **smoothness assumption**입니다.

- edgr로 강하게 connected된 node들은 비슷한 feature이나 label을 가질 확률이 높음
- 따라서 graph 위에서 "signal(ex. class score)"이 갑자기 튀지 않고 부드럽게 변함

> 해당 **smoothness**를 수학적으로 표현하는 대표적인 도구가 Graph Laplacian입니다.

## Graph Laplacian & Smoothness

먼저 몇 가지 기본적인 definition을 해봅시다.

- degree matrix $D$ : 

$$
D_{ii} = \sum_j A_{ij}
$$

- normalized adjacency:

$$
D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$

- normalized laplacian:

$$
L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$

여기서 $L$은 graph 상에서의 2차 미분 연산자(second-order differential operator)같은 것입니다.

Laplacian의 quadratic form $f^\top L f$는 다음과 같이 정의됩니다.

$$
f^\top L f = \frac{1}{2}\sum_{i,j} A_{ij}(f_i - f_j)^2
$$

이 값이 작을수록 graph 위에서 signal $f$가 smooth하다는 의미입니다.
즉, 연결된 node들 간의 값 차이가 작다는 것이죠.

> 이 smoothness를 최소화하는 것이 바로 기존 semi-supervised learning의 regularization term $L_{\mathrm{reg}}$의 목적이었습니다.

# Spectral Graph Convolutions

이제 graph 위에서 convolution을 정의하는 방법을 알아봅시다.

## Graph Fourier Transform

일반적인 signal processing에서 convolution은 Fourier domain에서 곱셈으로 변환됩니다.

$$
(f * g)(t) = \mathcal{F}^{-1}[\mathcal{F}(f) \cdot \mathcal{F}(g)]
$$

graph에서도 비슷하게 정의할 수 있습니다.

graph의 normalized Laplacian $L$을 eigendecomposition하면:

$$
L = U \Lambda U^\top
$$

- $U$: orthonormal eigenvectors (graph Fourier basis)
- $\Lambda$: eigenvalues (frequencies)

graph signal $x$의 Fourier transform은:

$$
\hat{x} = U^\top x
$$

역변환은:

$$
x = U \hat{x}
$$

## Spectral Convolution

graph signal $x$에 대해 spectral convolution은 다음과 같이 정의됩니다.

$$
g_\theta * x = U g_\theta(\Lambda) U^\top x
$$

- $g_\theta(\Lambda)$: diagonal matrix of learnable parameters (frequency domain filter)

하지만 이 방식은 몇 가지 문제가 있습니다:

1. **computational complexity**: $O(N^2)$ (eigendecomposition 필요)
2. **non-localized**: 전체 graph를 고려해야 함
3. **parameter sharing**: 각 frequency마다 다른 parameter 필요

## Localized First-Order Approximation

저자들은 Chebyshev polynomial을 사용한 localized approximation을 제안합니다.

먼저 $g_\theta(\Lambda)$를 Chebyshev polynomial $T_k(\cdot)$로 근사합니다.

$$
g_\theta(\Lambda) \approx \sum_{k=0}^{K} \theta_k T_k(\tilde{\Lambda})
$$

여기서 $\tilde{\Lambda} = \frac{2}{\lambda_{\max}}\Lambda - I_N$ (rescaled eigenvalues)

이때 $K=1$로 제한하면 (first-order approximation):

$$
g_\theta * x \approx \theta_0 x + \theta_1 \left(\frac{2}{\lambda_{\max}}L - I_N\right) x
$$

추가로 $\lambda_{\max} \approx 2$로 가정하고 $\theta = \theta_0 = -\theta_1$로 단순화하면:

$$
g_\theta * x \approx \theta \left(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\right) x
$$

하지만 이렇게 하면 numerical instability 문제가 발생할 수 있습니다.
따라서 renormalization trick을 적용합니다.

# Layer-wise Linear Model

## Renormalization Trick

self-loop을 추가한 adjacency matrix를 사용합니다.

$$
\tilde{A} = A + I_N
$$

그리고 normalized degree matrix도 업데이트:

$$
\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}
$$

이제 propagation rule은:

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

이게 바로 GCN의 핵심 공식입니다!

## 왜 이렇게 하면 좋은가?

1. **Numerical stability**: $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$의 eigenvalues가 $[0, 1]$ 범위에 있음
2. **Self-loop**: 각 node가 자신의 feature도 활용할 수 있음
3. **Localized**: 각 node는 1-hop neighbor만 고려 (layer가 깊어질수록 receptive field 확장)
4. **Efficient**: $O(\lvert E \rvert)$ complexity (sparse matrix multiplication)

## Multi-layer GCN

여러 layer를 쌓으면:

$$
Z = \text{softmax}(\tilde{A} \text{ ReLU}(\tilde{A}XW^{(0)}) W^{(1)})
$$

- $W^{(0)} \in \mathbb{R}^{C \times H}$: first layer weight (input feature dimension $C$ → hidden dimension $H$)
- $W^{(1)} \in \mathbb{R}^{H \times F}$: second layer weight (hidden dimension $H$ → output class $F$)
- $\tilde{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$: normalized adjacency (간단히 표기)

각 layer에서:
1. normalized adjacency로 neighbor feature를 aggregate
2. linear transformation $W^{(l)}$ 적용
3. non-linearity (ReLU, softmax) 적용

# Experiments

## Datasets

- **Cora**: 논문 인용 네트워크 (2,708 nodes, 5,429 edges, 7 classes)
- **Citeseer**: 논문 인용 네트워크 (3,327 nodes, 4,732 edges, 6 classes)
- **Pubmed**: 논문 인용 네트워크 (19,717 nodes, 44,338 edges, 3 classes)

각 dataset에서:
- node: 논문
- edge: 인용 관계
- feature: 논문의 단어 bag-of-words
- label: 논문의 주제 분야

## Experimental Setup

- **Training**: 각 class당 20개 labeled node만 사용
- **Validation**: 500개 node
- **Test**: 1,000개 node
- **Optimization**: Adam optimizer
- **Early stopping**: validation loss 기준

## Results

| Method | Cora | Citeseer | Pubmed |
|--------|------|----------|--------|
| Manifold Regularization | 59.5% | 60.1% | 70.7% |
| SemiEmb | 59.0% | 59.6% | 71.1% |
| Label Propagation | 68.0% | 45.3% | 63.0% |
| DeepWalk | 67.2% | 43.2% | 65.3% |
| ICA | 75.1% | 69.1% | 73.9% |
| Planetoid | 75.7% | 64.7% | 77.2% |
| **GCN** | **81.5%** | **70.3%** | **79.0%** |

> GCN이 모든 baseline을 크게 앞서는 성능을 보여줍니다!

## Ablation Study

### Layer Depth

- 2-layer GCN이 가장 좋은 성능
- 3-layer 이상은 over-smoothing 문제 발생 (모든 node representation이 비슷해짐)

### Propagation Rule Variants

여러 variant를 비교:

1. **GCN (proposed)**: $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$
2. **No normalization**: $\tilde{A}$
3. **Symmetric normalization**: $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ (self-loop 없음)

결과: 제안한 renormalization trick이 가장 안정적이고 성능이 좋음

# Conclusion

GCN은 다음과 같은 특징을 가진 **semi-supervised node classification** 방법입니다.

1. **Spectral theory 기반**: graph Fourier transform과 Chebyshev polynomial approximation에서 출발
2. **First-order approximation**: $K=1$로 제한하여 localized, efficient한 구조
3. **Renormalization trick**: self-loop 추가와 symmetric normalization으로 numerical stability 확보
4. **Layer-wise propagation**: 각 layer에서 neighbor feature를 aggregate하여 representation 학습

핵심 contribution:

- graph structure를 model architecture에 직접 통합 (loss regularization이 아닌)
- $O(\lvert E \rvert)$ complexity로 scalable
- 간단한 구조임에도 강력한 성능

> GCN은 이후 GraphSAGE, GAT, Graph Transformer 등 다양한 GNN architecture의 foundation이 되었습니다.

# Notes

## Over-smoothing Problem

GCN의 주요 한계점 중 하나는 **over-smoothing**입니다.

- layer가 깊어질수록 모든 node의 representation이 비슷해짐
- 이는 각 layer에서 neighbor feature를 평균내기 때문
- 해결책: residual connection, attention mechanism, normalization 등

## Inductive vs Transductive

본 논문의 GCN은 **transductive** setting입니다.
- 전체 graph 구조를 미리 알고 있어야 함
- 새로운 node가 추가되면 전체 graph를 다시 학습해야 함

**Inductive** setting (나중에 GraphSAGE에서 해결):
- 새로운 node에 대해 unseen graph에서도 inference 가능

## Relation to Message Passing

GCN은 사실 **message passing**의 특수한 케이스입니다.

- 각 node가 neighbor로부터 message를 받음 (aggregation)
- 받은 message를 transform (transformation)
- 이 과정을 여러 layer에 걸쳐 반복

이 관점에서 GCN은 "mean aggregation + linear transformation"을 사용하는 message passing GNN입니다.
