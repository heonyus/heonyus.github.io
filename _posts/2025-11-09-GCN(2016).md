---
layout: post
title: "GCN(2016)"
date: 2025-11-09
description: "Semi-Supervised Classification with Graph Convolutional Networks"
tags:
  - Graph
  - Vision
---

# GCN은 왜 생기게 되었을까?

인공지능 분야에서 이미지 데이터를 처리할 때에는 CNN (Convolutional Neural Network)을 가장 대표적으로 사용합니다.

그리고 순차적인 데이터 (ex. 음성 데이터, 텍스트 데이터)같은 경우에는 RNN (Recurrent Neural Network)을 대표적인 모델로 떠올릴 수 있습니다.

![CNN, RNN](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2F2KR9T%2FbtreKKDRlqp%2FAAAAAAAAAAAAAAAAAAAAACPok70yv59pq8UPPZURolP2UV4BvvMTCAzBIreQm80b%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DrZ6u4NAeU9nMx5uvjIVmZrgx4lU%253D)

하지만 위와 같은 데이터들이 아닌 **연관관계가 있는 데이터**들이 있다면 CNN, RNN같은 기존 모델들이 좋은 성능을 보일까요? 모델의 개념에 대해 공부해보신 분들은 연관관계를 고려하지 못할 것이라는 것을 쉽게 유추하실 수 있을 것입니다.

**연관관계**가 있는 데이터의 예를 든다면:

1. SNS의 Social Network
2. 인간의 뇌세포 구조
3. 3차원 형태의 픽셀 그림(3D Mesh)

![연관관계 데이터 예시](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbBHUyZ%2FbtreMuNsffz%2FAAAAAAAAAAAAAAAAAAAAAPwWRwbbnOxOqNVOTp7mtAhMvfP3kTiuMN3ULavpc9DD%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DCmpCJ3VRlczmXUEg1oQHuyEGOLA%253D)

이러한 **연관관계가 있는 데이터들을 다루기 위한 모델이 바로 GCN (Graph Convolution Network)**입니다.

GCN은 다른 모델들에 비해 연관관계를 확실히 잘 반영한다는 평가가 많은데요. 수치상으로 봤을 때에도 실제로 **연관관계가 있는 데이터들을 가지고 실험한 결과 GCN이 월등히 높은 성능**을 보였습니다.

![GCN 성능 비교](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FpOSGk%2FbtrePWJNAGG%2FAAAAAAAAAAAAAAAAAAAAALZ9-3lOYbM_c2I8EcNaL6oM_L8L-SOqZoZlhJvhPB46%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3Dw1ZIpV8o9S0kZM0d8g0%252FKG8m2uY%253D)

이제 GCN이 연관관계 데이터를 효과적으로 다루는 모델이라는 것을 알았으니, 모델의 개념과 구조에 대해 살펴보도록 하겠습니다.

# Introduction

우리는 graph 위의 node classification task를 하고 싶습니다.
예를들어 논문 인용 relation(edge)에서 각 논문(node)의 주제(label)을 예측하는 것이죠.
하지만 label이 있는 nodel는 소수이고 대부분의 node는 unlabeled입니다.

-> 전형적인 **semi-supervised learning** *but on graph problem*

기존의 approach는 다음과 같습니다.

$$
L = L_0 + \lambda L_{\mathrm{reg}}
$$

$$
L_{\mathrm{reg}} = \sum_{i,j} A_{ij} \left\lVert f(X_i) - f(X_j) \right\rVert^2 = f(X)^\top \Delta f(X)
$$

- $L_0$: labdel이 있는 node에 대한 supervised loss
- $L_{\mathrm{reg}}$: [graph laplacian](https://heonyus.github.io/2025/11/Laplacian-Matrix.html) 기반 regularization

-> 가정: 연결된 node는 유사한 label을 가짐(smoothness assumption; feature가 비슷한 샘플은 같은 class일 확률이 높음)

하지만 이 방식은 edge = similarity라는 assumption에 너무 의존적이고 label information은 loss에서만 쓰이게 됩니다.
그래서 graph architecture는 일종의 regularization term으로만 작용하게 됩니다.(feature level에서 직접 활용할 수 없음)

본 논문에서 저자들은 graph architecture를 loss의 regularization으로만 쓰는 대신에 model 자체를 graph에 **conditional한 neural network** $f(X, A)$로 설계합니다.

즉, **Graph Convolutional Network(GCN)**이라는 layer를 define해서 각 layer에서 neighbor node의 feature를 smoothing(섞어주고) 그 결과 representation을 사용해 label을 predict합니다.

자 그럼 architecture에 대해서 두괄식으로 설명해보겠습니다.

- graph spectral theory에서 출발해 매우 단순한 localized first-order approximation을 취함
- GCN propagation rule

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

> 위 GCN propagation rule을 통해서 성능이 잘나오고 복잡도가 $O(\lvert E \rvert)$로 매우 효율적인 architecture를 얻을 수 있음

# Background

## Graph Data Setting

본 논문에서 다루는 base object는 graph 더 detail하게는 graph-structured data죠.

### 그래프 데이터를 컴퓨터에 반영하는 법

GCN의 개념에 대해 알아보기 전에 먼저 **그래프 데이터를 어떻게 처리할지** 알아봅시다.

![그래프 예시](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fbp3SI7%2FbtreOtumNg2%2FAAAAAAAAAAAAAAAAAAAAAI3D-V9FectAU3WKobvmFo2h6FlmEDIzTGWSYgNq7QoW%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DHH1tidoXV3FIpUxY87vXJWtrUdU%253D)

위와 같이 여러 개의 노드(vertex)로 이루어진 그래프가 있다고 가정하겠습니다. 여기서 연관관계를 반영하기 위해선 어떤 식으로 정보를 컴퓨터에 담아야 할까요?

컴퓨터가 가장 빠르고 편리하게 정보를 담을 수 있는 방법은 바로 **행렬**을 이용하는 것입니다. 그래서 GCN은 다음 그림과 같이 두 개의 행렬을 이용하여 그래프의 정보를 담을 수 있습니다.

![Adjacency Matrix와 Feature Matrix](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbIKv8f%2FbtreVxiaSbt%2FAAAAAAAAAAAAAAAAAAAAAGQ45YUDDD0ZyMcO7oQV7KmoJ6vU3S9lJm6ZErsPJhi1%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3D%252FRBTb%252BxreskZ9h6K2PLrMWgHodc%253D)

이 두 개의 행렬을 사용해서 정말 간단하게 그래프의 모든 정보를 담았습니다.

왼쪽의 **Adjacency matrix(인접행렬)**은 그래프의 연결관계를 표현한 행렬입니다. v0 노드부터 각 노드를 기준으로 어떤 노드들과 연결되었는지 간결하게 행렬에 저장한 것을 볼 수 있는데요. 처음에 개념 이해를 위해선 그래프를 보고 연결관계에 따라 직접 인접행렬을 그려보시는 것을 추천드립니다. 약자로는 **A**를 사용하는데요, 이따 밑에서 모델 구조 설명할 때 나오니 잘 기억하시길 바랍니다.

오른쪽의 **Feature matrix**는 말 그대로 **노드들의 특성 정보를 반영하는 행렬**입니다. 여기서는 5개의 특성(열)을 두어 RGB 정보들과 명도, 채도를 반영하였습니다. RGB 값이나 명도, 채도는 0~255 사이의 값으로 보통 표현하지만 편의상 0과 1로만 표현을 했습니다. 이 부분은 코딩할 때 입맛에 맞춰서 사용하시면 될 것 같아요.

아무튼, 이런 식으로 하나의 행렬을 사용해 노드의 색깔과 정보들을 편리하게 담을 수 있었습니다. 약자로는 X나 H를 사용하는데 주로 **H**로 사용한다고 기억하고 계시면 될 것 같습니다.

### 수학적 정의

- graph $G = (V, E)$
  - $V$: node set, $N = \lvert V \rvert$
  - $E$: edge set(여기서는 undirected, unweighted 가정)
- adjacency matrix(인접 행렬) $A \in \mathbb{R}^{N \times N}$ (binary 또는 weighted, undirected 가정; graph spectral theory)
  - $A_{ij} = 1$ 이면 node $i$와 node $j$사이에 edge가 존재함을 의미
  - 없으면 $0$
- node feature matrix $X \in \mathbb{R}^{N \times C}$ (또는 $H$로 표기)
  - 각 node $i$에 대해 feature vector $x_i \in \mathbb{R}^C$(한 행)
- label matrix $Y$: 일부 node만 labeled되어 있음
  - labeled node index set: $\mathcal{Y}_L$

본 논문의 목적은 semi-supervised node classification task를 수행하는 것입니다.
-> graph architecture $A$와 node feature $X$를 모두 활용해서 labeling돼 있는 소수의 node의 정보를 나머지 node로 퍼뜨리며 모든 node의 class를 predict하는 것입니다.

핵심적인 가정은 아무래도 앞서 설명했던 **smoothness assumption**입니다.

- edgr로 강하게 connected된 node들은 비슷한 feature이나 label을 가질 확률이 높음
- 따라서 graph 위에서 "signal(ex. class score)"이 갑자기 튀지 않고 부드럽게 변함

> 해당 **smoothness**를 수학적으로 표현하는 대표적인 도구가 Graph Laplacian입니다.

## Graph Laplacian & Smoothness

먼저 몇 가지 기본적인 definition을 해봅시다.

- degree matrix $D$ : 

$$
D_{ii} = \sum_j A_{ij}
$$

- normalized adjacency:

$$
D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$

- normalized laplacian:

$$
L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$

여기서 $L$은 graph 상에서의 2차 미분 연산자(second-order differential operator)같은 것입니다.

Laplacian의 quadratic form $f^\top L f$는 다음과 같이 정의됩니다.

$$
f^\top L f = \frac{1}{2}\sum_{i,j} A_{ij}(f_i - f_j)^2
$$

이 값이 작을수록 graph 위에서 signal $f$가 smooth하다는 의미입니다.
즉, 연결된 node들 간의 값 차이가 작다는 것이죠.

> 이 smoothness를 최소화하는 것이 바로 기존 semi-supervised learning의 regularization term $L_{\mathrm{reg}}$의 목적이었습니다.

# Spectral Graph Convolutions

이제 graph 위에서 convolution을 정의하는 방법을 알아봅시다.

## Graph Fourier Transform

일반적인 signal processing에서 convolution은 Fourier domain에서 곱셈으로 변환됩니다.

$$
(f * g)(t) = \mathcal{F}^{-1}[\mathcal{F}(f) \cdot \mathcal{F}(g)]
$$

graph에서도 비슷하게 정의할 수 있습니다.

graph의 normalized Laplacian $L$을 eigendecomposition하면:

$$
L = U \Lambda U^\top
$$

- $U$: orthonormal eigenvectors (graph Fourier basis)
- $\Lambda$: eigenvalues (frequencies)

graph signal $x$의 Fourier transform은:

$$
\hat{x} = U^\top x
$$

역변환은:

$$
x = U \hat{x}
$$

## Spectral Convolution

graph signal $x$에 대해 spectral convolution은 다음과 같이 정의됩니다.

$$
g_\theta * x = U g_\theta(\Lambda) U^\top x
$$

- $g_\theta(\Lambda)$: diagonal matrix of learnable parameters (frequency domain filter)

하지만 이 방식은 몇 가지 문제가 있습니다:

1. **computational complexity**: $O(N^2)$ (eigendecomposition 필요)
2. **non-localized**: 전체 graph를 고려해야 함
3. **parameter sharing**: 각 frequency마다 다른 parameter 필요

## Localized First-Order Approximation

저자들은 Chebyshev polynomial을 사용한 localized approximation을 제안합니다.

먼저 $g_\theta(\Lambda)$를 Chebyshev polynomial $T_k(\cdot)$로 근사합니다.

$$
g_\theta(\Lambda) \approx \sum_{k=0}^{K} \theta_k T_k(\tilde{\Lambda})
$$

여기서 $\tilde{\Lambda} = \frac{2}{\lambda_{\max}}\Lambda - I_N$ (rescaled eigenvalues)

이때 $K=1$로 제한하면 (first-order approximation):

$$
g_\theta * x \approx \theta_0 x + \theta_1 \left(\frac{2}{\lambda_{\max}}L - I_N\right) x
$$

추가로 $\lambda_{\max} \approx 2$로 가정하고 $\theta = \theta_0 = -\theta_1$로 단순화하면:

$$
g_\theta * x \approx \theta \left(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\right) x
$$

하지만 이렇게 하면 numerical instability 문제가 발생할 수 있습니다.
따라서 renormalization trick을 적용합니다.

# Layer-wise Linear Model

## Renormalization Trick

self-loop을 추가한 adjacency matrix를 사용합니다.

$$
\tilde{A} = A + I_N
$$

그리고 normalized degree matrix도 업데이트:

$$
\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}
$$

이제 propagation rule은:

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

이게 바로 GCN의 핵심 공식입니다!

## 두 행렬을 어떻게 합쳐서 Convolution할 것인가?

GCN은 Graph **Convolution** Network의 약자입니다. Convolution이라는 단어하면 CNN이 생각나지 않으신가요? 눈치 채셨겠지만 GCN도 CNN과 상당히 유사한 구조를 지닙니다.

![GCN 모델 구조](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FTw3gU%2FbtreRSAmKq8%2FAAAAAAAAAAAAAAAAAAAAABbwcRsASwkmvhqYY7D4CZa6F3Bgb3bNviXxSZP-ISE_%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3Dxye5rgxXAKEH30%252FPrOFQX2u0pPg%253D)

하지만 여기서 문제점이 몇 가지 생깁니다:

1. **두 개의 행렬을 어떻게 합쳐서 Convolution할 것인가**
2. **그래프 데이터는 노드(vertex)의 위치가 바뀌면 행렬값이 여러 개로 바뀔 수도 있는데 어떻게 일반화를 시킬 것인가**

GCN의 핵심으로는 이런 문제점들을 해결하면서 CNN과 같은 구조를 갖춰 Convolution을 시키는 것인데요, 요 부분들만 이해를 하시면 GCN의 개념에 대해 다 이해를 하셨다고 봐도 무방합니다.

이제 차근차근 첫 번째 문제점부터 해결하는 과정에 대해 알아보겠습니다.

### 행렬 곱셈을 통한 Convolution

두 개의 행렬을 Convolution하는 과정은 당연하겠지만 이 Graph Conv 과정에서 이루어집니다.

![Graph Conv 과정](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fbquguh%2FbtrePXBUTiL%2FAAAAAAAAAAAAAAAAAAAAAKe8Ujb0cT2hOKXHfX-KxthR3TZdUX_pzu2xDHYwqRCG%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DzNUNamNZXp24D6n3cANs4%252BpsHpg%253D)

여기서 새로운 그래프를 예시로 들겠습니다.

![새로운 그래프 예시](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fyj8C5%2FbtreTIK1z5J%2FAAAAAAAAAAAAAAAAAAAAAAnyV9u-fWthYkYKWHKdju5bS7IiWx8xTUOwmESuPqgw%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3D9e0vgA7IrfxdsZ9j%252Fgn5pVG4icQ%253D)

위와 같은 그래프가 있다고 가정하면 여러 층의 Conv 과정을 거치기 위해서는 어떤 식으로 구현을 해야 할까요?

모델은 CNN과 상당히 유사한 구조지만 이 데이터는 연관관계(Edge)가 있다는 것을 기억하셔야 합니다. 따라서 노드(vertex)의 자체 정보와 연관관계를 동시에 적용하기엔 상당한 어려움이 있습니다.

먼저 노드의 자체 정보만 저장해서 현재 Layer에서 다음 Layer로 보내기 위해서는 이런 식으로 식을 쓸 수 있습니다.

![Vertex 1 기준](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FxH99k%2FbtreLLbcWuR%2FAAAAAAAAAAAAAAAAAAAAANmX0llsfPba5SFVDD_zkPCYog3rddJYMuxR1V6OGuUq%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DvfcrmPFRXhts6Fql%252FFuKEeqQvCw%253D)

이 그림은 Vertex 1을 기준으로 했을 때의 식입니다. **H**는 위에서 나왔던 것처럼 **Feature matrix**를 뜻하고 **W**는 **가중치**를 뜻합니다.

식을 보시면 Vertex 1과 연결되는 V2, V3, V4 모든 Vertex가 식에 들어가 있는 것이 확인 가능합니다. (자기 자신도 당연히 다음 Layer에 보내기 위해서는 정보를 포함해야 하기 때문에 식에 포함됩니다.)

여기서 모든 Vertex에 똑같은 W값이 붙는 이유는 4개의 모든 정보가 Vertex라는 같은 형식이기 때문입니다.

하지만 이 방식은 효율적이지 못해 실제로 컴퓨터가 사용하는 방법이 아닙니다. 각각의 노드를 기준으로 일일이 식을 나타내야 하기 때문에 너무 복잡하고, 데이터가 커지면 속도도 현저하게 느려지기 때문입니다.

이를 해결하기 위해 컴퓨터는 **모든 노드(Vertex)들이 서로 연결되어 있다고 먼저 가정**합니다. 위 그래프는 노드가 4개가 있기 때문에 식은 다음과 같이 나올 수 있습니다.

![전체 노드 행렬 표현](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbrOdqW%2FbtreRRVMXZ1%2FAAAAAAAAAAAAAAAAAAAAANqwhjYR73stkCT1L4J9XJ1boLyDw17xNo_ZsXYbHl8Z%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3D%252F1ZONCBt9n8%252FPkZQQC1t44Q5a8A%253D)

이제 이렇게 하나의 식으로 모든 노드의 정보를 포함하는 식을 만들었습니다. 이제 위의 저 식을 보기 좋게 각각의 블럭으로 쪼개서 나타내 보겠습니다.

![블럭으로 표현](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcGg6TT%2FbtreOMUC5to%2FAAAAAAAAAAAAAAAAAAAAALkDX98PLcPY9PICmHZevygwLwln4jk7-xfUEGiiK2oQ%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DcOlGvLnWQ82rTIidPNVJ%252BmpG1jQ%253D)

이렇게 블럭으로 보니 훨씬 이해하기 편해졌습니다. 여기서 만족하지 않고 더 보기 편해지게 같은 색의 블럭끼리 묶어보면 이렇게 큰 두 블록이 나옵니다.

![큰 블럭으로 묶기](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FDebbC%2FbtreOsa9IVk%2FAAAAAAAAAAAAAAAAAAAAAKn7pxlses7HpPNx75Zh_GzkMtFa41zNanwfgQAabjdX%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DBp8Nv7lZGZJDlKf%252Bfo1OMHV0Vu8%253D)

최종적으로는 밑 그림처럼 같은 색끼리 곱해지는 행렬곱의 형태가 나타납니다.

![행렬곱 형태](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FdFNef4%2FbtreMCyj92q%2FAAAAAAAAAAAAAAAAAAAAAELnC_lCWeKBGWrJSaQ4-jym4N6Qz2KeveCu8PZ-xCG8%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DBWVfeb3SC5zVw3qL9bVHnaLzU5o%253D)

이제 저 긴 블록들을 큰 블럭으로 합쳐서 조금 더 자세하게 표현해보겠습니다.

![최종 행렬 표현](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FblfuHw%2FbtreKKw9RBK%2FAAAAAAAAAAAAAAAAAAAAAD8yg40cZi8F75vSsc7HEdtsqqyvUiPsVtgD--OKOkTJ%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3Djoj6rZ%252FkQr6%252Fn1xUrogbq3FZ4A4%253D)

이렇게 큰 두 개의 블럭이 만들어졌는데요, 이제 두 블럭의 열과 행이 어떻게 이루어졌는지 한 번 알아보겠습니다.

먼저, H 블럭은 Vertex1 ~ Vertex 4까지 노드가 총 4개 구성되어 있기 때문에 행도 당연하게 4개의 행으로 이루어져 있습니다. 열은 지금 총 5개로 이루어져 있는데, 열이 5개인 이유는 Feature 값을 임의로 5개를 지정했기 때문입니다 (R, G, B, 명도, 채도). 만약 RGB 정보나 색 정보가 아닌 다른 정보가 있다면 그 정보 수만큼 열이 이루어지겠죠?

그리고, 가중치 행렬인 W 블럭은 H 행렬의 Feature 수인 5를 가져와 5개의 행으로 이루어집니다. 이유는 행렬곱을 편리하게 하기 위해서는 이전 행렬의 열 (=5)과 다음 행렬의 행의 수 (=5)가 맞아야 하기 때문입니다. 그리고, 열은 Filter의 수로 우리가 현재 있는 레이어의 Filter를 의미합니다.

![GCN 모델 구조 상세](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbsUpYF%2FbtreT3W0013%2FAAAAAAAAAAAAAAAAAAAAADHZQuxhih8eGLwn6HeWNunHX9JZY7Ss2K7GpvaSbiF1%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DjtDK0benJ8vnZ%252B%252BvmtRJly89fA0%253D)

현재 우리는 Graph Conv의 첫 번째 레이어에 있다고 가정하겠습니다. 레이어에 적혀있는 대로 필터의 수는 16이 됩니다. 여기서 다음 레이어로 가면 Filter 수는 16이 아니라 32로 바뀌겠죠?

이제 이 두 행렬을 곱하면 이렇게 큰 행렬이 하나 생깁니다.

![행렬 곱셈 결과](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcNrqxt%2FbtreOtnz24d%2FAAAAAAAAAAAAAAAAAAAAAI7NO0sWFnrmiVtRpDqFb4BqjFxgeRC1T6C_vFnSIEYe%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DbklILqBwZ4l23xY8mC1mBLoorsU%253D)

이제 우리는 노드의 정보에 가중치까지 곱한 정보를 가진 행렬을 만들었습니다. 하지만 이 행렬을 만들 때 모든 노드를 연결했다는 가정을 하고 만들었다는 것을 기억하셔야 합니다...

그럼 여기서 원래의 그래프 같은 정보를 가져오기 위해서는 어떻게 해야 할까요? 답은 정말 간단합니다. 저 행렬에서 아까 만들어두었던 Adjacency Matrix (A)만 곱해주면 되는데요.

![Adjacency Matrix 곱셈](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2F2uyVb%2FbtreKI0mDD6%2FAAAAAAAAAAAAAAAAAAAAAIc1CWkd_VUKwmik876B958JMTIyzeSeJsW5VCujeSrC%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3Df%252FfkSmiVVFZOhML9GovO7zqfayc%253D)

여기서, 식을 도출하기 위해서도 행렬곱 개념이 필요합니다. 행렬곱 과정을 모르시는 분들을 위해 간단하게 설명을 드리겠습니다.

![행렬곱 과정 설명](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FYYNu4%2FbtreMDjG3HP%2FAAAAAAAAAAAAAAAAAAAAAJh0k2oQOzwp-XTk7N7_6eiqhID6iU4gJWErVo_39voN%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DdMSWFcGkm7paDa7si7IuLV0oQRg%253D)

위에 있는 두 행렬은 방금까지 설명한 개념을 그대로 옮겨온 구조입니다. 여기서 V2를 기준으로 설명을 드리겠습니다.

V2를 기준으로 왼쪽 A 행렬의 두 번째 행과 오른쪽 테이블의 열들이 곱해지게 되는데요. 왼쪽 A 행렬을 보시면 V2는 V1과 V2가 연결되어 1로 표현된 것을 보실 수 있습니다.

![V2 기준 행렬곱](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FtzBaS%2FbtrePVYqYe4%2FAAAAAAAAAAAAAAAAAAAAAB2c8iJXha8YgrLpYof05eQDEth_HvAInoEUAf1UWiZE%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DLfnH%252BtRvpudt7ezqkTu9Ck3O3uw%253D)

행렬곱을 할 때에는 왼쪽 2번째 행을 기준으로 오른쪽 행렬의 열들이 차례대로 곱해져서 분홍색 행렬에 들어가게 됩니다. 이는 HW 행렬과 A 행렬이 곱해질 때 위 보라색 행렬의 첫 번째, 두 번째 행의 값만 살려서 최종 분홍색 행렬에 저장한다는 뜻입니다.

![행렬곱 진행 과정](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fba7rOR%2FbtrePWQy0ND%2FAAAAAAAAAAAAAAAAAAAAAEyHU6HwiqUdpFAY_JaEc5UukNSxNcXreIuTGUHy3hci%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DupxZVVGkGliNsXGDy0ydeST3MRs%253D)

마찬가지로 초록색 부분이 이런 식으로 저장되고, 쭉 다음 과정도 진행되어서 분홍색 행렬이 채워지게 됩니다. 여기서 값들이 저장되는 분홍색 행렬은 다음 레이어로 보낼 최종 값들로 다음 레이어에 전달이 되는 행렬입니다.

이 그림들을 식으로 표현한다면 이렇게 표현할 수 있습니다.

![최종 식 표현](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcEijGV%2FbtreKxjPAFC%2FAAAAAAAAAAAAAAAAAAAAAIjKwPl1MLY20Zmjv2lJYVpEJX8580MEv5X_ox9zada-%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DtOIix6dh162irbksbFonig4werA%253D)

결론적으로 A 행렬 × HW 행렬에 b(바이어스) 값과 활성화 함수를 곱한 값으로 정말 쉽게 식이 도출되었습니다. 이렇게 해서 과정은 복잡했지만 결과적으로는 너무 간단한 식으로 두 행렬을 Convolution에 적용시킬 수 있었습니다.

## 왜 이렇게 하면 좋은가?

1. **Numerical stability**: $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$의 eigenvalues가 $[0, 1]$ 범위에 있음
2. **Self-loop**: 각 node가 자신의 feature도 활용할 수 있음
3. **Localized**: 각 node는 1-hop neighbor만 고려 (layer가 깊어질수록 receptive field 확장)
4. **Efficient**: $O(\lvert E \rvert)$ complexity (sparse matrix multiplication)

## Multi-layer GCN

여러 layer를 쌓으면:

$$
Z = \text{softmax}(\tilde{A} \text{ ReLU}(\tilde{A}XW^{(0)}) W^{(1)})
$$

- $W^{(0)} \in \mathbb{R}^{C \times H}$: first layer weight (input feature dimension $C$ → hidden dimension $H$)
- $W^{(1)} \in \mathbb{R}^{H \times F}$: second layer weight (hidden dimension $H$ → output class $F$)
- $\tilde{A} = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$: normalized adjacency (간단히 표기)

각 layer에서:
1. normalized adjacency로 neighbor feature를 aggregate
2. linear transformation $W^{(l)}$ 적용
3. non-linearity (ReLU, softmax) 적용

# Experiments

## Datasets

- **Cora**: 논문 인용 네트워크 (2,708 nodes, 5,429 edges, 7 classes)
- **Citeseer**: 논문 인용 네트워크 (3,327 nodes, 4,732 edges, 6 classes)
- **Pubmed**: 논문 인용 네트워크 (19,717 nodes, 44,338 edges, 3 classes)

각 dataset에서:
- node: 논문
- edge: 인용 관계
- feature: 논문의 단어 bag-of-words
- label: 논문의 주제 분야

## Experimental Setup

- **Training**: 각 class당 20개 labeled node만 사용
- **Validation**: 500개 node
- **Test**: 1,000개 node
- **Optimization**: Adam optimizer
- **Early stopping**: validation loss 기준

## Results

| Method | Cora | Citeseer | Pubmed |
|--------|------|----------|--------|
| Manifold Regularization | 59.5% | 60.1% | 70.7% |
| SemiEmb | 59.0% | 59.6% | 71.1% |
| Label Propagation | 68.0% | 45.3% | 63.0% |
| DeepWalk | 67.2% | 43.2% | 65.3% |
| ICA | 75.1% | 69.1% | 73.9% |
| Planetoid | 75.7% | 64.7% | 77.2% |
| **GCN** | **81.5%** | **70.3%** | **79.0%** |

> GCN이 모든 baseline을 크게 앞서는 성능을 보여줍니다!

## Ablation Study

### Layer Depth

- 2-layer GCN이 가장 좋은 성능
- 3-layer 이상은 over-smoothing 문제 발생 (모든 node representation이 비슷해짐)

### Propagation Rule Variants

여러 variant를 비교:

1. **GCN (proposed)**: $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$
2. **No normalization**: $\tilde{A}$
3. **Symmetric normalization**: $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ (self-loop 없음)

결과: 제안한 renormalization trick이 가장 안정적이고 성능이 좋음

# Conclusion

GCN은 다음과 같은 특징을 가진 **semi-supervised node classification** 방법입니다.

1. **Spectral theory 기반**: graph Fourier transform과 Chebyshev polynomial approximation에서 출발
2. **First-order approximation**: $K=1$로 제한하여 localized, efficient한 구조
3. **Renormalization trick**: self-loop 추가와 symmetric normalization으로 numerical stability 확보
4. **Layer-wise propagation**: 각 layer에서 neighbor feature를 aggregate하여 representation 학습

핵심 contribution:

- graph structure를 model architecture에 직접 통합 (loss regularization이 아닌)
- $O(\lvert E \rvert)$ complexity로 scalable
- 간단한 구조임에도 강력한 성능

> GCN은 이후 GraphSAGE, GAT, Graph Transformer 등 다양한 GNN architecture의 foundation이 되었습니다.

# Notes

## Over-smoothing Problem

GCN의 주요 한계점 중 하나는 **over-smoothing**입니다.

- layer가 깊어질수록 모든 node의 representation이 비슷해짐
- 이는 각 layer에서 neighbor feature를 평균내기 때문
- 해결책: residual connection, attention mechanism, normalization 등

## Inductive vs Transductive

본 논문의 GCN은 **transductive** setting입니다.
- 전체 graph 구조를 미리 알고 있어야 함
- 새로운 node가 추가되면 전체 graph를 다시 학습해야 함

**Inductive** setting (나중에 GraphSAGE에서 해결):
- 새로운 node에 대해 unseen graph에서도 inference 가능

## Relation to Message Passing

GCN은 사실 **message passing**의 특수한 케이스입니다.

- 각 node가 neighbor로부터 message를 받음 (aggregation)
- 받은 message를 transform (transformation)
- 이 과정을 여러 layer에 걸쳐 반복

이 관점에서 GCN은 "mean aggregation + linear transformation"을 사용하는 message passing GNN입니다.

## 그래프 데이터의 일반화 문제와 Readout

방금까지 도출했던 식을 통해 이제 여러 번의 Conv 과정을 거쳤습니다. 하지만 Readout이라는 처음 보는 레이어를 만나게 되었습니다.

![Readout 레이어](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbqNqlN%2FbtrePWpvO9J%2FAAAAAAAAAAAAAAAAAAAAAGgiU193q8c-HuaL1gYyb7rhzKgVxIsf-qwm8b7sXE5N%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1764514799%26allow_ip%3D%26allow_referer%3D%26signature%3DXAXzJlZ3c%252BkeUP35fyPxKzoJNT0%253D)

이 **Readout이라는 레이어는** 결론부터 말씀드리면 두 번째 문제점을 해결하는 키 레이어입니다. 즉, **여러 행렬값들을 일반화시킬 수 있는 레이어**라는 뜻입니다.

그래프 데이터는 노드 간의 연결 정보가 같아도 **노드의 위치나 행렬의 표현 순서가 달라지면 같은 그래프라도 여러 가지 행렬로 표현**될 수 있습니다.

그래서 먼저 Conv 과정을 거친 후, **그래프를 일반화**시키기 위해 마지막 Conv에서 나온 행렬값을 **MLP를 통과시켜 행렬의 모든 값을 합치게 됩니다.**

이 과정을 **Readout 과정**이라고 하고, 이 과정을 거친 뒤 CNN과 같이 Predict하는 과정을 거치게 됩니다.

이 Readout 과정을 통해 우리는 여러 가지 형태가 나오는 행렬에 대해 고민할 필요가 없어졌습니다. 정말 간단하게 두 번째 문제를 해결하였습니다.

이제 모든 문제점을 해결했고, Graph 데이터를 가지고도 CNN과 같이 깊이 있는 모델을 구성할 수 있게 되었습니다.
