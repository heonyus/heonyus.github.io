---
layout: post
title: "SENN(2018)"
date: 2025-11-10
description: "Towards Robust Interpretability with Self-Explaining Neural Networks"
tags:
  - XAI
---

# Introduction

기존의 XAI 방법론들은 대부분 **사후 설명(post-hoc explanation)**에 집중하고 있죠.
즉, 이미 학습된 모델에 대해 특정 예측 주변에서 설명을 생성하는 방식입니다.
[LIME](https://heonyus.github.io/2025/11/LIME(2016).html)이나 [SHAP](https://heonyus.github.io/2025/11/SHAP(2017).html) 같은 방법들이 대표적인 예시입니다.

하지만 이런 방법들은 몇 가지 문제가 있습니다.

1. **설명의 불안정성**: 비슷한 입력에 대해 완전히 다른 설명을 생성할 수 있음
2. **충실도(faithfulness) 부족**: 설명이 실제 모델의 내부 작동 방식을 정확히 반영하지 못할 수 있음
3. **명시성(explicitness) 부족**: 모델 구조 자체가 해석 가능하지 않음

본 논문에서는 이런 문제들을 해결하기 위해 **Self-Explaining Neural Networks (SENN)**을 제안합니다.
이름 그대로 모델이 학습 과정에서부터 스스로 설명 가능한 구조를 가지도록 설계된 네트워크입니다.

> SENN은 모델이 예측을 할 때마다 그 예측의 근거를 **명시적으로** 제공하면서, 그 설명이 모델의 실제 작동 방식과 **일치(faithful)**하고, 비슷한 입력에 대해 **안정적인(stable)** 설명을 생성하도록 보장합니다.

![SENN 아키텍처](https://creatis-myriad.github.io/collections/images/SENN-model/SENN-model-architecture.jpg)

**Figure 1.** SENN의 전체 아키텍처. 입력 $x$가 concept encoder $h$, relevance parametrizer $\theta$, aggregator $g$를 거치면서 예측과 (concept, relevance) 쌍 형태의 설명을 동시에 생성한다.

# 핵심 Idea

두괄식으로 먼저 SENN의 핵심을 설명해보겠습니다.

SENN은 선형 분류기(linear classifier)에서 출발해서 점진적으로 복잡한 구조로 일반화합니다.
핵심은 **예측이 concept 기반의 가산적 결합(additive combination)**으로 이루어진다는 것입니다.

$$
f(x) = \theta(x)^T h(x)
$$

여기서:
- $h(x)$: 입력 $x$에서 추출된 **concept vector** (해석 가능한 중간 표현)
- $\theta(x)$: 각 concept에 대한 **relevance score** (중요도)
- $f(x)$: 최종 예측값

이 구조의 장점은 다음과 같습니다:

1. **명시성(Explicitness)**: 예측이 concept들의 가중합으로 명확하게 표현됨
2. **충실도(Faithfulness)**: 설명이 모델의 실제 계산 과정과 일치함
3. **안정성(Stability)**: 비슷한 입력에 대해 비슷한 설명을 생성하도록 regularization

자, 이제 이 구조가 어떻게 만들어지는지 단계적으로 살펴봅시다.

# Desiderata for Explanations

저자들은 좋은 설명이 만족해야 할 세 가지 조건을 제시합니다.

## 1. Explicitness (명시성)

설명이 모델의 구조에서 **직접적으로** 나와야 합니다.
즉, 모델이 예측을 할 때 그 예측의 근거가 모델 내부에서 명확하게 드러나야 합니다.

기존의 post-hoc 방법들은 모델을 "뒤에서" 분석해서 설명을 만들지만, SENN은 모델 자체가 설명을 생성합니다.

## 2. Faithfulness (충실도)

설명이 모델의 **실제 작동 방식**을 정확히 반영해야 합니다.
즉, 설명에서 중요한 concept가 실제로 예측에 기여하는 정도와 일치해야 합니다.

이를 보장하기 위해 SENN은 설명 생성 과정과 예측 과정을 **같은 계산 그래프**에서 수행합니다.

## 3. Stability (안정성)

**비슷한 입력에 대해서는 비슷한 설명**을 생성해야 합니다.
이건 정말 중요한데요, 같은 클래스의 샘플들이 완전히 다른 설명을 받으면 사용자가 혼란스러워할 수 있죠.

SENN은 이를 위해 **stability regularization**을 도입합니다.

다음 그림은 실제로 MNIST 숫자 7에 작은 Gaussian noise를 추가했을 때 다양한 설명 방법들이 얼마나 안정적인지를 보여줍니다.
예측 확률 자체는 거의 변하지 않지만(변화 ≤ 10⁻⁴), 대부분의 post-hoc 방법들은 설명이 크게 달라지는 것을 확인할 수 있습니다.

![안정성 비교](https://creatis-myriad.github.io/collections/images/SENN-model/figure4.jpg)

**Figure 4.** MNIST 숫자 7의 원본(상단 행)과 Gaussian noise 추가 버전(하단 행)에 대한 설명 안정성 비교. 

좌측부터 Original, Saliency, Grad*Input, Int.Grad., e-LRP, Occlusion, LIME, SENN 순서로 8가지 설명 방법의 heatmap이 표시됩니다. 각 방법 아래에 안정성을 정량화한 $\hat{L}$ 값이 제공되며, 예측 확률은 거의 변하지 않지만(변화 ≤ 10⁻⁴), 대부분의 post-hoc 기법들은 noise 추가 후 설명 패턴이 크게 달라집니다. 

특히 **LIME은 $\hat{L} = 6.23$으로 가장 불안정**하며, 반대로 **SENN은 $\hat{L} = 0.01$로 가장 안정적인 설명**을 생성하여 noise가 추가되어도 원본과 거의 동일한 heatmap 패턴을 유지합니다.

# Method

이제 SENN이 어떻게 작동하는지 단계적으로 알아봅시다.

## Linear Classifier에서 출발

가장 간단한 경우부터 시작해봅시다.
선형 분류기는 다음과 같이 표현할 수 있습니다.

$$
f(x) = w^T x + b
$$

이건 이미 해석 가능하죠!
각 feature $x_i$의 기여도가 가중치 $w_i$로 명확하게 드러납니다.

하지만 선형 모델은 복잡한 패턴을 학습하기 어렵습니다.
그래서 저자들은 이 구조를 점진적으로 일반화합니다.

## Concept-based Decomposition

먼저 입력 $x$를 **concept space**로 매핑합니다.

$$
h(x) = [h_1(x), h_2(x), \ldots, h_k(x)]^T
$$

여기서 $h_i(x)$는 $i$번째 concept의 활성화 정도를 나타냅니다.
이 concept들은 사람이 이해할 수 있는 의미 있는 단위입니다 (예: "줄무늬", "날개", "색상" 등).

그리고 각 concept의 중요도는 입력에 따라 달라질 수 있습니다.

$$
\theta(x) = [\theta_1(x), \theta_2(x), \ldots, \theta_k(x)]^T
$$

최종 예측은:

$$
f(x) = \theta(x)^T h(x) = \sum_{i=1}^{k} \theta_i(x) \cdot h_i(x)
$$

이렇게 하면 각 concept $h_i(x)$가 예측에 얼마나 기여했는지가 $\theta_i(x)$로 명확하게 드러납니다!

다음 그림은 전통적인 pixel-level 설명 방법들과 SENN의 concept-level 설명을 비교한 것입니다.
입력 기반 설명들은 픽셀별로 중요한 영역을 heatmap으로 표시하지만, SENN은 추상적인 concept들(C1-C5)의 relevance를 bar chart로 보여줍니다.
또한 각 concept의 의미를 직관적으로 이해할 수 있도록 해당 concept을 가장 강하게 활성화시키는 prototype 숫자들도 함께 제시합니다.

![Concept 기반 설명 비교](https://creatis-myriad.github.io/collections/images/SENN-model/figure2.jpg)

**Figure 2.** 다양한 입력 기반 설명(saliency, Grad*Input, IntGrad, e-LRP, Occlusion, LIME)과 SENN의 concept 기반 설명 비교.

상단 행은 숫자 9, 하단 행은 숫자 2에 대한 결과를 보여줍니다. 좌측부터 Input, Saliency, Grad*Input, Int.Grad., e-LRP, Occlusion, LIME, SENN 순서로 배치되어 있습니다. 

전통적인 입력 기반 설명들은 **pixel-level heatmap**으로 어떤 픽셀이나 superpixel이 예측에 중요한지 표시합니다(빨간색: 긍정적 기여, 파란색: 부정적 기여). 반면 SENN은 **concept-level bar chart**로 5개 concept(C1-C5)의 relevance 점수를 보여줍니다(숫자 9는 C2가 긍정적, 숫자 2는 C3가 긍정적). 

우측에는 각 concept(Cpt 1-5)를 가장 강하게 활성화시키는 5개의 prototype 숫자들이 함께 제시되어, 각 concept가 어떤 추상적 특징을 나타내는지 직관적으로 이해할 수 있도록 합니다.

## Architecture

SENN의 전체 구조는 다음과 같습니다.

$$
f(x) = \theta(x)^T h(x)
$$

여기서:
- $h: \mathcal{X} \to \mathbb{R}^k$: **concept encoder** - 입력을 concept vector로 변환
- $\theta: \mathcal{X} \to \mathbb{R}^k$: **relevance parameterizer** - 각 concept의 중요도를 계산

두 함수 모두 neural network로 구현됩니다.

### Concept Encoder $h(x)$

입력 $x$를 $k$개의 concept으로 분해합니다.
각 concept $h_i(x)$는 해당 concept이 입력에 얼마나 존재하는지를 나타냅니다.

예를 들어 이미지 분류에서:
- $h_1(x)$: "줄무늬" concept의 활성화 정도
- $h_2(x)$: "날개" concept의 활성화 정도
- $h_3(x)$: "색상" concept의 활성화 정도

### Relevance Parameterizer $\theta(x)$

각 concept이 현재 예측에 얼마나 중요한지를 계산합니다.
입력에 따라 달라질 수 있습니다 (예: 얼룩말 이미지에서는 "줄무늬"가 중요하지만, 새 이미지에서는 "날개"가 중요할 수 있음).

## Training Objective

SENN은 세 가지 목표를 동시에 최적화합니다.

### 1. Prediction Accuracy

당연히 예측 정확도가 높아야 합니다.

$$
\mathcal{L}_{\text{pred}} = \mathbb{E}_{(x,y)} [\ell(f(x), y)]
$$

여기서 $\ell$은 task에 맞는 loss function (예: cross-entropy, MSE 등)입니다.

### 2. Concept Alignment

Concept들이 의미 있게 학습되도록 합니다.
만약 concept supervision이 있다면 (즉, 일부 샘플에 대해 concept label이 있다면):

$$
\mathcal{L}_{\text{concept}} = \mathbb{E}_{(x,c)} [\ell_c(h(x), c)]
$$

여기서 $c$는 ground-truth concept vector입니다.

### 3. Stability Regularization

비슷한 입력에 대해 비슷한 설명을 생성하도록 합니다.

$$
\mathcal{L}_{\text{stability}} = \mathbb{E}_{x, x'} [\lVert\theta(x) - \theta(x')\rVert^2 \cdot \mathbf{1}[\lVert x - x' \rVert \leq \epsilon]]
$$

이 regularization은 입력이 비슷할 때 ($\lVert x - x' \rVert \leq \epsilon$) relevance score도 비슷해야 한다는 것을 강제합니다.

### 전체 Loss

$$
\mathcal{L} = \mathcal{L}_{\text{pred}} + \lambda_1 \mathcal{L}_{\text{concept}} + \lambda_2 \mathcal{L}_{\text{stability}}
$$

여기서 $\lambda_1$, $\lambda_2$는 각 regularization term의 가중치입니다.

## Faithfulness 보장

SENN의 faithfulness는 구조적으로 보장됩니다.
왜냐하면 설명 $\theta(x)$와 예측 $f(x) = \theta(x)^T h(x)$가 **같은 계산 그래프**에서 나오기 때문입니다.

즉, $\theta_i(x)$가 크다는 것은 실제로 $h_i(x)$가 예측에 큰 기여를 한다는 것을 의미합니다.
이건 post-hoc 방법들과의 큰 차이점입니다!

# Progressive Generalization

저자들은 선형 분류기에서 시작해서 점진적으로 복잡한 구조로 일반화합니다.

## Stage 1: Linear Model

$$
f(x) = w^T x + b
$$

가장 기본적인 형태입니다.
이미 해석 가능하지만 표현력이 제한적입니다.

## Stage 2: Concept-based Linear Model

$$
f(x) = \theta^T h(x)
$$

여기서 $\theta$는 고정된 가중치이고, $h(x)$는 concept encoder입니다.
Concept들이 학습되지만, relevance는 입력과 무관하게 고정됩니다.

## Stage 3: Input-dependent Relevance

$$
f(x) = \theta(x)^T h(x)
$$

이제 relevance도 입력에 따라 달라집니다.
이게 바로 SENN의 최종 형태입니다!

# Experiments

저자들은 여러 벤치마크 데이터셋에서 SENN을 평가했습니다.

## Stability 평가

비슷한 입력에 대해 설명이 얼마나 안정적인지 측정합니다.
SENN은 기존 post-hoc 방법들(LIME, SHAP 등)보다 훨씬 안정적인 설명을 생성합니다.

다음 그림은 **gradient regularization의 효과**를 명확하게 보여줍니다.
MNIST 숫자 5의 원본부터 최대 perturbation까지 7개의 입력에 대해, gradient regularization을 적용하지 않은 SENN(unregularized)과 적용한 SENN(regularized, λ = 2 × 10⁻⁴)의 concept relevance를 비교합니다.
Unregularized 모델은 작은 perturbation에도 concept relevance의 방향이 바뀌거나 크기가 크게 변하지만, regularized 모델은 perturbation 정도에 관계없이 일관된 패턴을 유지합니다.

![Gradient regularization 효과](https://creatis-myriad.github.io/collections/images/SENN-model/figure8.jpg)

**Figure 8.** Gradient-regularization 유무에 따른 SENN 설명의 안정성 비교.

첫 번째 행은 MNIST 숫자 5의 원본부터 최대 perturbation까지 7개의 입력 이미지를 보여줍니다. 

**두 번째 행의 unregularized 모델**은 perturbation 수준이 증가할수록 concept relevance bar의 방향과 크기가 크게 변하는 것을 볼 수 있습니다(예: C1, C2가 긍정에서 부정으로 바뀌거나, C5의 크기가 크게 변함). 

**세 번째 행의 regularized 모델**(λ = 2 × 10⁻⁴)은 perturbation 정도에 관계없이 concept relevance의 방향과 크기 모두 일관된 패턴을 유지하여 더 robust한 설명을 제공합니다.

## Faithfulness 평가

설명에서 중요한 concept을 제거했을 때 예측이 얼마나 변하는지 측정합니다.
SENN은 설명과 실제 기여도가 잘 일치합니다.

## Accuracy vs Interpretability Trade-off

SENN은 해석 가능성을 유지하면서도 경쟁력 있는 성능을 보입니다.
일부 task에서는 완전한 블랙박스 모델보다 약간 낮은 성능을 보이지만, 해석 가능성의 이점이 이를 상쇄합니다.

# 기존 방법과의 비교

## Post-hoc Methods (LIME, SHAP 등)

- **장점**: 이미 학습된 모델에 적용 가능
- **단점**: 
  - 설명이 모델의 실제 작동과 일치하지 않을 수 있음 (faithfulness 부족)
  - 비슷한 입력에 대해 다른 설명 생성 (stability 부족)
  - 모델 구조에서 직접 나오지 않음 (explicitness 부족)

## Concept Bottleneck Models (CBM)

- **장점**: Concept 기반 설명 제공
- **단점**: 
  - Concept supervision이 필요함
  - Relevance가 입력에 따라 변하지 않음

## SENN

- **장점**: 
  - 세 가지 desiderata 모두 만족
  - Concept supervision이 선택적 (optional)
  - 입력에 따라 relevance가 동적으로 변함
- **단점**: 
  - 모델 구조가 복잡해짐
  - Training이 더 어려울 수 있음

# Limitations

1. **Concept 정의의 어려움**: 어떤 concept을 사용할지 사전에 정의해야 함
2. **Computational Overhead**: Relevance parameterizer를 추가로 계산해야 함
3. **Hyperparameter Tuning**: $\lambda_1$, $\lambda_2$ 등 여러 하이퍼파라미터 조정 필요

# Conclusion

SENN은 **self-explaining** 접근 방식으로 XAI의 세 가지 핵심 요구사항을 동시에 만족하는 모델입니다.

1. **Explicitness**: 예측이 concept 기반 가산 결합으로 명시적으로 표현됨
2. **Faithfulness**: 설명이 모델의 실제 계산 과정과 일치함
3. **Stability**: 비슷한 입력에 대해 안정적인 설명 생성

핵심 contribution:

- 선형 분류기에서 점진적으로 일반화된 구조
- Stability regularization을 통한 안정적 설명 보장
- 구조적으로 faithfulness가 보장되는 아키텍처

> SENN은 post-hoc 방법들의 한계를 극복하고, 모델 자체가 설명 가능하도록 설계된 중요한 시도입니다. 특히 faithfulness와 stability를 동시에 보장한다는 점에서 실무 적용에 유용할 것으로 기대됩니다.

다만 concept의 정의와 하이퍼파라미터 튜닝이 여전히 과제로 남아있습니다.
향후 연구에서는 자동으로 concept을 발견하거나, 더 robust한 training 방법을 제안하는 방향으로 발전할 수 있을 것입니다.