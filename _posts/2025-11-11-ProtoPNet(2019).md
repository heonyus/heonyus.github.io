---
layout: post
title: "ProtoPNet(2019)"
date: 2025-11-11
description: "This Looks Like That: Deep Learning for Interpretable Image Recognition"
tags:
  - XAI
  - Vision
---

# Introduction

이전에 살펴본 [Grad-CAM](https://heonyus.github.io/2025/11/Grad-CAM(2017)-copy.html)이나 [TCAV](https://heonyus.github.io/2025/11/TCAV(2017).html) 같은 방법들은 모델을 학습한 **후에** 설명을 생성하는 post-hoc 방법이었죠.
하지만 이런 방법들은 모델의 실제 작동 방식과 설명이 일치하지 않을 수 있다는 문제가 있습니다.

본 논문에서는 모델 자체가 **"이것이 저것처럼 보인다(This Looks Like That)"**라는 설명을 제공하는 **ProtoPNet (Prototypical Part Network)**을 제안합니다.

> ProtoPNet은 각 클래스마다 **prototype**을 학습하고, 입력 이미지의 특정 부분이 어떤 prototype과 유사한지를 보여줌으로써 예측을 설명합니다.

예를 들어, "이 새는 **저 새의 머리 부분**과 비슷하고, **또 다른 새의 날개 부분**과도 비슷하다"라는 식으로 설명을 제공하는 거죠.
이렇게 하면 사용자가 모델이 왜 그렇게 예측했는지 직관적으로 이해할 수 있습니다.

# 핵심 Idea

먼저 ProtoPNet의 구조를 한 줄로 요약하면:

> **"입력 이미지의 부분(patch)들을, 각 클래스의 prototypical parts와 비교한 뒤, 유사도의 가중합으로 분류한다."**

ProtoPNet은 **prototype-based** 분류 방식을 사용합니다.

각 클래스마다 여러 개의 prototype을 학습하고, 입력 이미지의 각 patch가 어떤 prototype과 가장 유사한지를 계산한 후, 그 유사도 점수들을 가중합해서 최종 점수를 만듭니다.

$$
\text{score}(c \mid x) = \sum_{j=1}^{m} w_{c j} \, g_j(x)
$$

- $x$: 입력 이미지  
- $p_j$: $j$번째 prototype (prototypical part)  
- $g_j(x)$: 입력 이미지 $x$에서 prototype $p_j$와의 최대 유사도 (max-pooled similarity)  
- $w_{cj}$: prototype $p_j$가 클래스 $c$에 기여하는 정도 (last layer weight)  

최종 예측은

$$
\hat{y}(x) = \arg\max_c \text{score}(c \mid x)
$$

로 결정됩니다.

아래 그림은 **"query 이미지가 어떤 prototype들과 어떻게 비교되는지"**를 직관적으로 보여줍니다.

<p align="center">
  <img src="https://www.wjscheirer.com/images/prototype-full.png"
       alt="Query 이미지와 whole-image prototypes, prototypical parts 사이의 유사도 비교 예시"
       width="800" />
</p>

왼쪽은 전체 이미지를 prototype으로 쓰는 경우, 오른쪽은 이미지의 **일부(part)를 prototype**으로 쓰는 경우입니다.

ProtoPNet은 오른쪽과 같이 **이미지의 부분(part) 단위 prototype**을 학습해서 "이 부분이 저 prototypical part와 비슷하다"는 식으로 reasoning 합니다.

이 구조의 장점은 다음과 같습니다:

1. **명시성(Explicitness)**: 예측이 어떤 prototype과 유사한지로 명확하게 표현됨
2. **충실도(Faithfulness)**: 설명이 모델의 실제 계산 과정과 일치함 (같은 계산 그래프)
3. **직관성(Intuitiveness)**: "이것이 저것처럼 보인다"는 사람이 이해하기 쉬운 설명

---

# Architecture

ProtoPNet의 전체 구조는 아래 그림처럼 **기존 CNN backbone + Prototype layer + Fully connected layer**로 이루어져 있습니다.

<p align="center">
  <img src="https://cherrypicked.dev/content/images/2020/11/Screenshot-2020-11-30-at-5.07.42-PM.jpg"
       alt="ProtoPNet 전체 아키텍처: convolutional layers f, prototype layer g_p, fully connected layer h"
       width="900" />
</p>

아키텍처를 수식으로 정리하면 다음과 같습니다.

## 1. Convolutional Backbone $f$

먼저 입력 이미지 $x \in \mathbb{R}^{H_0 \times W_0 \times 3}$를 CNN backbone $f$에 통과시켜 **latent feature map**을 얻습니다.

$$
z = f(x) \in \mathbb{R}^{H \times W \times D}
$$

- $H \times W$: spatial resolution (예: $7 \times 7$)
- $D$: 채널 수 (예: 512)

Feature map의 각 위치 $(h, w)$는 원본 이미지의 작은 patch에 해당하며,  
이를 벡터로 쓰면 $z_{h,w} \in \mathbb{R}^D$로 볼 수 있습니다.

## 2. Prototype 집합 $\{p_j\}$

ProtoPNet은 latent space 상에 **prototype** 벡터들을 학습합니다.

- prototype 집합: $\mathcal{P} = \{p_1, \dots, p_m\}$
- 각 prototype: $p_j \in \mathbb{R}^{H_1 \times W_1 \times D}$

실험에서는 주로 $H_1 = W_1 = 1$로 두어, 각 prototype이 **feature map의 한 위치(=하나의 patch)**를 나타내도록 만듭니다.

각 prototype은 특정 클래스 $c(j)$에 할당되며,  
클래스 $c$에 속한 prototype들의 집합을 $\mathcal{P}_c \subset \mathcal{P}$라고 두겠습니다.

## 3. Prototype Layer $g_p$: 거리 → 유사도 맵

prototype layer는 **generalized convolution**처럼 동작합니다.

1. 우선 feature map의 모든 prototype-sized patch(보통 1×1 patch)를 모은 집합을

   $$
   \mathcal{Z}(x) = \{ z_{h,w} \mid 1 \leq h \leq H,\ 1 \leq w \leq W \}
   $$

   로 두겠습니다.

2. 각 prototype $p_j$에 대해, 모든 patch $z \in \mathcal{Z}(x)$와의 **squared Euclidean distance**를 계산합니다.

   $$
   d_j(z) = \lVert z - p_j \rVert_2^2
   $$

3. 이 거리를 **monotonically decreasing function** $s(\cdot)$을 통해 **similarity score**로 변환합니다.  

   (구현에서는 다음과 같은 함수를 자주 사용합니다.)

   $$
   s(d) = \log\left(
     \frac{d + 1}{d + \varepsilon}
   \right),\quad \varepsilon \approx 10^{-5}
   $$

4. prototype $p_j$에 대해, 전체 patch 중 similarity가 가장 큰 값을 취해 최종 **prototype activation** $g_j(x)$를 얻습니다.

   $$
   g_j(x) = \max_{z \in \mathcal{Z}(x)} s\big( \lVert z - p_j \rVert_2^2 \big)
   $$

즉, $g_j(x)$는

> "입력 이미지 안에 prototype $p_j$와 가장 비슷한 부분이 **얼마나 강하게 존재하는지**"

를 나타내는 스칼라입니다.

이 과정을 모든 prototype에 대해 수행하면

$$
g(x) = \big[ g_1(x), g_2(x), \dots, g_m(x) \big]^T \in \mathbb{R}^m
$$

라는 **prototype-activation vector**를 얻습니다.

## 4. Fully Connected Layer $h$

마지막으로, fully connected layer $h$는 prototype activation들을 가중합해서 클래스를 예측합니다.

- last layer weight matrix: $W \in \mathbb{R}^{C \times m}$  
- $W_{c j}$: 클래스 $c$의 logit에 prototype $p_j$의 activation이 기여하는 가중치

각 클래스 $c$의 logit은

$$
\text{logit}_c(x) = \sum_{j=1}^{m} W_{c j} \, g_j(x)
$$

그리고 softmax를 거쳐 최종 확률을 얻습니다.

$$
P(y = c \mid x) = \frac{\exp(\text{logit}_c(x))}{\sum_{c'} \exp(\text{logit}_{c'}(x))}
$$

---

# Training Objective (Loss & 3-stage Training)

ProtoPNet의 핵심은 **prototype이 실제 training image patch와 1:1로 대응**하고,  
**각 클래스의 중요한 부분이 그 클래스 prototype 주변에 모이도록(latent clustering)** 만드는 것입니다.

이를 위해 학습은 크게 세 단계로 진행됩니다.

1. **Stage 1: Backbone + prototypes joint training (Clst & Sep loss 포함)**
2. **Stage 2: Prototype projection ("push")**
3. **Stage 3: Last layer convex optimization**

## Stage 1 — Joint Training with Clustering / Separation

먼저 CNN backbone $f$와 prototype 집합 $\mathcal{P}$를 **joint하게** 학습합니다.  
이때 last layer $W$는 **특정 패턴**으로 고정해놓습니다.

- 같은 클래스 prototype: $W_{c j} = 1$ (positive evidence)  
- 다른 클래스 prototype: $W_{c j} = -0.5$ 등 (negative evidence)

이렇게 해 두면,

> "자기 클래스 prototype에 가깝게 가는 patch는 **좋은 것**,  
>  다른 클래스 prototype에 가깝게 가는 patch는 **나쁜 것**"

으로 학습되도록 유도할 수 있습니다.

### 전체 Objective

training set $\{(x_i, y_i)\}_{i=1}^n$에 대해 Stage 1의 최적화 문제는

$$
\min_{f, \mathcal{P}} \ \frac{1}{n} \sum_{i=1}^{n}
\Big[
\underbrace{\text{CrsEnt}\big(y_i, f_{\text{ProtoPNet}}(x_i)\big)}_{\text{분류 손실}}
+ \lambda_{\text{clst}} \,\text{Clst}(x_i)
+ \lambda_{\text{sep}} \,\text{Sep}(x_i)
\Big]
$$

입니다.

여기서

- $\text{CrsEnt}$: 일반 cross-entropy loss  
- $\text{Clst}(x_i)$: 같은 클래스 prototype에 **가깝도록** 만드는 loss  
- $\text{Sep}(x_i)$: 다른 클래스 prototype과 **멀어지도록** 만드는 loss  

### Clustering Loss $\text{Clst}$

각 training image가 **자기 클래스 prototype 중 적어도 하나**에 가깝도록 강제합니다.

$$
\text{Clst}(x_i) = \min_{p_j \in \mathcal{P}_{y_i}} \ \min_{z \in \mathcal{Z}(x_i)} \lVert z - p_j \rVert_2^2
$$

- $x_i$의 latent patch들 중  
  "자기 클래스 prototype들 중 하나에 가장 가까운 patch"의 거리를 최소화.

직관적으로,

> "각 이미지 안에는 적어도 하나의 patch가 **자기 클래스의 prototypical part**와 매우 비슷해야 한다."

는 제약입니다.

### Separation Loss $\text{Sep}$

반대로, 각 training image의 latent patch들이 **다른 클래스 prototype**과는 멀어지도록 유도합니다.

$$
\text{Sep}(x_i) = - \min_{p_j \notin \mathcal{P}_{y_i}} \ \min_{z \in \mathcal{Z}(x_i)} \lVert z - p_j \rVert_2^2
$$

음수 부호가 붙어 있으므로, 전체 objective에서 $\text{Sep}$을 **작게** 만드는 것은  
실질적으로 "다른 클래스 prototype과의 최소 거리를 **크게** 만드는 것"과 같습니다.

이 두 term이 합쳐져서 latent space에

- 같은 클래스 patch들은 자기 클래스 prototype 주변에 **클러스터링**되고,  
- 다른 클래스 patch들과는 **잘 분리(separated)** 되는 구조를 형성하게 됩니다.

## Stage 2 — Prototype Projection ("Push" 단계)

Stage 1에서 학습된 prototype $p_j$는 아직 "어디론가 떠 있는 latent 벡터"일 뿐입니다.  
이를 실제 training image patch와 **완전히 일치**시키기 위해 projection을 수행합니다.

각 prototype $p_j$ (클래스 $c(j)$에 속한다고 하자)에 대해,

$$
p_j \leftarrow \arg\min_{z \in \mathcal{Z}(x),\, y = c(j)} \lVert z - p_j \rVert_2^2
$$

즉,

> "자기 클래스의 training image들에서,  
>  latent space 상에서 $p_j$와 가장 가까운 patch"로  
>  prototype을 **끌어당겨(push)** 업데이트합니다.

이렇게 하면

- 각 prototype은 **반드시 어떤 training image patch와 1:1 대응**하게 되고,
- 그 patch를 원본 이미지 상에서 crop 하면 prototype의 **시각화**가 됩니다.

## Stage 3 — Last Layer Convex Optimization

마지막으로 backbone과 prototype은 고정한 채,  
last layer weight $W$만 convex optimization으로 fine-tuning 합니다.

목표는

1. 분류 성능 향상  
2. **sparsity**와 **positive reasoning** 강화  

   - "이 새는 A 클래스다, **왜냐하면 A 클래스의 prototypical parts가 보이기 때문이다**"
   - "다른 클래스의 prototype이 안 보인다"식의 부정적(reasoning by absence) 설명을 줄이기

일반적으로 다음과 같은 objective를 풉니다.

$$
\min_{W} \ \frac{1}{n} \sum_{i=1}^{n} \text{CrsEnt}\big(y_i, W g(x_i)\big) + \lambda_{\text{sp}} \sum_{c,j} \lvert W_{c j} \rvert
$$

subject to (초기 세팅과 일관되게)

- $W_{c j} \ge 0$ if $p_j \in \mathcal{P}_c$  
- $W_{c j} \le 0$ if $p_j \notin \mathcal{P}_c$

이 단계는 linear layer만 최적화하므로 **convex**하며, latent space나 prototype 자체는 바꾸지 않습니다.

---

# Explanation Generation

ProtoPNet의 가장 큰 장점은 **예측과 설명이 완전히 같은 computation graph 위**에 있다는 점입니다.

입력 이미지 $x$에 대해:

1. 각 prototype $p_j$에 대해 similarity map $s_j(h,w)$와  
   max-pooled activation $g_j(x)$를 계산

2. 클래스 $c$에 대해 점수

   $\text{score}(c \mid x) = \sum_j W_{c j} g_j(x)$ 계산

3. 점수 기여도 $W_{c j} g_j(x)$가 큰 prototype들만 골라  

   "이 새의 여기 부분이, 이 prototype과 비슷하기 때문에 클래스 $c$로 본다"  
   라는 **scoring sheet**를 만들 수 있음

아래 그림이 바로 논문에서 보여주는 **ProtoPNet의 reasoning process** 예시입니다.

<p align="center">
  <img src="https://cherrypicked.dev/content/images/2020/11/Screenshot-2020-11-30-at-6.33.28-PM.jpg"
       alt="ProtoPNet의 scoring sheet: red-bellied woodpecker vs red-cockaded woodpecker 예시"
       width="900" />
</p>

- 왼쪽 블록: "왜 이 새를 red-bellied woodpecker로 분류했는가?"  

  - Original image에서 bounding box로 표시된 부분이  
    해당 prototype (머리, 날개 등)과 얼마나 비슷한지  
    activation map과 함께 보여줌

  - 각 row에 `Similarity score × class weight = contribution` 형태로 점수 기여도를 계산

- 오른쪽 블록: 경쟁 클래스(red-cockaded woodpecker)에 대한 evidence

결국 ProtoPNet의 예측은

> "머리 부분이 이 prototype과 비슷하고, 날개 부분이 또 다른 prototype과 비슷해서  
>  red-bellied woodpecker일 가능성이 더 크다"

라는 **사람이 읽을 수 있는 scoring sheet**로 표현됩니다.

---

# 기존 방법과의 비교

## Post-hoc Methods (Grad-CAM, LIME 등)

- **장점**

  - 이미 학습된 임의의 CNN에 바로 적용 가능

- **단점**

  - 모델이 실제로 어떤 internal computation을 하는지와  
    설명이 **분리**되어 있음 (faithfulness 이슈)

  - 유사한 입력에 대해 설명이 급격히 변할 수 있음

  - 보통 "어디를 봤는지(attention)"만 알려주고  
    "무엇과 비슷하다고 생각하는지(case-based reasoning)"는 제공하지 않음

## Prototype-based / Concept-based Methods (TCAV, CBM 등)

- **장점**

  - 인간이 정의한 concept 혹은 prototype을 기반으로 설명

- **단점**

  - 별도의 concept supervision이 필요한 경우가 많음

  - 여전히 "이것이 저것처럼 보인다" 수준의 **부분 단위(part-level)** 설명은 제한적임

## ProtoPNet

- **장점**

  - 모델 구조 자체가 **intrinsic interpretability**를 가짐  

    (예측식 자체가 prototype 가중합)

  - 각 클래스에 대한 **prototypical parts**를 학습하고,  
    입력 이미지의 **해당 patch와 prototype을 직접 비교**해 설명

  - 예측에 사용된 prototype들은 실제 training image patch로 시각화 가능

- **단점**

  - prototype 개수, 할당 방식 등 hyperparameter를 사전에 정해야 함

  - prototype layer 때문에 계산량과 모델 복잡도가 증가

  - background prototype·confounding prototype을 처리하기 위한 추가 기법 필요

---

# Experiments (요약)

- **Datasets**  

  - CUB-200-2011 (새 200종)  

  - Stanford Cars, Stanford Dogs 등

- **결과**

  - 동일 backbone (VGG, ResNet, DenseNet)을 사용한 **baseline CNN**과  
    매우 비슷한 정확도 달성

  - 여러 ProtoPNet을 ensemble 하면 SOTA에 근접한 성능도 가능

  - 무엇보다, 각 예측에 대해 **"이것이 저것처럼 보인다"** 형태의  
    명시적 reasoning path를 제공한다는 점이 핵심 장점

- **Ablation Study**

  - **Clustering loss**: 같은 클래스의 이미지들이 prototype에 모이도록 하는 데 중요
  - **Separation loss**: 다른 클래스의 prototype과 구분하는 데 중요
  - **Prototype push**: Prototype을 실제 이미지 패치로 projection하는 것이 해석 가능성 향상에 중요

---

# Limitations

1. **Prototype 개수 & 분배**

   - 클래스당 몇 개의 prototype을 둘지, 전체 prototype 수를 얼마나 둘지  
     실험적으로 정해야 함

2. **Computational Overhead**

   - 각 prototype에 대해 모든 latent patch와의 거리를 계산해야 하므로  
     일반 CNN보다 계산량이 증가

3. **Confounding Prototype**

   - 배경(나뭇가지, 하늘 등)이 prototype으로 잡히는 문제 →  
     separation loss / prototype pruning 등 추가 기법 필요

4. **Domain Generalization**

   - fine-grained 이미지(새, 자동차 등)에 특화된 구조라  
     다른 도메인에선 별도 튜닝이 필요할 수 있음

---

# Conclusion

ProtoPNet은

> "이미지의 부분들을 **학습된 prototypical parts**와 비교해서  
>  가중합으로 분류하는, case-based interpretable CNN"

입니다.

핵심 contribution을 정리하면:

1. **Prototype layer + generalized convolution**  

   - latent space 상에서 patch–prototype 거리 기반 similarity를 계산

2. **Clustering / Separation Loss + Projection**  

   - prototype들이 실제 training patch와 1:1 대응되도록 만들고  

   - 같은 클래스 patch들은 prototype 주변에 모이고  

     다른 클래스 patch와는 멀어지도록 latent space를 형성

3. **Intrinsic, case-based explanation**  

   - 예측 과정 그 자체가 **"이것이 저것처럼 보인다"**라는 설명 구조를 가짐  

   - scoring sheet 형태로 예측 근거를 시각화 가능

이 논문은 post-hoc saliency/attention 기반 XAI에서 한 단계 더 나아가,  
**모델 구조 자체를 case-based reasoning 형태로 바꾸는** 대표적인 예시라고 볼 수 있습니다.

다만 prototype 개수 결정과 계산 비용이 여전히 과제로 남아있습니다.
향후 연구에서는 자동으로 prototype 개수를 결정하거나, 더 효율적인 유사도 계산 방법을 제안하는 방향으로 발전할 수 있을 것입니다.
