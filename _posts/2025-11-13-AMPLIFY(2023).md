---

layout: post
title: "AMPLIFY(2023)"
date: 2025-11-18
description: "Post Hoc Explanations of Language Models Can Improve Language Models"
tags:
  - XAI
  - LLM
---

# Introduction

이전에 살펴본 [LIME](https://heonyus.github.io/2025/11/LIME(2016).html)이나 [SHAP](https://heonyus.github.io/2025/11/SHAP(2017).html) 같은 post-hoc 설명 방법들은 모델을 학습한 **후에** 설명을 생성하는 방식이었죠.
하지만 이런 방법들은 모델의 실제 작동 방식과 설명이 일치하지 않을 수 있다는 문제가 있습니다.

본 논문은 이런 문제를 다른 각도에서 접근합니다. 
"post hoc explanation이 LLM을 더 잘 만들 수 있는가?"라는 질문에서 출발해서, **AMPLIFY**라는 프레임워크를 제안하면서 그 답이 "그렇다"에 가깝다는 것을 보여줍니다.

요즘 LLM들은 in-context learning만으로도 새로운 태스크를 꽤 잘 학습하죠. 
거기에 사람이 직접 쓴 Chain-of-Thought(CoT) rationale를 few-shot 예제에 붙여 넣으면 복잡한 reasoning 태스크에서 성능이 더 좋아진다는 것도 잘 알려져 있습니다.

하지만 CoT에는 몇 가지 문제가 있습니다:

1. **비용**: 사람 손으로 reasoning을 작성해야 해서 스케일이 안 나옵니다.
2. **편향·노이즈**: 사람이 쓴 rationale은 실제 모델 decision boundary와 다를 수 있고, **faithfulness 문제가 심각**합니다.
3. **태스크 의존성**: 모든 태스크가 CoT로 좋아지는 게 아니며, **언어 이해 중심 태스크에서는 오히려 떨어지는 경우**도 보고됩니다.

그래서 이 논문은 방향을 살짝 틉니다.

> "사람이 아닌 post hoc explanation으로부터 자동으로 rationale을 만들어 LLM에 in-context signal로 넣으면 어떨까?"

핵심 아이디어는 다음과 같습니다:

1. 직접 LLM에서 gradient를 뽑는 건 계산량/접근권한 문제로 거의 불가능
2. 대신 작은 proxy model(GPT-2, BERT 등)을 하나 두고 거기서 gradient 기반 attribution을 계산
3. 그 attribution에서 top-k 중요한 단어를 뽑음
4. `"The key words: ... are important clues to predict [label]..."` 같은 자연어 rationale로 바꿈
5. 이걸 few-shot 예제의 자동 rationale로 LLM prompt에 집어넣음

<p align="center">
  <img src="https://i.imgur.com/PBUTNoI.png" alt="AMPLIFY Framework" width="600" referrerpolicy="no-referrer">
</p>

이렇게 만든 프레임워크가 **AMPLIFY**입니다 (Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations).

정리하면, 이 논문은:

1. **post hoc explanation → 자연어 rationale**로 변환
2. **human CoT 없이**도 LLM in-context 성능을 체계적으로 끌어올릴 수 있다는 것을 보여줌
3. proxy model 선택, 샘플 선택 전략, explanation 방법, 템플릿 등 각 구성요소의 영향을 세밀하게 분석

즉, XAI를 "모델 해석"을 넘어서 **성능 증폭 도구**로 쓰는 꽤 깔끔한 use-case를 제시하고 있습니다.

# 핵심 Idea

두괄식으로 먼저 AMPLIFY의 핵심을 설명해보겠습니다.

AMPLIFY의 구조를 한 줄로 요약하면:

> **"작은 모델에서 얻은 gradient 기반 중요 단어를 이용해 자동 rationale을 만들고, 그걸 few-shot 예제로 LLM에 넣어 오답 패턴을 교정한다."**

이 논문의 핵심은 네 단계로 요약되는 AMPLIFY 프레임워크입니다.

아주 굵게만 보면:

1. **Proxy model 선택**

   * GPT-2, BERT 같은 **작고 오픈소스인 LM**을 선택합니다.
   * LLM(GPT-3/3.5)은 **그대로 black-box**로 두고 gradient는 proxy에서만 계산합니다.

2. **“가장 교정이 필요한” 샘플 선택**

   * validation set에서 LLM이 **틀린 샘플**들을 먼저 모읍니다.
   * 그 중에서 proxy model이 "얼마나 자신감 있게 엉뚱한 label을 찍는지"를 보는 **Misclassification Confidence Score(MCS)**로 상위 샘플을 고릅니다.

   <div align="center">

   $$
   \mathrm{MCS}(x) = f(x)_y - f(x)_{\hat{y}}
   $$

   </div>

   * $x \in \mathbb{R}^N$: 입력 토큰 시퀀스
   * $f: \mathbb{R}^N \to \mathbb{R}^{L}$: proxy model (fine-tuned LM)
   * $f(x)_y$: 모델이 예측한 **잘못된 label $y$**에 대한 확률
   * $f(x)_{\hat{y}}$: **정답 라벨 $\hat{y}$**에 대한 확률

   즉, "틀린 label에 대한 확률 − 정답 label에 대한 확률"이 클수록 **심각하게 틀린** 샘플입니다.

3. **Post hoc explanation으로 top-k 단어 뽑기**

   선택된 각 샘플에 대해, proxy model 기준으로 **정답 label $\hat{y}$에 대한 gradient-based attribution**을 계산합니다.

   가장 단순한 형태는 **Vanilla Gradient**:

   $$\mathrm{attr}_i = \frac{\partial f(x)_{\hat{y}}}{\partial x_i}$$

   * $x_i$: $i$번째 토큰 임베딩
   * $\mathrm{attr}_i$: 그 토큰이 정답 label logit에 미치는 민감도

   실제 논문에서는 주로 **Gradient × Input**을 default로 사용합니다:

   $$\mathrm{attr}_i = x_i \odot \frac{\partial f(x)_{\hat{y}}}{\partial x_i}$$

   입력 벡터 $x_i$와 gradient를 element-wise 곱해서 **특징 크기와 민감도를 함께 반영**합니다.

   subword 토크나이저 때문에 같은 단어가 여러 토큰으로 쪼개질 수 있으므로, 단어 단위 attribution은 **해당 단어를 이루는 토큰들의 attribution 평균**으로 정의합니다.

   그 다음, **가장 attribution이 높은 상위 $k$개 단어**를 뽑습니다 (예: $k = 5$).

4. **자동 rationale → few-shot prompt 구성**

   각 샘플 $(x, \hat{y})$와 그에 대한 top-$k$ 단어 ${w_1, ..., w_k}$를 가지고, 다음과 같은 **자연어 rationale 템플릿**을 만듭니다:

   > "The key words: $w_1, w_2, ..., w_k$ are important clues to predict [Label] as the correct answer."

   그리고 "**입력 문장 + 자동 rationale + 정답 label**" 형태의 few-shot 예제를 $s$개 이어 붙여 **s-shot prompt**를 만들고, 마지막에 테스트 입력만 붙여 LLM에 던집니다:

   ```
   [Input1]
   [Rationale1]
   [Label1]

   ...

   [Inputs]
   [Rationales]
   [Labels]

   [Test Input]
   A:
   ```

   이렇게 되면 LLM은 **"아, 이런 단어들이 이 label을 예측할 때 중요하구나"**라는 신호를 in-context로 학습하게 됩니다.

예시로 sentiment classification을 들면:

* 입력: `"RRR movie has a great story and amazing visuals."`
* post hoc explanation: `"great"`, `"amazing"`이 높은 attribution
* 자동 rationale: "The key words: great, amazing are important clues to predict [positive] as the correct answer."

이 rationale이 few-shot 예제에 포함되고, LLM은 이후 비슷한 문장을 볼 때 **이 단어들을 더 강하게 cue로 쓰도록** 유도됩니다.

이게 바로 "post hoc explanation이 LLM을 **교정하고 증폭(amplify)**하는 방식입니다.

# Method

이제 AMPLIFY가 어떻게 작동하는지 단계적으로 알아봅시다.

## Step 1 – Proxy Model Selection

목표는 LLM(GPT-3/3.5)로는 gradient를 구하기 어렵거나 불가능하니, 대신 **작은 proxy model에서 gradient를 구하고**, 그 **attribution 패턴을 LLM에 이식**하는 것입니다.

논문에서 고려하는 proxy 전략은 크게 두 가지입니다:

1. **Pre-trained LM 그대로 사용**
   * GPT-2 (≈125M), BERT (≈110M) 등
   * task-specific fine-tuning 없이 **pre-trained만으로 attribution을 계산**합니다.

2. **Task-specific fine-tuning**
   * GPT-2/BERT를 target task 데이터로 **E epoch만큼 fine-tuning**한 후, 그 모델을 proxy로 사용합니다.

실험 결과를 보면:
* E=0 (fine-tuning 없이 pre-trained만 사용)일 때도 LLM 성능이 **이미 CoT보다 좋아지는** 태스크가 상당수입니다.
* E=10, E=200으로 fine-tuning을 해도 성능은 조금 더 올라가지만 **marginal gain**인 경우가 많습니다.

즉, **proxy model을 굳이 열심히 fine-tuning할 필요는 없다**는 결론으로 이어집니다.

이건 실용적으로 매우 중요합니다. 작은 모델이라 해도, task마다 일일이 fine-tuning하는 것은 비용이기 때문입니다.

## Step 2 – Few-shot Sample Selection (MCS)

이 단계의 목적은 "어떤 샘플에 대해 rationale을 만들어 prompt에 넣으면 **LLM에 가장 큰 교정 효과**가 날까?"입니다.

논문은 이를 위해 **Misclassification Confidence Score(MCS)**를 정의합니다.

$$
\mathrm{MCS}(x) = f(x)_y - f(x)_{\hat{y}}
$$

* $x$: 입력 문장 (token 시퀀스)
* $f$: proxy model
* $y$: proxy 혹은 LLM이 예측한 **잘못된 label**
* $\hat{y}$: ground truth label
* $f(x)_y$: 잘못된 label에 대한 예측 확률 (혹은 logit)
* $f(x)_{\hat{y}}$: 정답 label에 대한 예측 확률

**해석**:

$f(x)_y$가 크고 $f(x)_{\hat{y}}$가 작을수록, 모델은 "이 오답이 맞다"고 **강하게 믿고 있고**, 동시에 정답을 거의 고려하지 않습니다.
이런 샘플이 **MCS가 큰 샘플**, 즉 "**가장 심각하게 틀린 사례**"입니다.

실제 선택 절차는:

1. 먼저 LLM이 **틀린 validation 샘플 집합**을 모읍니다.
2. 그 샘플들에 대해 proxy model로 MCS를 계산합니다.
3. 가장 MCS가 큰 상위 $s$개의 샘플을 골라 few-shot 예제로 사용합니다.

논문에서는 이 전략(H-MCS)을 아래 기준과 비교합니다.

* Random: 임의로 샘플 선택
* L-MCS: MCS가 **작은** 샘플 선택 (덜 confident misclassification)
* F-Exp: explanation의 **faithfulness 점수가 큰** 샘플 선택
  (top-k 피처를 perturb했을 때 output 변화가 큰 정도)

결과는:

* **H-MCS가 가장 성능이 높고**,
* Random이 최저,
* F-Exp는 의외로 H-MCS, L-MCS보다 못한 경우가 다수입니다.

즉,

> “얼마나 심각하게 틀리는지 (MCS)”가
> “설명이 얼마나 faithful한지”보다 **few-shot 교정 신호로 더 중요하다**

는 흥미로운 결과입니다.

## Step 3 – Rationale Generation (Post Hoc Explanations)

선택된 각 샘플 $(x, \hat{y})$에 대해, proxy model을 기준으로 **정답 label에 대한 gradient-based attribution**을 계산합니다.

대표적인 네 가지 방법을 사용합니다.

1. **Grad (Vanilla Gradients)**

   $$
   \mathrm{attr}_i = \left\lvert \frac{\partial f(x)_{\hat{y}}}{\partial x_i} \right\rvert
   $$

   * $x_i$: $i$번째 토큰 임베딩
   * norm은 보통 L2 norm
   * output이 그 토큰에 얼마나 민감한지 측정

2. **Grad×Inp (Gradient × Input)** – 논문 기본 세팅

   $$
   \mathrm{attr}_i = \left\lvert x_i \odot \frac{\partial f(x)_{\hat{y}}}{\partial x_i} \right\rvert
   $$

   * 입력 값과 gradient를 곱해, "값이 크고 gradient도 큰" 특징을 더 강조합니다.

3. **C-Grad (Contrastive Gradient)**

   * 정답 label과 모델이 실제 예측한 label의 gradient를 비교합니다.

   $$
   \mathrm{attr}_i = \left\lvert \frac{\partial f(x)_{\hat{y}}}{\partial x_i} - \frac{\partial f(x)_{y}}{\partial x_i} \right\rvert
   $$

   "정답 vs 오답" 사이의 차이를 강조하는 contrastive attribution입니다.

4. **C-Grad×Inp (Contrastive Gradient × Input)**

   * 위 C-Grad에 input을 곱한 버전입니다.

   $$
   \mathrm{attr}_i = \left\lvert x_i \odot \left( \frac{\partial f(x)_{\hat{y}}}{\partial x_i} - \frac{\partial f(x)_{y}}{\partial x_i} \right) \right\rvert
   $$

토큰 단위 attribution을 구한 뒤,

* 같은 단어에 속한 토큰들의 attribution 평균을 내어 **단어 단위 attribution**을 얻고,
* 가장 큰 상위 $k$개 단어를 **$w_1, \dots, w_k$**로 선택합니다.

실험적으로는,

* Grad×Inp 및 C-Grad×Inp가 평균적으로 가장 성능이 좋지만,
* 네 방법 모두 성능 차이가 **크진 않고**,
  LLM은 이들 사이의 선택에 꽤 **robust**합니다.

## Step 4 – Prompt Design for LLMs

마지막으로, 선택된 샘플과 top-$k$ 단어를 이용해 prompt를 구성합니다.

### 기본 rationale 템플릿

가장 단순한 템플릿은:

> "The key words: $w_1, w_2, ..., w_k$ are important clues to predict [Label] as the correct answer."

이 템플릿은 **태스크 비독립적인 공통 템플릿**입니다.

* 이 문장 자체가 LLM에게 "이 단어들을 근거로 이 label을 선택해야 한다"는 **meta-signal**을 줍니다.
* 인간 CoT처럼 step-by-step reason을 강요하진 않지만, 어떤 feature가 decision에 중요했는지를 정리해주는 정도입니다.

### 태스크 특화 템플릿

일부 태스크에서는, 질문 문장에 맞춰 템플릿을 조금 바꾸면 도움이 됩니다.

예: **Causal Judgment** 태스크의 경우, 질문이

> "How would a typical person answer each of the following questions about causation?"

형태라서, rationale도

> "After observing the key words: ... a typical person would respond with [Label] as the correct answer."

처럼 바꿔주면 GPT-3 성능이 60.5% → 63.1%로 소폭 상승합니다.

다만 문장부호나 사소한 phrasing 변화에는 성능이 거의 민감하지 않았고, "질문 구조에 의미적으로 잘 맞는지" 정도가 중요해 보입니다.

### Hyperparameters: (k, s)

* $k$: top-$k$ 단어 개수 (중요 단어 몇 개를 보여줄 것인가?)
* $s$: few-shot 샘플 개수

논문에서는 여러 조합을 비교하며, 전체적으로 **$(k=7, s=10)$ 근처가 가장 좋은 경향**을 보입니다.

흥미로운 점은 CoT는 $s$를 늘리면 **사람이 쓸 rationales가 비례해서 늘어야** 하지만, AMPLIFY는 **모두 자동 생성**이므로 $s$, $k$를 늘리는 데 인력 비용이 없습니다.
따라서 prompt 길이만 허용한다면, 스케일 관점에서 AMPLIFY가 훨씬 유리합니다.

# Experiments

## Datasets & Tasks

논문은 **Big-Bench-Hard** 및 관련 벤치마크 중 **언어 이해·추론 중심 태스크** 7개에 대해 실험합니다.

1. **Snarks**

   * 주어진 문장들 중 **비꼬는(sarcastic) 문장**을 골라내는 태스크.

2. **Causal Judgment**

   * 사건 설명을 보고 **어떤 factor가 원인인지**를 판단.

3. **Ruin Names**

   * 아티스트/영화 이름을 **웃기게 변형한 이름**을 맞추는 태스크.

4. **Formal Fallacies**

   * 논리적 구조를 보고 **형식 논리 오류(formal fallacy)**가 있는지 판단.

5. **Salient Translation Error Detection (STED)**

   * 두 번역 문장 중 하나를 고르고,
   * 그 번역에서 **6개 중 어떤 타입의 오류**가 있는지 맞추는 태스크.

6. **CommonsenseQA**

   * 일상 상식에 기반한 객관식 QA.

7. **Coin Flip (OOD)**

   * 동전 던지기 시나리오 기반 **symbolic reasoning** 태스크 (Out-of-Distribution).

LLM으로는 **GPT-3 (175B)**와 **GPT-3.5**를 사용합니다.

## Main Results: AO / CoT vs AMPLIFY

기본 baseline 두 가지:

* **AO (Answer-Only)**:

  * few-shot (input, label)만 주는 표준 prompting.
* **CoT**:

  * 사람-annotated rationale + label 형태의 prompt.

여기에 **AMPLIFY**를 추가해 3자 비교를 합니다.

아래 표는 GPT-3와 GPT-3.5에서 AO, CoT, AMPLIFY의 성능을 비교한 전체 실험 결과입니다. 
각 태스크에서 **AMPLIFY가 가장 높은 성능을 기록한 경우는 굵게 표시**되어 있으며, Random baseline, SOTA, Human-Rater 성능도 함께 비교하고 있습니다.

<p align="center">
  <img src="https://i.imgur.com/NX11NAA.png" alt="Table 1: Few-shot prompting performance comparison on seven datasets" width="500" referrerpolicy="no-referrer">
</p>

**주요 관찰 사항:**

* **GPT-3.5 기준**: AMPLIFY는 7개 태스크 중 **6개 태스크에서 최고 성능**을 달성했으며, 전체 평균에서도 **72.7%**로 AO(60.8%), CoT(62.9%)를 크게 상회합니다.
* **GPT-3 기준**: AMPLIFY는 7개 태스크 중 **4개 태스크에서 최고 성능**을 달성했으며, 전체 평균에서도 **67.2%**로 AO(56.8%), CoT(58.0%)보다 우수합니다.
* **Human-Rater 비교**: 대부분의 태스크에서 Human-Rater의 평균 성능(73.5%)은 AMPLIFY(GPT-3.5: 72.7%)에 근접한 상한선을 제시합니다.
* **SOTA 대비**: AMPLIFY는 대부분의 태스크에서 기존 SOTA를 넘어서는 성능을 보여줍니다 (예: Snarks 91.6% vs SOTA 71.3%, Causal Judgment 76.3% vs SOTA 62.1%).

아래에서는 각 태스크별 주요 결과를 상세히 살펴보겠습니다.

## 주요 결과 상세 분석

* **Snarks**

  * GPT-3.5 기준:

    * AO: ≈75%
    * CoT: ≈69%
    * **AMPLIFY: ≈91.6%**
  * CoT보다 **20%p 이상 상승**, AO보다도 크게 상회.

* **Causal Judgment**

  * GPT-3.5:

    * AO: ≈57.8%
    * CoT: ≈63.1%
    * **AMPLIFY: ≈76.3%**

  아래 그림은 Causal Judgment 태스크에서 AO, CoT, AMPLIFY의 실제 응답 예시를 보여줍니다. 
  CoT는 "never supposed" 같은 중요한 정규(normative) 정보를 놓치는 반면, AMPLIFY는 이러한 핵심 키워드를 정확히 포착해 올바른 답변을 도출합니다.

  <p align="center">
    <img src="https://i.imgur.com/HlozD2s.png" alt="Figure 2: Causal Judgment task example comparing AO, CoT, and AMPLIFY" width="500" referrerpolicy="no-referrer">
  </p>

* **Ruin Names, Formal Fallacies, STED**

  * GPT-3 / 3.5 모두에서

    * AMPLIFY가 AO/CoT보다 꾸준히 우위.

* **CommonsenseQA**

  * GPT-3.5:

    * AO: ≈75.7%
    * CoT: ≈75.2%
    * **AMPLIFY: ≈77.9%**
  * gain은 상대적으로 작지만 **일관되게 최고 성능**.

* **Coin Flip (OOD)**

  * CoT도 잘 먹히는 symbolic reasoning 태스크지만,
  * GPT-3.5:

    * AO: ≈52.9%
    * CoT: ≈61.0%
    * **AMPLIFY: ≈65.3%**

전체 평균으로 보면:

* GPT-3.5 기준 **모든 태스크 평균 정확도**:

  * AO: 60.8%
  * CoT: 62.9%
  * **AMPLIFY: 72.7%**

즉, **평균 10%p 이상 향상**입니다.

## Proxy Model Fine-tuning의 영향

Proxy model로 GPT-2를 쓸 때, fine-tuning epoch $E$에 따른 성능 변화도 봅니다.

* $E = 0$: pre-trained GPT-2 그대로
* $E = 10$: 약간 fine-tuning
* $E = 200$: best validation 성능까지 충분히 fine-tuning

결론:

* $E=0$에서도 대부분의 태스크에서 **이미 CoT보다 우수**하거나 근접한 성능.
* $E=10, 200$으로 갈수록 조금씩 올라가지만,

  * 평균 개선폭은 제한적입니다.
* BERT를 proxy로 썼을 때도 비슷한 패턴.

실무적으로는:

> “pre-trained GPT-2/BERT를 proxy로 그대로 쓰면서,
> **추가 fine-tuning 없이** AMPLIFY를 돌려도 충분히 좋은 성능을 얻을 수 있다.”

는 메시지를 줍니다.

## Sample Selection 전략 비교

Step 2에서 언급한 네 가지 전략 비교 결과:

* **H-MCS (highest MCS)**:

  * GPL-3/3.5 모두에서 **최고 평균 성능**.
* **L-MCS**:

  * H-MCS보다는 조금 떨어지지만, Random보다는 좋습니다.
* **F-Exp**:

  * faithfulness 높은 샘플만 골랐지만,
  * H-MCS, L-MCS보다 못한 경우가 많습니다.
* **Random**:

  * 일관되게 가장 낮은 성능.

이 결과는 **“어디서 가장 자신 있게 틀리는지”**를 기준으로 샘플을 고르는 것이
설명 품질 기반 선택보다도 **few-shot 교정 효과가 더 크다**는 점을 보여줍니다.

## Explanation Method 비교

네 가지 explanation method(Grad, Grad×Inp, C-Grad, C-Grad×Inp)를 비교한 결과:

* 평균적으로 **Grad×Inp 또는 C-Grad×Inp**가 약간 더 좋습니다.
* 그러나 전반적인 성능은 **네 방법 모두 비슷한 수준**이며,

  * GPT-3.5 기준 Snarks 등 대부분 태스크에서 88–92% 사이로 안정적.

즉, **post hoc explanation의 변종 선택에는 크리티컬하지 않다**는 점에서
방법론이 꽤 robust하다는 인상을 줍니다.

## GSM8K 및 기타 태스크

저자들은 수학 문제 데이터셋 **GSM8K**에 대해서도 시험해 봅니다.

* GSM8K (GPT-3 기준):

  * AO: 22.7%
  * CoT: 43.5%
  * AMPLIFY: 27.4%

여기서는 **CoT가 압도적으로 우수**합니다.

이유는 명확합니다.

* 수학 문제는 **multi-step symbolic reasoning**이 필요하고,
* gradient-based attribution이 제공하는 signal은

  * 특정 단어가 정답에 중요하다는 수준이지,
  * **수학적 단계(step-by-step reasoning)**를 제공해 주진 못합니다.

저자들도 이 점을 솔직하게 인정하면서:

> AMPLIFY는 **언어 이해·상식·비꼬기·논리 오류 판단** 같은
> **language understanding-heavy** 태스크에서는 CoT를 능가하지만,
> GSM8K처럼 **기계적인 수학 추론**에는 CoT가 여전히 더 적합하다.

고 정리합니다.

# Conclusion

이 논문의 핵심 기여를 정리하면 다음과 같습니다.

1. **AMPLIFY 프레임워크 제안**

   * post hoc explanation (gradient-based attribution)을 **자연어 rationale로 변환**해,
   * LLM in-context learning의 성능을 **자동으로 증폭**하는 첫 systematic framework 중 하나입니다.

2. **Human-annotated CoT를 대체 가능한 옵션으로 제시**

   * 사람 CoT 없이도,
   * 평균적으로 AO/CoT를 **10–25%p 상회하는 성능**을 얻을 수 있음을 실험으로 보입니다.
   * 특히, **언어 이해·상식·논리 오류·비꼬기 인식** 같은 태스크에서 큰 improvement.

3. **Proxy model, 샘플 선택, explanation 방법, 템플릿에 대한 상세 ablation**

   * **pre-trained proxy model만으로도 충분한 성능**을 얻을 수 있고,
   * **Misclassification Confidence Score(MCS)** 기반 샘플 선택이 가장 효과적이며,
   * Grad×Inp 계열 method가 소폭 유리하지만 전체적으로 robust하다는 것을 보여줍니다.

4. **XAI의 “용도 전환”**

   * post hoc explanation을 단순히 모델을 이해하는 도구를 넘어,
   * **LLM의 오류 패턴을 교정하는 supervision signal**로 활용할 수 있음을 보여줍니다.

## 장점

* **스케일러블**: CoT처럼 사람 손이 필요 없으므로, 새로운 태스크에 광범위하게 적용 가능
* **모델 접근 제약 완화**: LLM은 완전 black-box여도 되고, gradient는 작은 오픈소스 proxy에서만 계산하면 됩니다
* **robust한 성능 향상**: 다양한 태스크와 LLM 세팅에서 일관된 성능 향상

## 한계 및 향후 연구 방향

* **Post hoc explanation의 고질적 문제 상속**
  
  explanation의 robustness, stability, disagreement 등 기존 XAI 문제들을 그대로 떠안습니다.
  설명이 바뀌면 자동 rationale도 바뀌고, 이는 곧 LLM behavior의 변동으로 이어질 수 있습니다.

* **태스크 의존성**

  GSM8K 같은 **multi-step math reasoning**에는 CoT가 훨씬 효과적입니다.
  AMPLIFY는 **언어 의미·상식·논리 구조**에 기반한 태스크에서 특히 강합니다.

* **토큰 수준 attribution의 한계**

  "어떤 단어가 중요하다"는 signal은 주지만, 그 단어들이 **어떻게 상호작용하는지**(higher-order reasoning)는 직접적으로 표현하지 못합니다.

향후 연구로는:

* **개념 수준(concept-level)의 post hoc explanation**을 사용해 "단어" 대신 "개념(bottleneck concept, proto 등)"을 rationale로 쓰는 방향
* **KG나 structured reasoning module과 결합**하여 단순 important word 이상의 **structured rationale**을 생성하는 방향
* **task-adaptive rationale 템플릿 자동 최적화** 등도 자연스러운 확장선입니다.

# Appendix

## 전체 흐름 한 줄 요약

1. **작은 언어모델(GPT-2 같은 proxy model)을 하나 고르고**
2. **그 모델이 “자신 있게 틀리는” 샘플을 골라** (Few-shot 예제로 쓸 샘플 선택)
3. 그 샘플들에 대해 **post-hoc explanation으로 “중요한 단어들”을 뽑고**
4. 그 단어들을 자연어 rationale로 바꿔 **LLM용 few-shot 프롬프트로 포맷팅**하는 구조입니다.

이제 그림을 번호별로 보겠습니다.

## 1️⃣ Proxy Model Selection

<p align="center">
  <img src="https://i.imgur.com/vdTv5PO.png" alt="Proxy Model Selection" width="200" referrerpolicy="no-referrer">
</p>

* 여기서 **GPT-2**는 “proxy model”입니다.

  * 실제로 우리가 성능을 올리고 싶은 모델은 GPT-3 / GPT-3.5 같은 **큰 LLM**인데,
  * 그 모델에서는 gradient를 뽑기 어렵기 때문에, 훨씬 작은 모델(GPT-2, BERT 등)을 대신 써서 **gradient 기반 post-hoc explanation**을 계산합니다.
* Step 1의 의미:

  * “설명(explanation)을 만들 때 쓸, 계산 가능한 작은 모델을 하나 고른다.”

## 2️⃣ Few Shot Sample Selection

<p align="center">
  <img src="https://i.imgur.com/L4YjzuO.png" alt="Few Shot Sample Selection" width="200" referrerpolicy="no-referrer">
</p>

* 여기서 **MCS(Misclassification Confidence Score)**는

  * “proxy 모델이 **얼마나 자신 있게 틀리고 있는지**”를 수치로 나타낸 점수입니다.

* 수식으로는 대략 이렇게 정의됩니다.

  $$
  \mathrm{MCS}(x) = f(x)_y - f(x)_{\hat{y}}
  $$

  * $x$: 입력 문장 (Input 1, Input 2, …)
  * $f$: proxy model(GPT-2)의 예측 함수
  * $y$: 모델이 예측한 **오답 라벨**
  * $\hat{y}$: **정답 라벨**
  * $f(x)_y$: 오답에 대한 확률
  * $f(x)_{\hat{y}}$: 정답에 대한 확률

* MCS가 클수록

  * “정답은 무시하고 틀린 라벨을 강하게 믿는 샘플”이라는 뜻입니다.

* 그림에서 **MCS = 3.3인 Input 1**이 제일 위에 있는 이유가,

  * 이런 샘플이 **LLM을 교정하기 위한 few-shot 예제로 가장 가치 있다**고 보기 때문입니다.

* 이 단계의 역할:

  * **LLM이 틀린 샘플들 중에서**, proxy model 기준 MCS가 높은 것들을 골라
    나중에 few-shot 예제로 쓸 **소수의 샘플**을 선택합니다.

## 3️⃣ Compute Explanations

<p align="center">
  <img src="https://i.imgur.com/1cZotvh.png" alt="Compute Explanations" width="200" referrerpolicy="no-referrer">
</p>

* Step 2에서 선택된 각 입력(예: Input 1)에 대해,

  * proxy model에서 **정답 라벨 $\hat{y}$ 기준 gradient 기반 attribution**을 계산합니다.
* 그 결과,

  * 각 단어별로 “이 단어가 정답 라벨을 예측하는 데 얼마나 중요한지” 점수가 나오고,
  * 그 중 **상위 k개 단어**를 `[word_1], [word_2], [word_3], ...`로 뽑습니다.
* 막대 그래프는 각 단어의 **중요도(importance)**를 시각화한 것이라고 보면 됩니다.
* 이 단계의 역할:

  * 선택된 few-shot 샘플들에 대해
    **post-hoc explanation(특징 중요도) → “중요한 단어 리스트”**로 변환.

## 4️⃣ Format Prompts For Large Language Model

<p align="center">
  <img src="https://i.imgur.com/k5i2Kso.png" alt="Format Prompts For Large Language Model" width="300" referrerpolicy="no-referrer">
</p>

* 여기서 하는 일은 간단히 말해:

  * “**입력 문장 + 자동 rationale + 정답 라벨**을 이어 붙여 LLM용 few-shot 프롬프트를 만든다.”

* 그림에 있는 예시는 대략 이런 형태입니다.

  > Q: [Input 1].
  > The key words [word_1], [word_2] and [word_3] are important clues to predict [label 1] as the correct answer.
  > The correct answer is [label 1].

* 이렇게 만든 텍스트 블록을 여러 개 이어 붙이면,

  * LLM 입장에서는
    “아, 이런 식으로 **특정 단어들이 이 라벨의 단서 역할을 하는구나**”
    라는 패턴을 in-context로 학습하게 됩니다.

* 마지막에 테스트 입력만 던져주면,
  LLM은 이런 few-shot 패턴을 이용해 더 나은 예측을 하게 됩니다.

## 정리

1. 작은 proxy 모델(GPT-2)을 하나 고름름
2. 그 모델이 **가장 자신 있게 틀리는 샘플**들을 MCS 기준으로 골라 few-shot 후보로 삼음음
3. 그 샘플들에 대해 gradient 기반 설명으로 “중요 단어”를 뽑음음
4. 이를 자연어 rationale 문장 + 라벨 형태로 LLM 프롬프트에 넣어 in-context learning을 **증폭(amplify)**하는 구조
