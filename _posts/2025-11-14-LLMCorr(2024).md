---
layout: post
title: "LLMCorr(2024)"
date: 2025-11-14
description: "Harnessing Large Language Models as Post-hoc Correctors"
tags:
  - XAI
  - LLM
  - Post-hoc-Correction
---

### 1. Introduction

> "비싸게 다시 학습(fine-tuning)하지 않고, **이미 학습된 ML 모델의 예측을 LLM으로 얼마나 잘 고칠 수 있을까?**"

최근 ML 모델(특히 GNN, 대형 Transformer 등)은 점점 커지고 있고, 좋은 성능을 위해선 많은 데이터와 연산 자원이 필요합니다. 한 번 학습한 모델을 다시 고치고 싶을 때마다 전체를 재학습하는 것은 비용적으로도, 엔지니어링 측면에서도 부담이 크죠.

<p align="center">
  <img src="https://i.imgur.com/0AjWXY8.png" alt="LLMCORR – 고정된 LLM을 post-hoc corrector로 사용하는 구조" width="400" referrerpolicy="no-referrer">
</p>

위 그림은 이런 고민을 시각적으로 정리한 그림입니다. 위쪽 박스는 우리가 이미 익숙한 **기존 ML 모델 학습 파이프라인**이고, 아래쪽 박스는 학습이 끝난 뒤 **고정된 LLM을 이용해 예측을 보정하는 post-hoc correction 단계**를 보여주죠. 이 논문의 목표는 바로 이 아래쪽 박스를 얼마나 잘 설계할 수 있는지를 체계적으로 분석하는 것입니다.

이 논문은 이 문제를 정면으로 겨냥해서, **LLMCORR**라는 프레임워크를 제안합니다. 핵심 아이디어는 다음과 같습니다.

* 기존 **ML 모델은 그대로 둔 채** (파라미터 고정)

* 그 모델이 **validation set에서 어떤 데이터를 틀리고, 어떤 패턴으로 오차를 내는지**를 요약해서

* **LLM에게 "너는 보정기(corrector)야, 이 모델의 실수 패턴을 학습해서 새 예측을 고쳐줘"라고 시키는 것**입니다.

주요 기여를 정리해보면:

1. **Training-free post-hoc corrector 프레임워크 LLMCORR 제안**

   * 어떤 ML 모델이든 "검은 상자(black-box)"로 두고 그 위에 LLM 보정 레이어를 얹습니다.

   * 모델 재학습 없이, LLM 호출만으로 성능 개선을 노립니다.

2. **Contextual Knowledge Database + Embedding-based Retrieval 설계**

   * (입력, 모델 예측, 정답)을 포함하는 **맥락 지식 DB**를 만들고

   * LLM의 입력 토큰 한계를 넘지 않도록 **임베딩 기반 top-k retrieval**로 중요한 예시만 골라 prompt에 넣습니다.

3. **Self-correction 메커니즘 도입**

   * LLM이 원래 예측값을 과도하게 바꾸려 할 때,

   * 다시 한 번 "이렇게 크게 바꿔도 확실해?"라고 재질문하여 할루시네이션을 완화합니다.

4. **실험적으로 최대 39%까지 성능 향상**

   * 6개 OGB 분자 특성 예측 데이터셋에서, LM/GCN/GIN/HIG/PAS/TAPE 등 다양한 기본 모델 위에 LLMCORR를 붙여

   * ROC-AUC 및 RMSE 기준으로 일관된 성능 향상을 보여줍니다.

   * 추가로 텍스트 분석 과제에서도 효과를 보입니다.

5. **LLM을 predictor로 직접 쓰는 것보다 "post-hoc corrector"로 쓰는 것이 훨씬 효율적이라는 실증**

   * LLM에 직접 분자 특성을 예측하게 하면 성능이 미흡하지만,

   * 기존 ML의 예측을 "고치는 역할"로 쓰면 큰 이득을 얻을 수 있음을 보여줍니다.

요약하자면, 이 논문은 "LLM을 새로운 모델로 훈련하는 대신, **기존 모델의 사후 보정기(post-hoc corrector)**로 쓰자"는 매우 실용적인 방향을 제시하고 있습니다. 

### 2. 핵심 Idea / Method

이 논문의 핵심 아이디어는 간단히 말하면:

> **"Validation set에서 모델이 어떤 식으로 틀리는지 LLM에게 보여주고, 그 패턴을 이용해 test 예측을 고치게 하자"**입니다.

<p align="center">
  <img src="https://i.imgur.com/H73kAst.png" alt="LLMCORR 전체 파이프라인 – ML 모델 학습과 LLM 기반 post-hoc correction 흐름" width="600" referrerpolicy="no-referrer">
</p>

그림 2를 보면 LLMCORR의 전체 흐름이 한눈에 들어옵니다. 위쪽 (A) 블록은 기존 ML 모델이 분자 데이터셋으로 학습되고, validation/test 데이터에 대해 예측을 내는 과정이고, 아래쪽 (B) 블록은 이 예측 결과와 데이터 특징, 그리고 과거 예시들을 모아 **프롬프트 텍스트를 구성한 뒤 LLM API를 호출해 보정된 예측과 설명을 얻는 과정**을 보여줍니다. 이제 이 그림을 기준으로 각 단계를 조금 더 세분화해서 살펴보겠습니다.

조금 단계적으로 풀어보면 다음과 같습니다.

1. **ML 모델은 이미 학습이 끝난 상태**

   * 임의의 ML 모델 $f_{\mathrm{ML}}$을 생각합시다. (예: GCN, GIN, TAPE, DeBERTa 등)

   * 우리는 이 모델의 **파라미터를 건드리지 않습니다.**

2. **Contextual Knowledge Database $D$ 구축**

   * Training/validation 데이터에 대해

     * 입력 (분자 그래프/SMILES 등)

     * 모델 예측값 $\hat{y}$

     * 정답 레이블 $y$

   * 이 세 가지를 모두 기록해서,

     **"어떤 입력에 대해 모델이 어떤 값을 내고, 그게 맞았는지 틀렸는지"**를 전부 저장한 DB $D$를 만듭니다.

3. **새로운 쿼리 데이터 $G_u$에 대해**

   * 먼저 ML 모델로 **1차 예측** $\hat{y}_u = f_{\mathrm{ML}}(G_u)$를 얻습니다.

   * 이 쿼리 데이터와 비슷한 과거 사례를 DB $D$에서 찾기 위해

     텍스트 인코더 $f_{\mathrm{Emb}}$로 임베딩을 만들고, **top-k 유사 사례**를 골라냅니다.

4. **Prompt 구성**

   * LLM에게 줄 prompt에는 대략 이런 정보들이 들어갑니다:

     * "너는 이 ML 모델의 **post-hoc corrector**이다"라는 **역할 지시(Instruction)**

     * Training set에서의 (입력, 정답) 예시들

     * Validation set에서의 (입력, 모델 예측, 정답) 예시들

       → 여기서 **모델이 어떤 패턴으로 오차를 내는지**가 드러납니다.

     * 마지막에, 쿼리 데이터 $(G_u, \hat{y}_u)$를 보여주고

       "이 모델의 예측을 참고하되, 위의 패턴을 활용해 **필요하면 보정된 예측을 제안하라**"고 질문합니다.

5. **LLM의 출력 = 보정된 예측 $\tilde{y}_u$**

   * LLM은 이 prompt를 보고,

     * 최종 예측 $\tilde{y}_u$

     * 각 클래스(또는 값)에 대한 "확률" 또는 신뢰도

     * 왜 그렇게 바꾸었는지에 대한 자연어 설명

   * 을 함께 돌려줍니다.

6. **Self-correction (자기 교정)**

   * 만약 $\tilde{y}_u$가 원래 예측 $\hat{y}_u$와 **너무 크게 차이**가 나면,

     * 분류: 레이블을 뒤집었다거나

     * 회귀: 값이 20% 이상 크게 바뀐 경우 등

   * 또 다른 prompt를 만들어

     > "원래 예측은 $\hat{y}_u$였고 네가 제안한 값은 $\tilde{y}_u$인데, 이 정도로 바꾸는 게 정말 타당한지 다시 한 번 검토해라"

   * 라고 LLM에게 다시 묻습니다. 이를 통해 **과도한 할루시네이션성 보정**을 줄이려고 합니다.

이 구조 덕분에 LLM은 **"예측 모델"이 아니라 "오차 패턴을 읽어내는 메타 보정기"** 역할을 하게 됩니다. 즉,

* $f_{\mathrm{ML}}$은 특정 도메인 데이터에 대해 열심히 학습된 모델이고,

* $f_{\mathrm{LLM}}$은 그 모델의 행동 패턴(어디서 잘 틀리는지)을 "텍스트로 된 로그(context)"를 통해 학습해서,

  **후처리(post-processing) 관점에서 성능을 올려주는 모듈**로 작동합니다.

### 3. Background

#### 3.1 Large Language Models & In-Context Learning

LLM은 Transformer 기반 언어 모델로, 대규모 텍스트 데이터에 대해 다음 토큰 예측을 학습하면서 **상식, 추론, 언어 이해 능력**을 획득한 모델입니다. GPT-3, GPT-3.5, GPT-4, Llama2 등이 대표적이죠.

이 논문에서 특히 중요한 개념은 **In-Context Learning(ICL)**입니다.

* 별도의 파라미터 업데이트 없이,

* Prompt 안에 들어 있는 몇 개의 예시 (입력–출력 쌍)를 보고,

* 마치 "그 자리에서 새 태스크를 학습한 것처럼" generalization을 하는 능력입니다.

LLMCORR는 바로 이 ICL 능력을 이용해:

* "모델이 어떤 경우에 어떤 식으로 틀렸는지"라는 **맥락(context)**을 prompt로 보여주고

* 그 패턴을 기반으로 새 데이터의 예측을 **보정(correct)**하게 만드는 방식입니다.

#### 3.2 Post-hoc Correction for ML Models

일반적으로 모델 성능을 올리는 방법은:

* 더 좋은 아키텍처 설계

* 더 많은/좋은 데이터로 재학습

* 추가적인 fine-tuning

등 "모델 내부"를 수정하는 방향이 많습니다.

하지만 이런 접근은:

* 연산 비용이 크고

* 이미 배포된 시스템에서는 적용하기 어려운 경우가 많습니다.

그래서 일부 연구에서는 **post-processing / post-hoc correction** 방식으로

* 기존 예측을 온도 스케일링, 캘리브레이션, label propagation 등으로 보정하는 시도를 해왔습니다.

* 다만 대부분 **특정 도메인(예: 그래프)**이나 **특정 태스크**에 한정되어 범용성이 떨어졌습니다.

이 논문은:

* 어떤 ML 모델이든 "예측 로그"를 텍스트로 만들어 LLM에 먹일 수 있다면,

* **LLM을 범용 post-hoc corrector로 쓸 수 있다**는 방향성을 보여줍니다.

#### 3.3 Molecular Property Prediction

실험의 주요 타깃은 **분자 특성 예측(molecular property prediction)**입니다.

* 입력: 분자 그래프(노드=원자, 엣지=결합) + SMILES 문자열

* 출력:

  * 분자 활성을 나타내는 **이진 레이블** (예: HIV 활성 여부, BBB 침투 가능 여부 등)

  * 혹은 **연속 값** (예: 용해도, 자유에너지, 지용성 등)

논문에서는 OGB(OGB-MOL) 벤치마크의 6개 데이터셋을 사용합니다:

* ogbg-molbace, ogbg-molbbbp, ogbg-molhiv (classification)

* ogbg-molesol, ogbg-molfreesolv, ogbg-mollipo (regression)

기본 모델로는 LM(DeBERTa), GCN, GIN, HIG, PAS, TAPE 등을 사용하며, 이들 위에 LLMCORR를 얹어 성능을 측정합니다. 

### 4. Method (상세 설명)

#### 4.1 문제 정의와 표기

각 분자는 다음과 같이 표현됩니다.

* 분자 그래프

  $$G = (S, \mathcal{G}, D)$$

  * $S$: 분자를 나타내는 **SMILES 문자열**

  * $\mathcal{G}$: 분자의 **기하학적 구조**(그래프 구조)

  * $D$: 원자 특징과 그래프 구조를 텍스트로 풀어쓴 **설명(description)**

* 레이블

  $$y \in \mathcal{Y}$$

  * 분류 문제: $\mathcal{Y}$는 클래스 집합 (예: ${0,1}$)

  * 회귀 문제: $\mathcal{Y} \subset \mathbb{R}$

데이터셋은 다음과 같이 나뉩니다.

* 전체 분자 집합: $\mathcal{M} = {G_1, \dots, G_m}$

* 레이블이 있는 부분집합: $\mathcal{M}_T \subset \mathcal{M}$

  * 다시 학습/검증/테스트로 분할:

    * $\mathcal{M}_{\text{train}}, \mathcal{M}_{\text{val}}, \mathcal{M}_{\text{test}}$

기존 ML 모델 $f_{\mathrm{ML}}$은

$$
f_{\mathrm{ML}} : \mathcal{M} \rightarrow \hat{\mathcal{Y}}
$$

를 학습합니다. 학습은 전형적으로 다음과 같은 loss 최소화 문제로 표현됩니다.

$$
\min_{\Theta} \sum_{i=1}^{n} \mathcal{L}(\hat{y}_i, y_i)
$$

* $\Theta$: $f_{\mathrm{ML}}$의 파라미터

* $(G_i, y_i)$: 학습 데이터

* $\hat{y}_i = f_{\mathrm{ML}}(G_i)$: 모델 예측

* $\mathcal{L}$: 분류에서는 cross-entropy, 회귀에서는 MSE 등

학습이 끝난 뒤에는 $\Theta$를 고정(fixed)하고, $\mathcal{M}_{\text{test}}$에 대해 $\hat{y}$를 예측합니다.

논문의 목표는, 이 고정된 $f_{\mathrm{ML}}$ 위에 **LLM 기반 보정기 $f_{\mathrm{LLM}}$**를 붙여서 최종 예측

$$
\tilde{y}_u = f_{\mathrm{LLM}}(G_u, \hat{y}_u, D)
$$

를 얻는 것입니다.

* $G_u$: 쿼리 분자

* $\hat{y}_u = f_{\mathrm{ML}}(G_u)$: 기존 ML의 1차 예측

* $D$: contextual knowledge database에서 뽑은 관련 정보

#### 4.2 Step 1 – Contextual Knowledge Database 구축

ML 모델 학습이 끝나면, 다음 정보를 모아서 **맥락 지식 DB $\mathcal{D}$**를 만듭니다.

* 학습/검증 데이터에 대해:

  * 분자 표현 $G_v$ (SMILES, 그래프, 설명 텍스트 등)

  * 정답 레이블 $y_v$

* 검증 데이터에 대해서는 추가로:

  * 모델 예측값 $\hat{y}_v = f_{\mathrm{ML}}(G_v)$

이를 요약하면:

$$
\mathcal{D} = {(G_v, y_v, \hat{y}_v) \mid G_v \in \mathcal{M}_{\text{train}} \cup \mathcal{M}_{\text{val}}}
$$

이 DB는 다음과 같은 **맥락적 지식(contextual knowledge)**를 담고 있습니다.

* 어떤 분자에 대해 모델이 **정답을 잘 맞추는지**

* 어떤 분자에서 **일관되게 틀리는 패턴이 있는지**

* 특정 예측값 $\hat{y}$가 실제 레이블 $y$와 어떤 상관관계를 가지는지

LLM은 이 DB에서 뽑아온 사례들을 보고, **"기존 모델의 error landscape"**를 텍스트 형태로 간접 학습하게 됩니다. 

#### 4.3 Step 2 – Embedding-based Information Retrieval (EIR)

LLM은 입력 토큰 수에 제한이 있기 때문에, DB의 모든 예시를 prompt에 넣을 수는 없습니다.

그래서 논문은 **임베딩 기반 정보 검색(EIR)**을 사용합니다.

1. **텍스트 임베딩 함수 $f_{\mathrm{Emb}}$ 정의**

   분자의 텍스트 표현 $(S, D)$를 임베딩 벡터로 매핑합니다.

   $$
   \mathbf{z}_v = f_{\mathrm{Emb}}(S_v, D_v) \in \mathbb{R}^d
   $$

   * $S_v$: 분자 $G_v$의 SMILES

   * $D_v$: 분자 설명 텍스트

   * $f_{\mathrm{Emb}}$: OpenAI embedding 모델(text-embedding-3-large 등)

   쿼리 분자 $G_u$에 대해서도 동일하게:

   $$
   \mathbf{z}_u = f_{\mathrm{Emb}}(S_u, D_u)
   $$

2. **유사도 기반 top-k 이웃 검색**

   코사인 유사도 등으로 $\mathbf{z}_u$와 DB 내 임베딩 $\mathbf{z}_v$들을 비교해,

   가장 비슷한 top-k 사례 집합 $\mathcal{N}_k(u)$를 구합니다.

   $$
   \mathcal{N}_k(u) = \operatorname{TopK}_v , \text{sim}(\mathbf{z}_u, \mathbf{z}_v)
   $$

   * $\text{sim}$: 코사인 유사도 등

   * $\mathcal{N}_k(u)$: 쿼리 $u$와 가장 비슷한 $k$개의 과거 사례 인덱스 집합

3. **데이터 누수 방지**

   * 쿼리 데이터가 **검증셋 $\mathcal{M}_{\text{val}}$에 속하는 경우**,

   * 검색할 때 그 쿼리 자기 자신은 제외하여 **data leakage**를 막습니다.

   * 이렇게 하면 LLMCORR를 사용한 검증 성능 평가도 기존 파이프라인과 동일한 기준으로 비교할 수 있습니다.

EIR의 역할은 간단히 말해:

> "LLM에게 던져줄 예시를, 쿼리와 **의미적으로 가장 비슷한 것들** 위주로 잘 골라주자"

라는 것입니다. 랜덤하게 예시를 넣는 것보다 훨씬 강력한 ICL을 유도할 수 있습니다.

#### 4.4 Step 3 – LLMCORR Prompt 설계

이제 top-k로 고른 $\mathcal{N}_k(u)$를 이용해 prompt를 구성합니다. 논문에서 사용하는 템플릿의 구성 요소를 정리하면:

1. **Instruction (역할 지시)**  

2. **Training 데이터 기반 컨텍스트**  

3. **Validation 데이터 기반 컨텍스트 (핵심)**  

4. **Query + 질문**

<p align="center">
  <img src="https://i.imgur.com/AjMJbKx.png" alt="LLMCORR 프롬프트 템플릿 – training/validation 컨텍스트와 쿼리로 구성된 대화 포맷" width="400" referrerpolicy="no-referrer">
</p>

그림 3은 실제 LLMCORR 프롬프트를 그대로 코드 형태로 보여줍니다. `system` 역할로 "너는 분자 도메인 전문가이자 ML 보정기"라는 **역할 지시**를 주고, 이어서 `user`–`assistant` turn 형태로 training/validation 예시를 쌓은 뒤, 마지막에 쿼리 분자를 넣고 "Prediction: …, Probability: …" 형식으로 대답을 요구하는 구조죠. 이처럼 프롬프트 구조를 명시적으로 통제함으로써, LLM이 예측뿐 아니라 **신뢰도와 간단한 설명까지 일관된 포맷으로** 출력하도록 유도합니다.

1. **Instruction (역할 지시)**

   * "너는 '분자 예측 ML 모델'을 보정하는 **post-hoc corrector**이다."

   * "아래에 주어진 예시들을 보고, 이 모델이 어떤 패턴으로 틀리는지 이해한 뒤, 마지막에 주어지는 쿼리 데이터의 예측을 고쳐라."

   * 이런 식으로 LLM의 역할을 명확히 지정합니다.

2. **Training 데이터 기반 컨텍스트**

   * 예:

     > Molecule-1: SMILES = ...

     > True label = ...

   * 모델 예측은 필요 없고, **도메인 분포와 레이블 형식을 익히는 용도**입니다.

3. **Validation 데이터 기반 컨텍스트 (핵심)**

   * 예:

     > Molecule-2: SMILES = ...

     > Model prediction = ...

     > True label = ...

     > (모델이 여기서 틀렸다 / 맞았다)

   * 이 부분이 LLM에게

     **"모델이 어떤 상황에서 어떤식으로 틀리는지"**를 알려 주는 main signal입니다.

4. **Query + 질문**

   * 마지막에 쿼리 분자 $G_u$와 모델 예측 $\hat{y}_u$를 제시하고,

   * "위 예시들을 참고해서, 이 분자의 올바른 레이블(또는 값)을 추정하고, 모델 예측이 맞는지 틀렸는지 판단해라. 필요하다면 수정된 값을 제안해라."

   * 라고 묻습니다.

수식 형태로 표현하면, prompt $P_u$는

$$
P_u = \text{Template}\big(\mathcal{N}_k(u), G_u, \hat{y}_u\big)
$$

이고, LLM 출력은

$$
\tilde{y}_u, \text{explanation}_u = f_{\mathrm{LLM}}(P_u)
$$

입니다.

* $\tilde{y}_u$: 보정된 예측

* $\text{explanation}_u$: 왜 그렇게 판단했는지에 대한 자연어 설명

이 구조는 기존의 "few-shot ICL prompt"와 비슷하지만,

* **예시 안에 항상 "ML의 예측 + 정답"이 같이 들어간다**는 점에서

* "모델의 error pattern을 학습하는 prompt"라는 차별점이 있습니다. 

#### 4.5 Step 4 – Self-correction Prompting

LLM이 때때로 **과도하게 "창의적인" 수정**을 할 위험이 있습니다. 이를 막기 위해 논문은 다음과 같은 **self-correction 단계**를 둡니다. 분류 문제에서는 레이블을 뒤집을 정도로 크게 바꿨을 때, 회귀 문제에서는 원래 예측과 20% 이상 차이가 날 때 추가 검증을 거치도록 만드는 식입니다.

<p align="center">
  <img src="https://i.imgur.com/NmMXy4R.png" alt="Self-correction 프롬프트 템플릿 – LLM에게 자신의 보정을 다시 검토하도록 요구" width="400" referrerpolicy="no-referrer">
</p>

그림 4의 프롬프트를 보면, LLM에게 "너의 예측이 ML 모델의 원래 예측과 다르고, 현재 검증 성능이 어느 정도인지"를 알려주면서, **스스로 추론 과정을 점검하고 추가로 관련 정보를 찾아보라**고 요구합니다. 마지막에는 다시 한 번 "Prediction / Probability / Explanation" 형식으로 응답하게 만들어, 너무 공격적인 수정은 줄이고 보다 근거 있는 보정만 남도록 유도하는 구조입니다.

* 분류 문제:

  * $\tilde{y}_u$가 $\hat{y}_u$와 **클래스 자체가 다른 경우** (예: 0 → 1로 flipping)

* 회귀 문제:

  * $|\tilde{y}_u - \hat{y}_u|$가 예측 스케일 기준으로 **20% 이상** 크게 차이 나는 경우

위 조건에 해당하면, 별도의 self-correction용 prompt $P^{\text{self}}_u$를 만들어 다시 LLM에 묻습니다.

* "원래 모델 예측은 $\hat{y}_u$이고, 네가 제안한 값은 $\tilde{y}_u$이다."

* "위에서 본 예시들과 이 두 값의 차이를 고려해서,

  진짜로 $\tilde{y}_u$가 더 타당한지, 아니면 원래 값에 더 가까운 값으로 수정하는 게 나은지 다시 판단해라."

이를 통해:

* LLM이 "무턱대고 크게 바꾸는 것"을 줄이고

* 바꿀 때는 더 설득력 있는 근거를 기반으로 바꾸도록 유도합니다.

실험 결과, self-correction은 일부 데이터셋에선 성능을 향상시키지만,

일부에선 LLM이 너무 조심스러워져서 효과가 애매해지기도 합니다.

그래서 논문은 **"더 좋은 self-correction 프롬프트 설계가 앞으로의 과제"**라고 정리합니다.

#### 4.6 알고리즘 요약 및 확장성

알고리즘 1을 요약하면:

1. $f_{\mathrm{ML}}$을 $\mathcal{M}_{\text{train}}$으로 학습

2. $\mathcal{M}_{\text{train}}, \mathcal{M}_{\text{val}}$과 $\hat{y}_{\text{val}}$을 이용해 DB $\mathcal{D}$ 구성

3. 임의의 쿼리 $G_u$에 대해

   * $\hat{y}_u = f_{\mathrm{ML}}(G_u)$ 계산

   * $\mathcal{D}$에서 EIR로 top-k 유사 사례 $\mathcal{N}_k(u)$ 검색

   * 이를 이용해 prompt $P_u$ 생성

   * $f_{\mathrm{LLM}}(P_u)$로 보정 예측 $\tilde{y}_u$ 획득

   * 필요 시 self-correction 수행

확장성 측면에서 중요한 점:

* $f_{\mathrm{ML}}$은 어떤 형태의 모델이든 상관없습니다. (GNN, CNN, Transformer, tabular 모델 등)

* $f_{\mathrm{LLM}}$도 "텍스트 입출력"만 받으면 되므로, GPT 계열이든 Llama 계열이든 활용 가능합니다.

* 결국 **"모든 것을 텍스트로 풀어서 LLM에게 보여줄 수 있느냐"**가 관건입니다.


### 5. Experiments

#### 5.1 실험 설정

1. **데이터셋 (OGB Molecule Benchmarks)**

   * **이진 분류 (ROC-AUC↑)**

     * ogbg-molbace

     * ogbg-molbbbp

     * ogbg-molhiv

   * **회귀 (RMSE↓)**

     * ogbg-molesol

     * ogbg-molfreesolv

     * ogbg-mollipo

   각 데이터셋은 train/valid/test로 표준 분할(scaffold split 등)이 되어 있고,

   MoleculeNet/OGB에서 널리 쓰이는 세팅을 따릅니다. 

2. **기본 ML 모델 ($f_{\mathrm{ML}}$)**

   세 가지 계열의 모델을 사용합니다.

   * **Language Model (LM)**

     * DeBERTa를 SMILES 문자열에 적용한 모델

     * 구조 정보 없이 텍스트만 활용

   * **Graph Neural Networks (GNNs)**

     * GCN, GIN: 대표적인 GNN 기본형

     * HIG, PAS: OGB 리더보드에 올라온 SOTA 계열 GNN

   * **Hybrid 모델 (TAPE)**

     * LM/LLM에서 뽑은 텍스트 특징을 GNN과 결합하는 하이브리드 프레임워크

3. **LLM 및 임베딩 모델 ($f_{\mathrm{LLM}}, f_{\mathrm{Emb}}$)**

   * LLM:

     * GPT-3.5 (주력 실험)

     * GPT-4

     * Llama2-13B

   * Embedding 모델:

     * text-embedding-3-large (주 사용)

     * text-embedding-3-small (비교용)

   GPT 계열은 완전한 **black-box API**로 사용하며, 파라미터 업데이트는 전혀 하지 않습니다.

#### 5.2 주요 결과

먼저, LLMCORR를 적용하기 전후의 성능 변화를 전체적으로 보면 다음과 같습니다.

<p align="center">
  <img src="https://i.imgur.com/bvvEnDs.png" alt="Table 1 – 다양한 분자 데이터셋에서 기본 모델과 LLMCORR 적용 후 성능 비교" width="600" referrerpolicy="no-referrer">
</p>

표 1은 6개 OGB 분자 데이터셋에서 LM, GCN, GIN, HIG, PAS, TAPE 같은 다양한 기본 모델에 LLMCORR를 붙였을 때의 **ROC-AUC(분류)와 RMSE(회귀)**를 정리한 결과입니다. 각 행의 아래쪽에 있는 숫자는 "LLMCORR 적용으로 얼마나 이득을 봤는지"를 %로 표기한 값인데, 일부 회귀 태스크에서는 **RMSE가 최대 39%까지 감소**하는 등 꽤 큰 향상을 보여줍니다. 이를 바탕으로 논문은 몇 가지 핵심 관찰(Observation)을 정리합니다.

1. **Observation 1 – LLMCORR는 강력한 post-hoc corrector이다.**

   * 분류/회귀 모든 데이터셋에서 **기본 모델 대비 유의미한 성능 향상**을 보입니다.

   * 예를 들어, LM 기반 분자 예측에서:

     * ogbg-molfreesolv의 테스트 RMSE가

       원래 4.4532 → LLMCORR 적용 후 3.5595로 **약 20% 감소**

     * ogbg-molesol 등 일부에서는 **최대 39%까지 RMSE 감소**를 보고합니다.

   * 이는 "기존 모델을 손대지 않고도, LLM만으로 상당한 성능 이득을 얻을 수 있다"는 점을 보여줍니다.

2. **Observation 2 – 기하 구조(geometric structure)를 쓰는 모델이 여전히 강하다.**

   * GNN/HIG/PAS처럼 구조 정보를 활용하는 모델이

     LM(텍스트만 쓰는 모델)보다 기본 성능이 높습니다.

   * LLMCORR는 이런 모델들 위에서도 성능을 조금 더 밀어 올리지만,

     **기본 성능이 낮은 모델(LM)일수록 gain이 더 큽니다.**

3. **Observation 3 – ML 모델이 약할수록 LLM의 도움 폭이 크다.**

   * LM처럼 구조를 못 쓰는 모델은 LLMCORR를 통해 **상대적으로 큰 개선 폭**을 얻고,

   * 이미 강한 GNN 계열은 개선 폭이 작지만 그래도 대부분 플러스입니다.

   * 즉, LLMCORR는 일종의 **"약한 모델을 똑똑하게 만들어주는 후처리 레이어"**로 볼 수 있습니다.

4. **Observation 4 – GPT-3.5가 GPT-4, Llama2보다 이 세팅에서는 더 낫다.**

   LLMCORR의 상단 LLM을 어떤 모델로 쓰느냐에 따라 성능과 비용이 달라집니다. 논문에서는 Llama2-13B, GPT-3.5, GPT-4를 비교하면서, 각 조합의 예측 성능과 실행 시간을 함께 보고합니다.

   <p align="center">
     <img src="https://i.imgur.com/mjs9pIE.png" alt="Table 2 – 서로 다른 LLM(GPT-3.5, GPT-4, Llama2)을 썼을 때 성능과 실행 시간 비교" width="600" referrerpolicy="no-referrer">
   </p>

   표 2를 보면, 놀랍게도 **GPT-3.5 기반 LLMCORR가 여러 데이터셋에서 GPT-4 기반보다 더 좋은 성능을 내는 경우가 많고**, 실행 시간도 상대적으로 짧습니다. 저자들은 GPT-4가 RLHF 등 다양한 튜닝을 거치면서, 이런 "오류 패턴 보정" 스타일의 ICL 태스크에는 오히려 과도하게 보수적이거나 형식적인 응답을 하게 될 수 있다고 추측합니다. 반대로 Llama2는 비용 대비 나쁘지 않지만, 전체적으로는 GPT-3.5보다는 약간 뒤처지는 모습을 보입니다.

5. **Observation 5 – LLM을 "단독 predictor"로 쓰면 아직 경쟁력이 부족하다.**

   * 별도의 실험에서, LLM에게 직접 분자 특성을 **zero-shot / few-shot prompt**로 예측하게 했을 때,

     * ROC-AUC와 RMSE 모두 기존 ML 모델들에 비해 훨씬 낮은 성능을 보입니다 (Table 3).

   * 이는

     * LLM이 도메인 특화 수치 예측(특히 분자 특성)에 대해서는 아직 한계가 있고

     * 대신 **"기존 모델의 예측 결과와 실수 패턴을 읽고 보정하는 역할"**에 훨씬 잘 맞는다는 것을 시사합니다.

#### 5.3 Ablation Study

1. **Retrieval 전략 (Top-k vs Jump vs Random)**

   * Top-k: 쿼리와 가장 유사한 $k$개의 사례를 선택

   * Jump: DB 전체를 균등 간격으로 "점프"하며 $k$개 선택

   * Random: 무작위로 $k$개 선택

   결과:

   * **Top-k > Jump > Random** 순서로 성능이 좋습니다.

   * 이는 LLM이 **쿼리와 의미적으로 가까운 예시**를 볼수록

     오차 패턴을 잘 활용해 정확한 보정을 한다는 것을 보여줍니다. 

2. **$k$ (컨텍스트 개수)의 영향**

   LLMCORR에서는 쿼리 분자와 비슷한 예시를 몇 개나 LLM에게 보여줄지, 즉 $k$를 어떻게 잡느냐가 성능에 큰 영향을 줍니다. 논문은 $k$를 10, 20, 30, 50 등으로 바꿔가며 ablation을 수행합니다.

   <p align="center">
     <img src="https://i.imgur.com/KT6wX7w.png" alt="Figure 5 – 컨텍스트 개수 k에 따른 ROC-AUC 및 RMSE 변화" width="400" referrerpolicy="no-referrer">
   </p>

   그림 5를 보면, $k$가 너무 작을 때보다 30~50 정도로 늘렸을 때 **분류 태스크의 ROC-AUC가 꾸준히 상승**하고, 회귀 태스크에서도 RMSE가 전반적으로 감소하는 경향을 확인할 수 있습니다. 즉, LLM에게 "참고할 예시를 많이 보여줄수록" 보정 품질이 좋아지지만, 동시에 토큰 비용도 함께 커지므로 실제 시스템에서는 성능과 비용 사이에서 적절한 $k$를 선택해야 합니다.

3. **Embedding 모델 $f_{\mathrm{Emb}}$의 영향**

   * 큰 모델(text-embedding-3-large)이 작은 모델보다

     검색 품질(semantic similarity) 측면에서 우수하고,

   * 그 결과 LLMCORR의 최종 성능도 더 좋습니다 (Figure 6).

   * 즉, **retrieval 품질이 곧 correction 품질로 이어지는 구조**입니다.

4. **Self-correction의 영향**

   마지막으로, self-correction 단계를 넣었을 때와 빼었을 때의 차이도 별도로 분석합니다.

   <p align="center">
     <img src="https://i.imgur.com/LVRNulP.png" alt="Figure 7 – self-correction 유무에 따른 6개 데이터셋 성능 비교" width="400" referrerpolicy="no-referrer">
   </p>

   그림 7에서 보듯이, 일부 데이터셋에서는 self-correction이 **ROC-AUC를 조금 더 끌어올리거나 RMSE를 줄이는 긍정적인 효과**를 보이는 반면, 다른 데이터셋에서는 거의 차이가 없거나 오히려 약간의 성능 저하가 나타나기도 합니다. 이는 LLM이 self-correction 프롬프트 이후 지나치게 보수적으로 변해, 원래는 괜찮았던 수정까지 되돌려 버리는 경우가 있기 때문으로 보입니다. 저자들은 따라서 "self-correction의 아이디어는 유효하지만, 프롬프트 설계는 더 정교하게 다듬을 여지가 크다"고 결론을 내립니다.

### 6. Conclusion

이 논문은 **"LLM을 활용해 기존 ML 모델의 예측을 post-hoc으로 보정한다"**는 매우 실용적인 아이디어를

구체적인 프레임워크 LLMCORR로 구현하고, 분자 특성 예측 및 텍스트 분석에서 **정량적으로 유의미한 성능 향상**을 보여줍니다.

정리하면, 주요 기여는 다음과 같습니다.

1. **Training-free post-hoc correction 프레임워크**

   * 어떤 ML 모델이든 재학습 없이,

   * LLM + retrieval + prompt 설계만으로 성능을 개선할 수 있는 길을 제시했습니다.

2. **Contextual Knowledge Database + EIR + ICL의 조합**

   * Validation set에서의 예측·정답 정보를 맥락 DB로 만들고,

   * 임베딩 기반 retrieval로 LLM에게 **가장 도움이 되는 사례들**을 보여줌으로써

   * LLM이 모델의 "실수 패턴"을 학습하게 했습니다.

3. **실험적으로 최대 39% RMSE 감소 등 상당한 이득**

   * LM, GCN, GIN, HIG, PAS, TAPE 등 다양한 기반 모델 위에서

   * 분류/회귀 모두 일관된 성능 향상을 달성했습니다.

4. **LLM은 predictor라기보다 meta-corrector로 더 효과적**

   * LLM 단독 예측은 아직 기존 GNN/LM 모델보다 약하지만,

   * **기존 모델의 출력을 post-hoc으로 수정하는 역할**에서는 상당히 강력하다는 점을 보여주었습니다.

한계와 향후 연구 방향도 명확히 언급합니다.

* 현재 검증은 주로 **분자 그래프 + 일부 텍스트 과제**에 국한되어 있어,

  더 다양한 도메인(vision, tabular, 멀티모달 산업 데이터 등)에 대한 검증이 필요합니다.

* GPT-4의 underperformance 원인 규명, 더 긴 context를 사용할 수 있는 LLM에서

  **그래프/기하 구조 설명까지 prompt에 포함했을 때**의 효과도 흥미로운 연구 주제입니다.

* Retrieval 시스템을 더 고도화(RAG, 고급 selection 전략 등)하고,

  self-correction 프롬프트를 개선하는 것도 중요한 후속 과제입니다.

* 마지막으로, 대형 LLM 사용에 따른 **연산 자원 격차와 잠재적 프라이버시 이슈**도 논문에서 짚고 있습니다. 

요약하자면, LLMCORR는 "**기존 도메인 모델 + LLM**" 조합이

단순 RAG나 surrogate 모델링을 넘어서, **post-hoc correction 레이어**로도 매우 유효하다는 것을 보여주는 좋은 사례라고 볼 수 있습니다.

### 7. Appendix (선택적)

* **코드 및 모델**

  * 공식 구현: [LLMCorr GitHub](https://github.com/zhiqiangzhongddu/LLMCorr)

* **핵심 용어 정리 (간단)**

  * **Post-hoc corrector**: 학습이 끝난 모델의 출력을 사후적으로 수정하는 모듈

  * **Contextual knowledge database**: (입력, 모델 예측, 정답)을 모두 기록한 DB

  * **EIR (Embedding-based Information Retrieval)**: 텍스트 임베딩으로 유사한 예시를 검색하는 모듈

  * **In-Context Learning(ICL)**: 파라미터 업데이트 없이, prompt 안의 예시만으로 새로운 태스크에 적응하는 능력

* **참고 자료**

  * **논문**: [Harnessing Large Language Models as Post-hoc Correctors](https://www.semanticscholar.org/paper/9cae0b08b0b490d476e1ea75e9327da2ed1f8d78)
  * **arXiv**: [2402.13414](https://arxiv.org/abs/2402.13414)
  * **저자**: Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin
  * **학회**: ACL 2024

